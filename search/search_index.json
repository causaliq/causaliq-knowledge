{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"CausalIQ Knowledge","text":""},{"location":"#welcome","title":"Welcome","text":"<p>Welcome to the documentation for CausalIQ Knowledge, which combines traditional statistical structure learning algorithms with the contextual understanding and reasoning capabilities of Large Language Models. This integration enables more interpretable, domain-aware, and human-friendly causal discovery workflows. It is part of the CausalIQ ecosystem for intelligent causal discovery.</p>"},{"location":"#overview","title":"Overview","text":"<p>This site provides detailed documentation, including:</p> <ul> <li>Development roadmap</li> <li>User guide</li> <li>Architectural vision</li> <li>Design notes</li> <li>API reference for users and contributors</li> </ul>"},{"location":"#quickstart-installation","title":"Quickstart &amp; Installation","text":"<p>For a quickstart guide and installation instructions, see the README on GitHub.</p>"},{"location":"#documentation-contents","title":"Documentation Contents","text":"<ul> <li>Development Roadmap: Roadmap of upcoming features</li> <li>User Guide: Comprehensive user guide</li> <li>Architecture: Overall architecture and design notes</li> <li>API Reference: Complete reference for Python code</li> <li>Development Guidelines: CausalIQ guidelines for developers</li> <li>Changelog</li> <li>License</li> </ul>"},{"location":"#support-community","title":"Support &amp; Community","text":"<ul> <li>GitHub Issues: Report bugs or request features.</li> <li>GitHub Discussions: Ask questions and join the community.</li> </ul> <p>Tip: Use the navigation sidebar to explore the documentation. For the latest code and releases, visit the causaliq-knowledge GitHub repository.</p> <p>Supported Python Versions: 3.9, 3.10, 3.11, 3.12, 3.13 Default Python Version: 3.11</p>"},{"location":"roadmap/","title":"CausalIQ Knowledge - Development Roadmap","text":"<p>Last updated: January 10, 2026  </p> <p>This project roadmap fits into the overall ecosystem roadmap</p>"},{"location":"roadmap/#current-release","title":"\ud83c\udfaf Current Release","text":""},{"location":"roadmap/#release-v020-additional-llms-january-2026","title":"Release v0.2.0 - Additional LLMs [January 2026]","text":"<p>Expanded LLM provider support from 2 to 7 providers.</p> <p>Scope:</p> <ul> <li>OpenAI client for GPT-4o and GPT-4o-mini models</li> <li>Anthropic client for Claude models</li> <li>DeepSeek client for DeepSeek-V3 and R1 models</li> <li>Mistral client for Mistral AI models</li> <li>Ollama client for local LLM inference</li> <li>OpenAI-compatible base client for API-compatible providers</li> <li>Integration tests for all providers</li> <li>Cost estimation utilities for each provider</li> </ul>"},{"location":"roadmap/#previous-releases","title":"\u2705 Previous Releases","text":""},{"location":"roadmap/#release-v010-foundation-llm-january-2026","title":"Release v0.1.0 - Foundation LLM [January 2026]","text":"<p>Simple LLM queries to 1 or 2 LLMs about edge existence and orientation to support graph averaging.</p> <p>Delivered:</p> <ul> <li>Abstract <code>KnowledgeProvider</code> interface</li> <li><code>EdgeKnowledge</code> Pydantic model for structured responses</li> <li><code>LLMKnowledge</code> implementation using vendor-specific API clients</li> <li>Direct API clients for Groq and Google Gemini</li> <li>Single-model and multi-model consensus queries</li> <li>Basic prompt templates for edge existence/orientation</li> <li>CLI for testing queries</li> <li>100% test coverage</li> <li>Comprehensive documentation</li> </ul>"},{"location":"roadmap/#upcoming-releases","title":"\ud83d\udee3\ufe0f Upcoming Releases","text":""},{"location":"roadmap/#release-v030-llm-caching","title":"Release v0.3.0 - LLM Caching","text":"<ul> <li>Disk-based response caching (diskcache)</li> <li>Cache key: (node_a, node_b, context_hash, model)</li> <li>Cache invalidation strategies</li> <li>Semantic similarity caching (optional)</li> </ul>"},{"location":"roadmap/#release-v040-llm-context","title":"Release v0.4.0 - LLM Context","text":"<ul> <li>Variable descriptions and roles</li> <li>Domain context specification</li> <li>Literature retrieval (RAG) - evaluate lightweight alternatives to LangChain</li> <li>Vector store integration for document search</li> </ul>"},{"location":"roadmap/#release-v050-algorithm-integration","title":"Release v0.5.0 - Algorithm Integration","text":"<ul> <li>Integration hooks for structure learning algorithms</li> <li>Knowledge-guided constraint generation</li> <li>Integration with causaliq-analysis <code>average()</code> function</li> <li>Entropy-based automatic query triggering</li> </ul>"},{"location":"roadmap/#release-v060-legacy-reference","title":"Release v0.6.0 - Legacy Reference","text":"<ul> <li>Support for deriving knowledge from reference networks</li> <li>Migration of functionality from legacy discovery repo</li> </ul>"},{"location":"roadmap/#dependencies-evolution","title":"\ud83d\udce6 Dependencies Evolution","text":"<pre><code># v0.1.0 (current)\ndependencies = [\n    \"click&gt;=8.0.0\",\n    \"httpx&gt;=0.24.0\",\n    \"pydantic&gt;=2.0.0\",\n]\n\n# v0.3.0 (add)\n\"diskcache&gt;=5.0.0\"\n\n# v0.4.0 (evaluate - prefer lightweight solutions)\n# Consider: llama-index, simple RAG, or custom implementation\n# Avoid: langchain (heavy dependencies)\n</code></pre>"},{"location":"api/base/","title":"Base API Reference","text":"<p>Abstract base classes defining the knowledge provider interface.</p>"},{"location":"api/base/#knowledgeprovider","title":"KnowledgeProvider","text":""},{"location":"api/base/#causaliq_knowledge.base.KnowledgeProvider","title":"KnowledgeProvider","text":"<p>Abstract interface for all knowledge sources.</p> <p>This is the base class that all knowledge providers must implement. Knowledge providers can be LLM-based, rule-based, human-input based, or any other source of causal knowledge.</p> <p>The primary method is <code>query_edge()</code> which asks about the causal relationship between two variables.</p> Example <p>class MyKnowledgeProvider(KnowledgeProvider): ...     def query_edge(self, node_a, node_b, context=None): ...         # Implementation here ...         return EdgeKnowledge(exists=True, confidence=0.8, ...) ... provider = MyKnowledgeProvider() result = provider.query_edge(\"smoking\", \"cancer\")</p> <p>Methods:</p> <ul> <li> <code>query_edge</code>             \u2013              <p>Query whether a causal edge exists between two nodes.</p> </li> <li> <code>query_edges</code>             \u2013              <p>Query multiple edges at once.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>Return the name of this knowledge provider.</p> </li> </ul>"},{"location":"api/base/#causaliq_knowledge.base.KnowledgeProvider.name","title":"name  <code>property</code>","text":"<pre><code>name: str\n</code></pre> <p>Return the name of this knowledge provider.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>Class name by default. Subclasses may override.</p> </li> </ul>"},{"location":"api/base/#causaliq_knowledge.base.KnowledgeProvider.query_edge","title":"query_edge  <code>abstractmethod</code>","text":"<pre><code>query_edge(node_a: str, node_b: str, context: Optional[dict] = None) -&gt; EdgeKnowledge\n</code></pre> <p>Query whether a causal edge exists between two nodes.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>EdgeKnowledge</code>           \u2013            <p>EdgeKnowledge with: - exists: True, False, or None (uncertain) - direction: \"a_to_b\", \"b_to_a\", \"undirected\", or None - confidence: 0.0 to 1.0 - reasoning: Human-readable explanation - model: Source identifier (optional)</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>NotImplementedError</code>             \u2013            <p>If not implemented by subclass.</p> </li> </ul>"},{"location":"api/base/#causaliq_knowledge.base.KnowledgeProvider.query_edge(node_a)","title":"<code>node_a</code>","text":"(<code>str</code>)           \u2013            <p>Name of the first variable.</p>"},{"location":"api/base/#causaliq_knowledge.base.KnowledgeProvider.query_edge(node_b)","title":"<code>node_b</code>","text":"(<code>str</code>)           \u2013            <p>Name of the second variable.</p>"},{"location":"api/base/#causaliq_knowledge.base.KnowledgeProvider.query_edge(context)","title":"<code>context</code>","text":"(<code>Optional[dict]</code>, default:                   <code>None</code> )           \u2013            <p>Optional context dictionary that may include: - domain: The domain (e.g., \"medicine\", \"economics\") - descriptions: Dict mapping variable names to descriptions - additional_info: Any other relevant context</p>"},{"location":"api/base/#causaliq_knowledge.base.KnowledgeProvider.query_edges","title":"query_edges","text":"<pre><code>query_edges(\n    edges: list[tuple[str, str]], context: Optional[dict] = None\n) -&gt; list[EdgeKnowledge]\n</code></pre> <p>Query multiple edges at once.</p> <p>Default implementation calls query_edge for each pair. Subclasses may override for batch optimization.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>list[EdgeKnowledge]</code>           \u2013            <p>List of EdgeKnowledge results, one per edge pair.</p> </li> </ul>"},{"location":"api/base/#causaliq_knowledge.base.KnowledgeProvider.query_edges(edges)","title":"<code>edges</code>","text":"(<code>list[tuple[str, str]]</code>)           \u2013            <p>List of (node_a, node_b) tuples to query.</p>"},{"location":"api/base/#causaliq_knowledge.base.KnowledgeProvider.query_edges(context)","title":"<code>context</code>","text":"(<code>Optional[dict]</code>, default:                   <code>None</code> )           \u2013            <p>Optional context dictionary (shared across all queries).</p>"},{"location":"api/base_client/","title":"LLM Client Base Interface","text":"<p>Abstract base class and common types for all LLM vendor clients. This module defines the interface that all vendor-specific clients must implement, ensuring consistent behavior across different LLM providers.</p>"},{"location":"api/base_client/#overview","title":"Overview","text":"<p>The base client module provides:</p> <ul> <li>BaseLLMClient - Abstract base class defining the client interface</li> <li>LLMConfig - Base configuration dataclass for all clients</li> <li>LLMResponse - Unified response format from any LLM provider</li> </ul>"},{"location":"api/base_client/#design-philosophy","title":"Design Philosophy","text":"<p>We use vendor-specific API clients rather than wrapper libraries like LiteLLM or LangChain. This provides:</p> <ul> <li>Minimal dependencies (httpx only for HTTP)</li> <li>Reliable and predictable behavior</li> <li>Easy debugging without abstraction layers</li> <li>Full control over API interactions</li> </ul> <p>The abstract interface ensures that all vendor clients behave consistently, making it easy to swap providers or add new ones.</p>"},{"location":"api/base_client/#usage","title":"Usage","text":"<p>Vendor-specific clients inherit from <code>BaseLLMClient</code>:</p> <pre><code>from causaliq_knowledge.llm import (\n    BaseLLMClient,\n    LLMConfig,\n    LLMResponse,\n    GroqClient,\n    GeminiClient,\n)\n\n# All clients share the same interface\ndef query_llm(client: BaseLLMClient, prompt: str) -&gt; str:\n    messages = [{\"role\": \"user\", \"content\": prompt}]\n    response = client.completion(messages)\n    return response.content\n\n# Works with any client\ngroq = GroqClient()\ngemini = GeminiClient()\n\nresult1 = query_llm(groq, \"What is 2+2?\")\nresult2 = query_llm(gemini, \"What is 2+2?\")\n</code></pre>"},{"location":"api/base_client/#creating-a-custom-client","title":"Creating a Custom Client","text":"<p>To add support for a new LLM provider, implement the <code>BaseLLMClient</code> interface:</p> <pre><code>from causaliq_knowledge.llm import BaseLLMClient, LLMConfig, LLMResponse\n\nclass MyCustomClient(BaseLLMClient):\n    def __init__(self, config: LLMConfig) -&gt; None:\n        self.config = config\n        self._total_calls = 0\n\n    @property\n    def provider_name(self) -&gt; str:\n        return \"my_provider\"\n\n    def completion(self, messages, **kwargs) -&gt; LLMResponse:\n        # Implement API call here\n        ...\n        return LLMResponse(\n            content=\"response text\",\n            model=self.config.model,\n            input_tokens=10,\n            output_tokens=20,\n        )\n\n    @property\n    def call_count(self) -&gt; int:\n        return self._total_calls\n</code></pre>"},{"location":"api/base_client/#llmconfig","title":"LLMConfig","text":""},{"location":"api/base_client/#causaliq_knowledge.llm.base_client.LLMConfig","title":"LLMConfig  <code>dataclass</code>","text":"<pre><code>LLMConfig(\n    model: str,\n    temperature: float = 0.1,\n    max_tokens: int = 500,\n    timeout: float = 30.0,\n    api_key: Optional[str] = None,\n)\n</code></pre> <p>Base configuration for all LLM clients.</p> <p>This dataclass defines common configuration options shared by all LLM provider clients. Vendor-specific clients may extend this with additional options.</p> <p>Attributes:</p> <ul> <li> <code>model</code>               (<code>str</code>)           \u2013            <p>Model identifier (provider-specific format).</p> </li> <li> <code>temperature</code>               (<code>float</code>)           \u2013            <p>Sampling temperature (0.0=deterministic, 1.0=creative).</p> </li> <li> <code>max_tokens</code>               (<code>int</code>)           \u2013            <p>Maximum tokens in the response.</p> </li> <li> <code>timeout</code>               (<code>float</code>)           \u2013            <p>Request timeout in seconds.</p> </li> <li> <code>api_key</code>               (<code>Optional[str]</code>)           \u2013            <p>API key for authentication (optional, can use env var).</p> </li> </ul>"},{"location":"api/base_client/#llmresponse","title":"LLMResponse","text":""},{"location":"api/base_client/#causaliq_knowledge.llm.base_client.LLMResponse","title":"LLMResponse  <code>dataclass</code>","text":"<pre><code>LLMResponse(\n    content: str,\n    model: str,\n    input_tokens: int = 0,\n    output_tokens: int = 0,\n    cost: float = 0.0,\n    raw_response: Optional[Dict[str, Any]] = None,\n)\n</code></pre> <p>Standard response from any LLM client.</p> <p>This dataclass provides a unified response format across all LLM providers, abstracting away provider-specific response structures.</p> <p>Attributes:</p> <ul> <li> <code>content</code>               (<code>str</code>)           \u2013            <p>The text content of the response.</p> </li> <li> <code>model</code>               (<code>str</code>)           \u2013            <p>The model that generated the response.</p> </li> <li> <code>input_tokens</code>               (<code>int</code>)           \u2013            <p>Number of input/prompt tokens used.</p> </li> <li> <code>output_tokens</code>               (<code>int</code>)           \u2013            <p>Number of output/completion tokens generated.</p> </li> <li> <code>cost</code>               (<code>float</code>)           \u2013            <p>Estimated cost of the request (if available).</p> </li> <li> <code>raw_response</code>               (<code>Optional[Dict[str, Any]]</code>)           \u2013            <p>The original provider-specific response (for debugging).</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>parse_json</code>             \u2013              <p>Parse content as JSON, handling common formatting issues.</p> </li> </ul>"},{"location":"api/base_client/#causaliq_knowledge.llm.base_client.LLMResponse.parse_json","title":"parse_json","text":"<pre><code>parse_json() -&gt; Optional[Dict[str, Any]]\n</code></pre> <p>Parse content as JSON, handling common formatting issues.</p> <p>LLMs sometimes wrap JSON in markdown code blocks. This method handles those cases and attempts to extract valid JSON.</p> <p>Returns:</p> <ul> <li> <code>Optional[Dict[str, Any]]</code>           \u2013            <p>Parsed JSON as dict, or None if parsing fails.</p> </li> </ul>"},{"location":"api/base_client/#basellmclient","title":"BaseLLMClient","text":""},{"location":"api/base_client/#causaliq_knowledge.llm.base_client.BaseLLMClient","title":"BaseLLMClient","text":"<pre><code>BaseLLMClient(config: LLMConfig)\n</code></pre> <p>Abstract base class for LLM clients.</p> <p>All LLM vendor clients (OpenAI, Anthropic, Groq, Gemini, Llama, etc.) must implement this interface to ensure consistent behavior across the codebase.</p> <p>This abstraction allows: - Easy addition of new LLM providers - Consistent API for all providers - Provider-agnostic code in higher-level modules - Simplified testing with mock implementations</p> Example <p>class MyClient(BaseLLMClient): ...     def completion(self, messages, **kwargs): ...         # Implementation here ...         pass ... client = MyClient(config) msgs = [{\"role\": \"user\", \"content\": \"Hello\"}] response = client.completion(msgs) print(response.content)</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>complete_json</code>             \u2013              <p>Make a completion request and parse response as JSON.</p> </li> <li> <code>completion</code>             \u2013              <p>Make a chat completion request.</p> </li> <li> <code>is_available</code>             \u2013              <p>Check if the LLM provider is available and configured.</p> </li> <li> <code>list_models</code>             \u2013              <p>List available models from the provider.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>call_count</code>               (<code>int</code>)           \u2013            <p>Return the number of API calls made by this client.</p> </li> <li> <code>model_name</code>               (<code>str</code>)           \u2013            <p>Return the model name being used.</p> </li> <li> <code>provider_name</code>               (<code>str</code>)           \u2013            <p>Return the name of the LLM provider.</p> </li> </ul>"},{"location":"api/base_client/#causaliq_knowledge.llm.base_client.BaseLLMClient(config)","title":"<code>config</code>","text":"(<code>LLMConfig</code>)           \u2013            <p>Configuration for the LLM client.</p>"},{"location":"api/base_client/#causaliq_knowledge.llm.base_client.BaseLLMClient.call_count","title":"call_count  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>call_count: int\n</code></pre> <p>Return the number of API calls made by this client.</p> <p>Returns:</p> <ul> <li> <code>int</code>           \u2013            <p>Total number of completion calls made.</p> </li> </ul>"},{"location":"api/base_client/#causaliq_knowledge.llm.base_client.BaseLLMClient.model_name","title":"model_name  <code>property</code>","text":"<pre><code>model_name: str\n</code></pre> <p>Return the model name being used.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>Model identifier string.</p> </li> </ul>"},{"location":"api/base_client/#causaliq_knowledge.llm.base_client.BaseLLMClient.provider_name","title":"provider_name  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>provider_name: str\n</code></pre> <p>Return the name of the LLM provider.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>Provider name (e.g., \"openai\", \"anthropic\", \"groq\").</p> </li> </ul>"},{"location":"api/base_client/#causaliq_knowledge.llm.base_client.BaseLLMClient.complete_json","title":"complete_json","text":"<pre><code>complete_json(\n    messages: List[Dict[str, str]], **kwargs: Any\n) -&gt; tuple[Optional[Dict[str, Any]], LLMResponse]\n</code></pre> <p>Make a completion request and parse response as JSON.</p> <p>Convenience method that calls completion() and attempts to parse the response content as JSON.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>tuple[Optional[Dict[str, Any]], LLMResponse]</code>           \u2013            <p>Tuple of (parsed JSON dict or None, raw LLMResponse).</p> </li> </ul>"},{"location":"api/base_client/#causaliq_knowledge.llm.base_client.BaseLLMClient.complete_json(messages)","title":"<code>messages</code>","text":"(<code>List[Dict[str, str]]</code>)           \u2013            <p>List of message dicts with \"role\" and \"content\" keys.</p>"},{"location":"api/base_client/#causaliq_knowledge.llm.base_client.BaseLLMClient.complete_json(**kwargs)","title":"<code>**kwargs</code>","text":"(<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Provider-specific options passed to completion().</p>"},{"location":"api/base_client/#causaliq_knowledge.llm.base_client.BaseLLMClient.completion","title":"completion  <code>abstractmethod</code>","text":"<pre><code>completion(messages: List[Dict[str, str]], **kwargs: Any) -&gt; LLMResponse\n</code></pre> <p>Make a chat completion request.</p> <p>This is the core method that sends a request to the LLM provider and returns a standardized response.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>LLMResponse</code>           \u2013            <p>LLMResponse with the generated content and metadata.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the API request fails or returns an error.</p> </li> </ul>"},{"location":"api/base_client/#causaliq_knowledge.llm.base_client.BaseLLMClient.completion(messages)","title":"<code>messages</code>","text":"(<code>List[Dict[str, str]]</code>)           \u2013            <p>List of message dicts with \"role\" and \"content\" keys. Roles can be: \"system\", \"user\", \"assistant\".</p>"},{"location":"api/base_client/#causaliq_knowledge.llm.base_client.BaseLLMClient.completion(**kwargs)","title":"<code>**kwargs</code>","text":"(<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Provider-specific options (temperature, max_tokens, etc.) that override the config defaults.</p>"},{"location":"api/base_client/#causaliq_knowledge.llm.base_client.BaseLLMClient.is_available","title":"is_available  <code>abstractmethod</code>","text":"<pre><code>is_available() -&gt; bool\n</code></pre> <p>Check if the LLM provider is available and configured.</p> <p>This method checks whether the client can make API calls: - For cloud providers: checks if API key is set - For local providers: checks if server is running</p> <p>Returns:</p> <ul> <li> <code>bool</code>           \u2013            <p>True if the provider is available and ready for requests.</p> </li> </ul>"},{"location":"api/base_client/#causaliq_knowledge.llm.base_client.BaseLLMClient.list_models","title":"list_models  <code>abstractmethod</code>","text":"<pre><code>list_models() -&gt; List[str]\n</code></pre> <p>List available models from the provider.</p> <p>Queries the provider's API to get the list of models accessible with the current API key or configuration. Results are filtered by the user's subscription/access level.</p> <p>Returns:</p> <ul> <li> <code>List[str]</code>           \u2013            <p>List of model identifiers available for use.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the API request fails.</p> </li> </ul>"},{"location":"api/cache/","title":"Cache Module","text":"<p>SQLite-backed caching infrastructure with shared token dictionary for efficient storage. This module provides the core caching capability that will eventually migrate to <code>causaliq-core</code>.</p>"},{"location":"api/cache/#overview","title":"Overview","text":"<p>The cache module provides:</p> <ul> <li>TokenCache - SQLite-backed cache with connection management</li> <li>EntryEncoder - Abstract base for type-specific encoders (future)</li> </ul>"},{"location":"api/cache/#design-philosophy","title":"Design Philosophy","text":"<p>The cache uses SQLite for storage, providing:</p> <ul> <li>Fast indexed key lookup</li> <li>Built-in concurrency via SQLite locking</li> <li>In-memory mode via <code>:memory:</code> for testing</li> <li>Incremental updates without rewriting</li> </ul> <p>See Caching Architecture for full design details.</p>"},{"location":"api/cache/#usage","title":"Usage","text":""},{"location":"api/cache/#basic-in-memory-cache","title":"Basic In-Memory Cache","text":"<pre><code>from causaliq_knowledge.cache import TokenCache\n\n# In-memory cache (fast, non-persistent)\nwith TokenCache(\":memory:\") as cache:\n    assert cache.table_exists(\"tokens\")\n    assert cache.table_exists(\"cache_entries\")\n</code></pre>"},{"location":"api/cache/#file-based-persistent-cache","title":"File-Based Persistent Cache","text":"<pre><code>from causaliq_knowledge.cache import TokenCache\n\n# File-based cache (persistent)\nwith TokenCache(\"my_cache.db\") as cache:\n    # Data persists across sessions\n    print(f\"Entries: {cache.entry_count()}\")\n    print(f\"Tokens: {cache.token_count()}\")\n</code></pre>"},{"location":"api/cache/#transaction-support","title":"Transaction Support","text":"<pre><code>from causaliq_knowledge.cache import TokenCache\n\nwith TokenCache(\":memory:\") as cache:\n    # Transactions auto-commit on success, rollback on exception\n    with cache.transaction() as cursor:\n        cursor.execute(\"INSERT INTO tokens (token) VALUES (?)\", (\"example\",))\n</code></pre>"},{"location":"api/cache/#token-dictionary","title":"Token Dictionary","text":"<p>The cache maintains a shared token dictionary for cross-entry compression. Encoders use this to convert strings to compact integer IDs:</p> <pre><code>from causaliq_knowledge.cache import TokenCache\n\nwith TokenCache(\":memory:\") as cache:\n    # Get or create token IDs (used by encoders)\n    id1 = cache.get_or_create_token(\"hello\")  # Returns 1\n    id2 = cache.get_or_create_token(\"world\")  # Returns 2\n    id1_again = cache.get_or_create_token(\"hello\")  # Returns 1 (cached)\n\n    # Look up token by ID (used by decoders)\n    token = cache.get_token(1)  # Returns \"hello\"\n</code></pre>"},{"location":"api/cache/#api-reference","title":"API Reference","text":""},{"location":"api/cache/#causaliq_knowledge.cache.TokenCache","title":"TokenCache","text":"<pre><code>TokenCache(db_path: str | Path)\n</code></pre> <p>SQLite-backed cache with shared token dictionary.</p> <p>Attributes:</p> <ul> <li> <code>db_path</code>           \u2013            <p>Path to SQLite database file, or \":memory:\" for in-memory.</p> </li> <li> <code>conn</code>               (<code>Connection</code>)           \u2013            <p>SQLite connection (None until open() called or context entered).</p> </li> </ul> Example <p>with TokenCache(\":memory:\") as cache: ...     cache.put(\"abc123\", \"test\", b\"hello\") ...     data = cache.get(\"abc123\", \"test\")</p> <p>Parameters:</p> <ul> <li> </li> </ul> <p>Methods:</p> <ul> <li> <code>open</code>             \u2013              <p>Open the database connection and initialise schema.</p> </li> <li> <code>close</code>             \u2013              <p>Close the database connection.</p> </li> <li> <code>transaction</code>             \u2013              <p>Context manager for a database transaction.</p> </li> <li> <code>table_exists</code>             \u2013              <p>Check if a table exists in the database.</p> </li> <li> <code>entry_count</code>             \u2013              <p>Count cache entries, optionally filtered by type.</p> </li> <li> <code>token_count</code>             \u2013              <p>Count tokens in the dictionary.</p> </li> <li> <code>get_or_create_token</code>             \u2013              <p>Get token ID, creating a new entry if needed.</p> </li> <li> <code>get_token</code>             \u2013              <p>Get token string by ID.</p> </li> </ul>"},{"location":"api/cache/#causaliq_knowledge.cache.TokenCache(db_path)","title":"<code>db_path</code>","text":"(<code>str | Path</code>)           \u2013            <p>Path to SQLite database file. Use \":memory:\" for in-memory database (fast, non-persistent).</p>"},{"location":"api/cache/#causaliq_knowledge.cache.TokenCache.is_open","title":"is_open  <code>property</code>","text":"<pre><code>is_open: bool\n</code></pre> <p>Check if the cache connection is open.</p>"},{"location":"api/cache/#causaliq_knowledge.cache.TokenCache.is_memory","title":"is_memory  <code>property</code>","text":"<pre><code>is_memory: bool\n</code></pre> <p>Check if this is an in-memory database.</p>"},{"location":"api/cache/#causaliq_knowledge.cache.TokenCache.conn","title":"conn  <code>property</code>","text":"<pre><code>conn: Connection\n</code></pre> <p>Get the database connection, raising if not connected.</p>"},{"location":"api/cache/#causaliq_knowledge.cache.TokenCache.open","title":"open","text":"<pre><code>open() -&gt; TokenCache\n</code></pre> <p>Open the database connection and initialise schema.</p> <p>Returns:</p> <ul> <li> <code>TokenCache</code>           \u2013            <p>self for method chaining.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>RuntimeError</code>             \u2013            <p>If already connected.</p> </li> </ul>"},{"location":"api/cache/#causaliq_knowledge.cache.TokenCache.close","title":"close","text":"<pre><code>close() -&gt; None\n</code></pre> <p>Close the database connection.</p>"},{"location":"api/cache/#causaliq_knowledge.cache.TokenCache.transaction","title":"transaction","text":"<pre><code>transaction() -&gt; Iterator[Cursor]\n</code></pre> <p>Context manager for a database transaction.</p> <p>Commits on success, rolls back on exception.</p> <p>Yields:</p> <ul> <li> <code>Cursor</code>           \u2013            <p>SQLite cursor for executing statements.</p> </li> </ul>"},{"location":"api/cache/#causaliq_knowledge.cache.TokenCache.table_exists","title":"table_exists","text":"<pre><code>table_exists(table_name: str) -&gt; bool\n</code></pre> <p>Check if a table exists in the database.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>bool</code>           \u2013            <p>True if table exists, False otherwise.</p> </li> </ul>"},{"location":"api/cache/#causaliq_knowledge.cache.TokenCache.table_exists(table_name)","title":"<code>table_name</code>","text":"(<code>str</code>)           \u2013            <p>Name of the table to check.</p>"},{"location":"api/cache/#causaliq_knowledge.cache.TokenCache.entry_count","title":"entry_count","text":"<pre><code>entry_count(entry_type: str | None = None) -&gt; int\n</code></pre> <p>Count cache entries, optionally filtered by type.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>int</code>           \u2013            <p>Number of matching entries.</p> </li> </ul>"},{"location":"api/cache/#causaliq_knowledge.cache.TokenCache.entry_count(entry_type)","title":"<code>entry_type</code>","text":"(<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>If provided, count only entries of this type.</p>"},{"location":"api/cache/#causaliq_knowledge.cache.TokenCache.token_count","title":"token_count","text":"<pre><code>token_count() -&gt; int\n</code></pre> <p>Count tokens in the dictionary.</p> <p>Returns:</p> <ul> <li> <code>int</code>           \u2013            <p>Number of tokens.</p> </li> </ul>"},{"location":"api/cache/#causaliq_knowledge.cache.TokenCache.get_or_create_token","title":"get_or_create_token","text":"<pre><code>get_or_create_token(token: str) -&gt; int\n</code></pre> <p>Get token ID, creating a new entry if needed.</p> <p>This method is used by encoders to compress strings to integer IDs. The token dictionary grows dynamically as new tokens are encountered.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>int</code>           \u2013            <p>Integer ID for the token (1-65535 range).</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If token dictionary exceeds uint16 capacity.</p> </li> </ul>"},{"location":"api/cache/#causaliq_knowledge.cache.TokenCache.get_or_create_token(token)","title":"<code>token</code>","text":"(<code>str</code>)           \u2013            <p>The string token to look up or create.</p>"},{"location":"api/cache/#causaliq_knowledge.cache.TokenCache.get_token","title":"get_token","text":"<pre><code>get_token(token_id: int) -&gt; str | None\n</code></pre> <p>Get token string by ID.</p> <p>This method is used by decoders to expand integer IDs back to strings.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>str | None</code>           \u2013            <p>The token string, or None if not found.</p> </li> </ul>"},{"location":"api/cache/#causaliq_knowledge.cache.TokenCache.get_token(token_id)","title":"<code>token_id</code>","text":"(<code>int</code>)           \u2013            <p>The integer ID to look up.</p>"},{"location":"api/cli/","title":"CausalIQ Knowledge CLI","text":"<p>The command-line interface provides a quick way to test LLM queries about causal relationships.</p>"},{"location":"api/cli/#installation","title":"Installation","text":"<p>The CLI is automatically installed when you install the package:</p> <pre><code>pip install causaliq-knowledge\n</code></pre>"},{"location":"api/cli/#usage","title":"Usage","text":""},{"location":"api/cli/#basic-query","title":"Basic Query","text":"<pre><code># Query using default model (Groq)\ncqknow query smoking lung_cancer\n\n# With domain context\ncqknow query smoking lung_cancer --domain medicine\n</code></pre>"},{"location":"api/cli/#multiple-models","title":"Multiple Models","text":"<pre><code># Query multiple models for consensus\ncqknow query X Y --model groq/llama-3.1-8b-instant --model gemini/gemini-2.5-flash\n</code></pre>"},{"location":"api/cli/#json-output","title":"JSON Output","text":"<pre><code># Get structured JSON output\ncqknow query smoking lung_cancer --json\n</code></pre>"},{"location":"api/cli/#options","title":"Options","text":"Option Short Description <code>--model</code> <code>-m</code> LLM model to query (can be repeated) <code>--domain</code> <code>-d</code> Domain context (e.g., \"medicine\") <code>--strategy</code> <code>-s</code> Consensus strategy: weighted_vote or highest_confidence <code>--json</code> Output as JSON <code>--temperature</code> <code>-t</code> LLM temperature (0.0-1.0)"},{"location":"api/cli/#cli-entry-point","title":"CLI Entry Point","text":""},{"location":"api/cli/#causaliq_knowledge.cli","title":"cli","text":"<p>Command-line interface for causaliq-knowledge.</p> <p>Functions:</p> <ul> <li> <code>cli</code>             \u2013              <p>CausalIQ Knowledge - LLM knowledge for causal discovery.</p> </li> <li> <code>list_models</code>             \u2013              <p>List available LLM models from each provider.</p> </li> <li> <code>main</code>             \u2013              <p>Entry point for the CLI.</p> </li> <li> <code>query_edge</code>             \u2013              <p>Query LLMs about a causal relationship between two variables.</p> </li> </ul>"},{"location":"api/cli/#causaliq_knowledge.cli.cli","title":"cli","text":"<pre><code>cli() -&gt; None\n</code></pre> <p>CausalIQ Knowledge - LLM knowledge for causal discovery.</p> <p>Query LLMs about causal relationships between variables.</p>"},{"location":"api/cli/#causaliq_knowledge.cli.list_models","title":"list_models","text":"<pre><code>list_models(provider: Optional[str]) -&gt; None\n</code></pre> <p>List available LLM models from each provider.</p> <p>Queries each provider's API to show models accessible with your current configuration. Results are filtered by your API key's access level or locally installed models.</p> <p>Optionally specify PROVIDER to list models from a single provider: groq, anthropic, gemini, ollama, openai, deepseek, or mistral.</p> <p>Examples:</p> <pre><code>cqknow models              # List all providers\n\ncqknow models groq         # List only Groq models\n\ncqknow models mistral      # List only Mistral models\n</code></pre>"},{"location":"api/cli/#causaliq_knowledge.cli.main","title":"main","text":"<pre><code>main() -&gt; None\n</code></pre> <p>Entry point for the CLI.</p>"},{"location":"api/cli/#causaliq_knowledge.cli.query_edge","title":"query_edge","text":"<pre><code>query_edge(\n    node_a: str,\n    node_b: str,\n    model: tuple[str, ...],\n    domain: Optional[str],\n    strategy: str,\n    output_json: bool,\n    temperature: float,\n) -&gt; None\n</code></pre> <p>Query LLMs about a causal relationship between two variables.</p> <p>NODE_A and NODE_B are the variable names to query about.</p> <p>Examples:</p> <pre><code>cqknow query smoking lung_cancer\n\ncqknow query smoking lung_cancer --domain medicine\n\ncqknow query X Y --model groq/llama-3.1-8b-instant                          --model gemini/gemini-2.5-flash\n</code></pre>"},{"location":"api/models/","title":"Models API Reference","text":"<p>Core Pydantic models for representing causal knowledge.</p>"},{"location":"api/models/#edgedirection","title":"EdgeDirection","text":""},{"location":"api/models/#causaliq_knowledge.models.EdgeDirection","title":"EdgeDirection","text":"<p>Direction of a causal edge between two nodes.</p> <p>Attributes:</p> <ul> <li> <code>A_TO_B</code>           \u2013            </li> <li> <code>B_TO_A</code>           \u2013            </li> <li> <code>UNDIRECTED</code>           \u2013            </li> </ul>"},{"location":"api/models/#causaliq_knowledge.models.EdgeDirection.A_TO_B","title":"A_TO_B  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>A_TO_B = 'a_to_b'\n</code></pre>"},{"location":"api/models/#causaliq_knowledge.models.EdgeDirection.B_TO_A","title":"B_TO_A  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>B_TO_A = 'b_to_a'\n</code></pre>"},{"location":"api/models/#causaliq_knowledge.models.EdgeDirection.UNDIRECTED","title":"UNDIRECTED  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>UNDIRECTED = 'undirected'\n</code></pre>"},{"location":"api/models/#edgeknowledge","title":"EdgeKnowledge","text":""},{"location":"api/models/#causaliq_knowledge.models.EdgeKnowledge","title":"EdgeKnowledge","text":"<p>Structured knowledge about a potential causal edge.</p> <p>This model represents the result of querying a knowledge source about whether a causal relationship exists between two variables.</p> <p>Attributes:</p> <ul> <li> <code>exists</code>               (<code>Optional[bool]</code>)           \u2013            <p>Whether a causal edge exists. True, False, or None (uncertain).</p> </li> <li> <code>direction</code>               (<code>Optional[EdgeDirection]</code>)           \u2013            <p>The direction of the causal relationship if it exists. \"a_to_b\" means node_a causes node_b, \"b_to_a\" means the reverse, \"undirected\" means bidirectional or direction unknown.</p> </li> <li> <code>confidence</code>               (<code>float</code>)           \u2013            <p>Confidence score from 0.0 (no confidence) to 1.0 (certain).</p> </li> <li> <code>reasoning</code>               (<code>str</code>)           \u2013            <p>Human-readable explanation for the knowledge assessment.</p> </li> <li> <code>model</code>               (<code>Optional[str]</code>)           \u2013            <p>The LLM or knowledge source that provided this response.</p> </li> </ul> Example <p>knowledge = EdgeKnowledge( ...     exists=True, ...     direction=\"a_to_b\", ...     confidence=0.85, ...     reasoning=\"Smoking causes lung cancer.\", ...     model=\"gpt-4o-mini\" ... )</p> <p>Methods:</p> <ul> <li> <code>is_uncertain</code>             \u2013              <p>Check if this knowledge is uncertain.</p> </li> <li> <code>to_dict</code>             \u2013              <p>Convert to dictionary with string direction.</p> </li> <li> <code>uncertain</code>             \u2013              <p>Create an uncertain EdgeKnowledge instance.</p> </li> <li> <code>validate_direction</code>             \u2013              <p>Convert string direction to EdgeDirection enum.</p> </li> </ul>"},{"location":"api/models/#causaliq_knowledge.models.EdgeKnowledge.confidence","title":"confidence  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>confidence: float = Field(\n    default=0.0, ge=0.0, le=1.0, description=\"Confidence score from 0.0 to 1.0.\"\n)\n</code></pre>"},{"location":"api/models/#causaliq_knowledge.models.EdgeKnowledge.direction","title":"direction  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>direction: Optional[EdgeDirection] = Field(\n    default=None, description=\"Direction of the causal relationship if it exists.\"\n)\n</code></pre>"},{"location":"api/models/#causaliq_knowledge.models.EdgeKnowledge.exists","title":"exists  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>exists: Optional[bool] = Field(\n    default=None, description=\"Whether a causal edge exists. None = uncertain.\"\n)\n</code></pre>"},{"location":"api/models/#causaliq_knowledge.models.EdgeKnowledge.model","title":"model  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model: Optional[str] = Field(\n    default=None, description=\"The model/source that provided this knowledge.\"\n)\n</code></pre>"},{"location":"api/models/#causaliq_knowledge.models.EdgeKnowledge.reasoning","title":"reasoning  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>reasoning: str = Field(\n    default=\"\", description=\"Human-readable explanation for the assessment.\"\n)\n</code></pre>"},{"location":"api/models/#causaliq_knowledge.models.EdgeKnowledge.is_uncertain","title":"is_uncertain","text":"<pre><code>is_uncertain() -&gt; bool\n</code></pre> <p>Check if this knowledge is uncertain.</p> <p>Returns:</p> <ul> <li> <code>bool</code>           \u2013            <p>True if exists is None or confidence is below 0.5.</p> </li> </ul>"},{"location":"api/models/#causaliq_knowledge.models.EdgeKnowledge.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; dict\n</code></pre> <p>Convert to dictionary with string direction.</p> <p>Returns:</p> <ul> <li> <code>dict</code>           \u2013            <p>Dictionary representation suitable for JSON serialization.</p> </li> </ul>"},{"location":"api/models/#causaliq_knowledge.models.EdgeKnowledge.uncertain","title":"uncertain  <code>classmethod</code>","text":"<pre><code>uncertain(\n    reasoning: str = \"Unable to determine\", model: Optional[str] = None\n) -&gt; EdgeKnowledge\n</code></pre> <p>Create an uncertain EdgeKnowledge instance.</p> <p>Useful for error cases or when knowledge source cannot provide an answer.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>EdgeKnowledge</code>           \u2013            <p>EdgeKnowledge with exists=None and confidence=0.0.</p> </li> </ul>"},{"location":"api/models/#causaliq_knowledge.models.EdgeKnowledge.uncertain(reasoning)","title":"<code>reasoning</code>","text":"(<code>str</code>, default:                   <code>'Unable to determine'</code> )           \u2013            <p>Explanation for why the result is uncertain.</p>"},{"location":"api/models/#causaliq_knowledge.models.EdgeKnowledge.uncertain(model)","title":"<code>model</code>","text":"(<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>The model/source that was queried.</p>"},{"location":"api/models/#causaliq_knowledge.models.EdgeKnowledge.validate_direction","title":"validate_direction  <code>classmethod</code>","text":"<pre><code>validate_direction(v: Optional[str]) -&gt; Optional[EdgeDirection]\n</code></pre> <p>Convert string direction to EdgeDirection enum.</p>"},{"location":"api/overview/","title":"CausalIQ Knowledge API Reference","text":"<p>API documentation for causaliq-knowledge, organized by module.</p>"},{"location":"api/overview/#import-patterns","title":"Import Patterns","text":"<p>Core models are available from the top-level package:</p> <pre><code>from causaliq_knowledge import EdgeKnowledge, EdgeDirection, KnowledgeProvider\n</code></pre> <p>Cache infrastructure is available from the <code>cache</code> submodule:</p> <pre><code>from causaliq_knowledge.cache import TokenCache\n</code></pre> <p>LLM-specific classes should be imported from the <code>llm</code> submodule:</p> <pre><code>from causaliq_knowledge.llm import (\n    # Abstract base interface\n    BaseLLMClient,\n    LLMConfig,\n    LLMResponse,\n    # Main provider\n    LLMKnowledge,\n    # Vendor clients\n    GroqClient,\n    GroqConfig,\n    GeminiClient,\n    GeminiConfig,\n    OpenAIClient,\n    OpenAIConfig,\n    AnthropicClient,\n    AnthropicConfig,\n    DeepSeekClient,\n    DeepSeekConfig,\n    MistralClient,\n    MistralConfig,\n    OllamaClient,\n    OllamaConfig,\n    # Prompts\n    EdgeQueryPrompt,\n    parse_edge_response,\n)\n</code></pre>"},{"location":"api/overview/#modules","title":"Modules","text":""},{"location":"api/overview/#models","title":"Models","text":"<p>Core Pydantic models for representing causal knowledge:</p> <ul> <li>EdgeDirection - Enum for causal edge direction (a_to_b, b_to_a, undirected)</li> <li>EdgeKnowledge - Structured knowledge about a potential causal edge</li> </ul>"},{"location":"api/overview/#cache","title":"Cache","text":"<p>SQLite-backed caching infrastructure:</p> <ul> <li>TokenCache - Cache with connection management and transaction support</li> </ul>"},{"location":"api/overview/#base","title":"Base","text":"<p>Abstract interfaces for knowledge providers:</p> <ul> <li>KnowledgeProvider - Abstract base class that all knowledge sources implement</li> </ul>"},{"location":"api/overview/#llm-provider","title":"LLM Provider","text":"<p>Main entry point for LLM-based knowledge queries:</p> <ul> <li>LLMKnowledge - KnowledgeProvider implementation using vendor-specific API clients</li> <li>weighted_vote - Multi-model consensus by weighted voting</li> <li>highest_confidence - Select response with highest confidence</li> </ul>"},{"location":"api/overview/#llm-client-interface","title":"LLM Client Interface","text":"<p>Abstract base class and common types for LLM vendor clients:</p> <ul> <li>BaseLLMClient - Abstract interface all vendor clients implement</li> <li>LLMConfig - Base configuration dataclass</li> <li>LLMResponse - Unified response format</li> </ul>"},{"location":"api/overview/#vendor-api-clients","title":"Vendor API Clients","text":"<p>Direct API clients for specific LLM providers. All implement the <code>BaseLLMClient</code> interface.</p> <ul> <li>Groq Client - Fast inference via Groq API</li> <li>Gemini Client - Google Gemini API</li> <li>OpenAI Client - OpenAI GPT models</li> <li>Anthropic Client - Anthropic Claude models</li> <li>DeepSeek Client - DeepSeek models</li> <li>Mistral Client - Mistral AI models</li> <li>Ollama Client - Local LLMs via Ollama</li> </ul>"},{"location":"api/overview/#prompts","title":"Prompts","text":"<p>Prompt templates for LLM edge queries:</p> <ul> <li>EdgeQueryPrompt - Builder for edge existence/orientation prompts</li> <li>parse_edge_response - Parse LLM JSON responses to EdgeKnowledge</li> </ul>"},{"location":"api/overview/#cli","title":"CLI","text":"<p>Command-line interface for testing and querying.</p>"},{"location":"api/prompts/","title":"Prompts Module","text":"<p>The <code>prompts</code> module provides prompt templates and utilities for querying LLMs about causal relationships between variables.</p>"},{"location":"api/prompts/#overview","title":"Overview","text":"<p>This module contains:</p> <ul> <li>EdgeQueryPrompt: A dataclass for building prompts to query edge existence and orientation</li> <li>parse_edge_response: A function to parse LLM JSON responses into <code>EdgeKnowledge</code> objects</li> <li>Template constants: Pre-defined prompt templates for system and user messages</li> </ul>"},{"location":"api/prompts/#edgequeryprompt","title":"EdgeQueryPrompt","text":""},{"location":"api/prompts/#causaliq_knowledge.llm.prompts.EdgeQueryPrompt","title":"EdgeQueryPrompt  <code>dataclass</code>","text":"<pre><code>EdgeQueryPrompt(\n    node_a: str,\n    node_b: str,\n    domain: Optional[str] = None,\n    descriptions: Optional[dict[str, str]] = None,\n    system_prompt: Optional[str] = None,\n)\n</code></pre> <p>Builder for edge existence/orientation query prompts.</p> <p>This class constructs system and user prompts for querying an LLM about causal relationships between variables.</p> <p>Attributes:</p> <ul> <li> <code>node_a</code>               (<code>str</code>)           \u2013            <p>Name of the first variable.</p> </li> <li> <code>node_b</code>               (<code>str</code>)           \u2013            <p>Name of the second variable.</p> </li> <li> <code>domain</code>               (<code>Optional[str]</code>)           \u2013            <p>Optional domain context (e.g., \"medicine\", \"economics\").</p> </li> <li> <code>descriptions</code>               (<code>Optional[dict[str, str]]</code>)           \u2013            <p>Optional dict mapping variable names to descriptions.</p> </li> <li> <code>system_prompt</code>               (<code>Optional[str]</code>)           \u2013            <p>Custom system prompt (uses default if None).</p> </li> </ul> Example <p>prompt = EdgeQueryPrompt(\"smoking\", \"cancer\", domain=\"medicine\") system, user = prompt.build()</p> <p>Methods:</p> <ul> <li> <code>build</code>             \u2013              <p>Build the system and user prompts.</p> </li> <li> <code>from_context</code>             \u2013              <p>Create an EdgeQueryPrompt from a context dictionary.</p> </li> </ul>"},{"location":"api/prompts/#causaliq_knowledge.llm.prompts.EdgeQueryPrompt--use-with-llmclient","title":"Use with LLMClient","text":"<p>response = client.complete(system=system, user=user)</p>"},{"location":"api/prompts/#causaliq_knowledge.llm.prompts.EdgeQueryPrompt.build","title":"build","text":"<pre><code>build() -&gt; tuple[str, str]\n</code></pre> <p>Build the system and user prompts.</p> <p>Returns:</p> <ul> <li> <code>tuple[str, str]</code>           \u2013            <p>Tuple of (system_prompt, user_prompt).</p> </li> </ul> Source code in <code>src\\causaliq_knowledge\\llm\\prompts.py</code> <pre><code>def build(self) -&gt; tuple[str, str]:\n    \"\"\"Build the system and user prompts.\n\n    Returns:\n        Tuple of (system_prompt, user_prompt).\n    \"\"\"\n    system = self.system_prompt or DEFAULT_SYSTEM_PROMPT\n\n    # Build user prompt\n    if self.domain:\n        user = USER_PROMPT_WITH_DOMAIN_TEMPLATE.format(\n            domain=self.domain,\n            node_a=self.node_a,\n            node_b=self.node_b,\n        )\n    else:\n        user = USER_PROMPT_TEMPLATE.format(\n            node_a=self.node_a,\n            node_b=self.node_b,\n        )\n\n    # Add variable descriptions if provided\n    if self.descriptions:\n        desc_a = self.descriptions.get(self.node_a, \"No description\")\n        desc_b = self.descriptions.get(self.node_b, \"No description\")\n        user += VARIABLE_DESCRIPTIONS_TEMPLATE.format(\n            node_a=self.node_a,\n            desc_a=desc_a,\n            node_b=self.node_b,\n            desc_b=desc_b,\n        )\n\n    return system, user\n</code></pre>"},{"location":"api/prompts/#causaliq_knowledge.llm.prompts.EdgeQueryPrompt.from_context","title":"from_context  <code>classmethod</code>","text":"<pre><code>from_context(\n    node_a: str, node_b: str, context: Optional[dict] = None\n) -&gt; EdgeQueryPrompt\n</code></pre> <p>Create an EdgeQueryPrompt from a context dictionary.</p> <p>This is a convenience method for creating prompts from the context dict used by KnowledgeProvider.query_edge().</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>EdgeQueryPrompt</code>           \u2013            <p>EdgeQueryPrompt instance.</p> </li> </ul> Source code in <code>src\\causaliq_knowledge\\llm\\prompts.py</code> <pre><code>@classmethod\ndef from_context(\n    cls,\n    node_a: str,\n    node_b: str,\n    context: Optional[dict] = None,\n) -&gt; \"EdgeQueryPrompt\":\n    \"\"\"Create an EdgeQueryPrompt from a context dictionary.\n\n    This is a convenience method for creating prompts from the\n    context dict used by KnowledgeProvider.query_edge().\n\n    Args:\n        node_a: Name of the first variable.\n        node_b: Name of the second variable.\n        context: Optional context dict with keys:\n            - domain: str\n            - descriptions: dict[str, str]\n            - system_prompt: str\n\n    Returns:\n        EdgeQueryPrompt instance.\n    \"\"\"\n    if context is None:\n        return cls(node_a=node_a, node_b=node_b)\n\n    return cls(\n        node_a=node_a,\n        node_b=node_b,\n        domain=context.get(\"domain\"),\n        descriptions=context.get(\"descriptions\"),\n        system_prompt=context.get(\"system_prompt\"),\n    )\n</code></pre>"},{"location":"api/prompts/#causaliq_knowledge.llm.prompts.EdgeQueryPrompt.from_context(node_a)","title":"<code>node_a</code>","text":"(<code>str</code>)           \u2013            <p>Name of the first variable.</p>"},{"location":"api/prompts/#causaliq_knowledge.llm.prompts.EdgeQueryPrompt.from_context(node_b)","title":"<code>node_b</code>","text":"(<code>str</code>)           \u2013            <p>Name of the second variable.</p>"},{"location":"api/prompts/#causaliq_knowledge.llm.prompts.EdgeQueryPrompt.from_context(context)","title":"<code>context</code>","text":"(<code>Optional[dict]</code>, default:                   <code>None</code> )           \u2013            <p>Optional context dict with keys: - domain: str - descriptions: dict[str, str] - system_prompt: str</p>"},{"location":"api/prompts/#usage-example","title":"Usage Example","text":"<pre><code>from causaliq_knowledge.llm import EdgeQueryPrompt\nfrom causaliq_knowledge.llm import GroqClient, GroqConfig\n\n# Create a prompt for querying the relationship between two variables\nprompt = EdgeQueryPrompt(\n    node_a=\"smoking\",\n    node_b=\"lung_cancer\",\n    domain=\"medicine\",\n    descriptions={\n        \"smoking\": \"Tobacco consumption frequency\",\n        \"lung_cancer\": \"Diagnosis of lung cancer\",\n    },\n)\n\n# Build the system and user prompts\nsystem_prompt, user_prompt = prompt.build()\n\n# Use with GroqClient\nconfig = GroqConfig(model=\"llama-3.1-8b-instant\")\nclient = GroqClient(config=config)\nmessages = [\n    {\"role\": \"system\", \"content\": system_prompt},\n    {\"role\": \"user\", \"content\": user_prompt},\n]\njson_data, response = client.complete_json(messages)\n</code></pre>"},{"location":"api/prompts/#using-from_context","title":"Using from_context","text":"<p>The <code>from_context</code> class method provides a convenient way to create prompts from a context dictionary, which is the format used by <code>KnowledgeProvider.query_edge()</code>:</p> <pre><code>context = {\n    \"domain\": \"economics\",\n    \"descriptions\": {\n        \"interest_rate\": \"Central bank interest rate\",\n        \"inflation\": \"Consumer price index change\",\n    },\n}\n\nprompt = EdgeQueryPrompt.from_context(\n    node_a=\"interest_rate\",\n    node_b=\"inflation\",\n    context=context,\n)\n</code></pre>"},{"location":"api/prompts/#parse_edge_response","title":"parse_edge_response","text":""},{"location":"api/prompts/#causaliq_knowledge.llm.prompts.parse_edge_response","title":"parse_edge_response","text":"<pre><code>parse_edge_response(\n    json_data: Optional[dict], model: Optional[str] = None\n) -&gt; EdgeKnowledge\n</code></pre> <p>Parse a JSON response dict into an EdgeKnowledge object.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>EdgeKnowledge</code>           \u2013            <p>EdgeKnowledge object. Returns uncertain result if json_data is None</p> </li> <li> <code>EdgeKnowledge</code>           \u2013            <p>or missing required fields.</p> </li> </ul> Source code in <code>src\\causaliq_knowledge\\llm\\prompts.py</code> <pre><code>def parse_edge_response(\n    json_data: Optional[dict],\n    model: Optional[str] = None,\n) -&gt; EdgeKnowledge:\n    \"\"\"Parse a JSON response dict into an EdgeKnowledge object.\n\n    Args:\n        json_data: Parsed JSON dict from LLM response, or None if parsing\n            failed.\n        model: Optional model identifier to include in the result.\n\n    Returns:\n        EdgeKnowledge object. Returns uncertain result if json_data is None\n        or missing required fields.\n    \"\"\"\n    if json_data is None:\n        return EdgeKnowledge.uncertain(\n            reasoning=\"Failed to parse LLM response as JSON\",\n            model=model,\n        )\n\n    # Extract fields with defaults\n    exists = json_data.get(\"exists\")\n    direction_str = json_data.get(\"direction\")\n    confidence = json_data.get(\"confidence\", 0.0)\n    reasoning = json_data.get(\"reasoning\", \"\")\n\n    # Validate confidence is a number\n    try:\n        confidence = float(confidence)\n        confidence = max(0.0, min(1.0, confidence))\n    except (TypeError, ValueError):\n        confidence = 0.0\n\n    # Convert direction string to enum\n    direction = None\n    if direction_str:\n        try:\n            direction = EdgeDirection(direction_str.lower())\n        except ValueError:\n            # Invalid direction, leave as None\n            pass\n\n    return EdgeKnowledge(\n        exists=exists,\n        direction=direction,\n        confidence=confidence,\n        reasoning=str(reasoning),\n        model=model,\n    )\n</code></pre>"},{"location":"api/prompts/#causaliq_knowledge.llm.prompts.parse_edge_response(json_data)","title":"<code>json_data</code>","text":"(<code>Optional[dict]</code>)           \u2013            <p>Parsed JSON dict from LLM response, or None if parsing failed.</p>"},{"location":"api/prompts/#causaliq_knowledge.llm.prompts.parse_edge_response(model)","title":"<code>model</code>","text":"(<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional model identifier to include in the result.</p>"},{"location":"api/prompts/#usage-example_1","title":"Usage Example","text":"<pre><code>from causaliq_knowledge.llm import (\n    EdgeQueryPrompt,\n    GroqClient,\n    GroqConfig,\n    parse_edge_response,\n)\n\n# Create client and prompt\nconfig = GroqConfig(model=\"llama-3.1-8b-instant\")\nclient = GroqClient(config=config)\nprompt = EdgeQueryPrompt(\"X\", \"Y\", domain=\"statistics\")\nsystem, user = prompt.build()\n\n# Query the LLM\nmessages = [\n    {\"role\": \"system\", \"content\": system},\n    {\"role\": \"user\", \"content\": user},\n]\njson_data, response = client.complete_json(messages)\n\n# Parse the response into EdgeKnowledge\nknowledge = parse_edge_response(json_data, model=\"groq/llama-3.1-8b-instant\")\n\nprint(f\"Edge exists: {knowledge.exists}\")\nprint(f\"Direction: {knowledge.direction}\")\nprint(f\"Confidence: {knowledge.confidence}\")\nprint(f\"Reasoning: {knowledge.reasoning}\")\n</code></pre>"},{"location":"api/prompts/#prompt-templates","title":"Prompt Templates","text":"<p>The module exports several template constants that can be customized:</p>"},{"location":"api/prompts/#default_system_prompt","title":"DEFAULT_SYSTEM_PROMPT","text":"<p>The default system prompt instructs the LLM to act as a causal reasoning expert and respond with structured JSON.</p>"},{"location":"api/prompts/#user_prompt_template","title":"USER_PROMPT_TEMPLATE","text":"<p>Basic user prompt template for querying edge relationships without domain context.</p>"},{"location":"api/prompts/#user_prompt_with_domain_template","title":"USER_PROMPT_WITH_DOMAIN_TEMPLATE","text":"<p>User prompt template that includes domain context for more accurate responses.</p>"},{"location":"api/prompts/#variable_descriptions_template","title":"VARIABLE_DESCRIPTIONS_TEMPLATE","text":"<p>Template addition for including variable descriptions in the prompt.</p>"},{"location":"api/prompts/#custom-system-prompts","title":"Custom System Prompts","text":"<p>You can provide a custom system prompt to <code>EdgeQueryPrompt</code>:</p> <pre><code>custom_system = \"\"\"You are a biomedical expert.\nAssess causal relationships based on established medical literature.\nRespond with JSON: {\"exists\": bool, \"direction\": str, \"confidence\": float, \"reasoning\": str}\n\"\"\"\n\nprompt = EdgeQueryPrompt(\n    node_a=\"gene_X\",\n    node_b=\"protein_Y\",\n    domain=\"molecular_biology\",\n    system_prompt=custom_system,\n)\n</code></pre>"},{"location":"api/provider/","title":"LLM Knowledge Provider","text":"<p>The <code>LLMKnowledge</code> class is the main entry point for querying LLMs about causal relationships. It implements the <code>KnowledgeProvider</code> interface and supports multi-model consensus using vendor-specific API clients.</p>"},{"location":"api/provider/#architecture","title":"Architecture","text":"<p><code>LLMKnowledge</code> uses direct vendor-specific API clients rather than wrapper libraries like LiteLLM or LangChain. Currently supported providers:</p> <ul> <li>Groq: Fast inference for open-source models (free tier)</li> <li>Gemini: Google's Gemini models (generous free tier)</li> <li>OpenAI: GPT-4o and other OpenAI models</li> <li>Anthropic: Claude models</li> <li>DeepSeek: DeepSeek-V3 and DeepSeek-R1 models</li> <li>Mistral: Mistral AI models</li> <li>Ollama: Local LLMs (Llama, Mistral, Phi, etc.)</li> </ul>"},{"location":"api/provider/#usage","title":"Usage","text":"<pre><code>from causaliq_knowledge.llm import LLMKnowledge\n\n# Single model (default: Groq)\nprovider = LLMKnowledge()\n\n# Query about a potential edge\nresult = provider.query_edge(\"smoking\", \"lung_cancer\")\nprint(f\"Exists: {result.exists}\")\nprint(f\"Direction: {result.direction}\")\nprint(f\"Confidence: {result.confidence}\")\n\n# Multi-model consensus\nprovider = LLMKnowledge(\n    models=[\"groq/llama-3.1-8b-instant\", \"gemini/gemini-2.5-flash\"],\n    consensus_strategy=\"weighted_vote\",\n)\nresult = provider.query_edge(\n    \"exercise\",\n    \"heart_health\",\n    context={\"domain\": \"medicine\"},\n)\n</code></pre>"},{"location":"api/provider/#model-identifiers","title":"Model Identifiers","text":"<p>Models are specified with a provider prefix:</p> Provider Format Example Groq <code>groq/&lt;model&gt;</code> <code>groq/llama-3.1-8b-instant</code> Gemini <code>gemini/&lt;model&gt;</code> <code>gemini/gemini-2.5-flash</code> OpenAI <code>openai/&lt;model&gt;</code> <code>openai/gpt-4o-mini</code> Anthropic <code>anthropic/&lt;model&gt;</code> <code>anthropic/claude-sonnet-4-20250514</code> DeepSeek <code>deepseek/&lt;model&gt;</code> <code>deepseek/deepseek-chat</code> Mistral <code>mistral/&lt;model&gt;</code> <code>mistral/mistral-small-latest</code> Ollama <code>ollama/&lt;model&gt;</code> <code>ollama/llama3</code>"},{"location":"api/provider/#llmknowledge","title":"LLMKnowledge","text":""},{"location":"api/provider/#causaliq_knowledge.llm.provider.LLMKnowledge","title":"LLMKnowledge","text":"<pre><code>LLMKnowledge(\n    models: Optional[list[str]] = None,\n    consensus_strategy: str = \"weighted_vote\",\n    temperature: float = 0.1,\n    max_tokens: int = 500,\n    timeout: float = 30.0,\n    max_retries: int = 3,\n)\n</code></pre> <p>LLM-based knowledge provider using direct API clients.</p> <p>This provider queries one or more LLMs about causal relationships and combines their responses using a configurable consensus strategy. Uses direct API clients for reliability and control.</p> <p>Attributes:</p> <ul> <li> <code>models</code>               (<code>list[str]</code>)           \u2013            <p>List of model identifiers (e.g., \"groq/llama-3.1-8b-instant\").</p> </li> <li> <code>consensus_strategy</code>               (<code>str</code>)           \u2013            <p>Strategy for combining multi-model responses.</p> </li> <li> <code>clients</code>               (<code>str</code>)           \u2013            <p>Dict mapping model names to direct client instances.</p> </li> </ul> Example <p>provider = LLMKnowledge(models=[\"groq/llama-3.1-8b-instant\"]) result = provider.query_edge(\"smoking\", \"lung_cancer\") print(f\"Exists: {result.exists}, Confidence: {result.confidence}\")</p> <p>Parameters:</p> <ul> <li> </li> <li> </li> <li> </li> <li> </li> <li> </li> <li> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If consensus_strategy is not recognized or        unsupported model.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>query_edge</code>             \u2013              <p>Query LLMs about a potential causal edge.</p> </li> <li> <code>get_stats</code>             \u2013              <p>Get combined statistics from all clients.</p> </li> </ul>"},{"location":"api/provider/#causaliq_knowledge.llm.provider.LLMKnowledge--multi-model-consensus","title":"Multi-model consensus","text":"<p>provider = LLMKnowledge( ...     models=[ ...         \"groq/llama-3.1-8b-instant\", ...         \"gemini/gemini-2.5-flash\" ...     ], ...     consensus_strategy=\"weighted_vote\" ... )</p>"},{"location":"api/provider/#causaliq_knowledge.llm.provider.LLMKnowledge(models)","title":"<code>models</code>","text":"(<code>Optional[list[str]]</code>, default:                   <code>None</code> )           \u2013            <p>List of model identifiers. Supported formats: - \"groq/llama-3.1-8b-instant\" (Groq API) - \"gemini/gemini-2.5-flash\" (Google Gemini API) - \"ollama/llama3.2:1b\" (Local Ollama server) Defaults to [\"groq/llama-3.1-8b-instant\"].</p>"},{"location":"api/provider/#causaliq_knowledge.llm.provider.LLMKnowledge(consensus_strategy)","title":"<code>consensus_strategy</code>","text":"(<code>str</code>, default:                   <code>'weighted_vote'</code> )           \u2013            <p>How to combine multi-model responses. Options: \"weighted_vote\", \"highest_confidence\".</p>"},{"location":"api/provider/#causaliq_knowledge.llm.provider.LLMKnowledge(temperature)","title":"<code>temperature</code>","text":"(<code>float</code>, default:                   <code>0.1</code> )           \u2013            <p>LLM temperature (lower = more deterministic).</p>"},{"location":"api/provider/#causaliq_knowledge.llm.provider.LLMKnowledge(max_tokens)","title":"<code>max_tokens</code>","text":"(<code>int</code>, default:                   <code>500</code> )           \u2013            <p>Maximum tokens in LLM response.</p>"},{"location":"api/provider/#causaliq_knowledge.llm.provider.LLMKnowledge(timeout)","title":"<code>timeout</code>","text":"(<code>float</code>, default:                   <code>30.0</code> )           \u2013            <p>Request timeout in seconds.</p>"},{"location":"api/provider/#causaliq_knowledge.llm.provider.LLMKnowledge(max_retries)","title":"<code>max_retries</code>","text":"(<code>int</code>, default:                   <code>3</code> )           \u2013            <p>Number of retries on failure (unused for direct APIs).</p>"},{"location":"api/provider/#causaliq_knowledge.llm.provider.LLMKnowledge.name","title":"name  <code>property</code>","text":"<pre><code>name: str\n</code></pre> <p>Return provider name with model list.</p>"},{"location":"api/provider/#causaliq_knowledge.llm.provider.LLMKnowledge.models","title":"models  <code>property</code>","text":"<pre><code>models: list[str]\n</code></pre> <p>Return list of model identifiers.</p>"},{"location":"api/provider/#causaliq_knowledge.llm.provider.LLMKnowledge.consensus_strategy","title":"consensus_strategy  <code>property</code>","text":"<pre><code>consensus_strategy: str\n</code></pre> <p>Return the consensus strategy name.</p>"},{"location":"api/provider/#causaliq_knowledge.llm.provider.LLMKnowledge.query_edge","title":"query_edge","text":"<pre><code>query_edge(node_a: str, node_b: str, context: Optional[dict] = None) -&gt; EdgeKnowledge\n</code></pre> <p>Query LLMs about a potential causal edge.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>EdgeKnowledge</code>           \u2013            <p>EdgeKnowledge with combined result from all models.</p> </li> </ul>"},{"location":"api/provider/#causaliq_knowledge.llm.provider.LLMKnowledge.query_edge(node_a)","title":"<code>node_a</code>","text":"(<code>str</code>)           \u2013            <p>Name of the first variable.</p>"},{"location":"api/provider/#causaliq_knowledge.llm.provider.LLMKnowledge.query_edge(node_b)","title":"<code>node_b</code>","text":"(<code>str</code>)           \u2013            <p>Name of the second variable.</p>"},{"location":"api/provider/#causaliq_knowledge.llm.provider.LLMKnowledge.query_edge(context)","title":"<code>context</code>","text":"(<code>Optional[dict]</code>, default:                   <code>None</code> )           \u2013            <p>Optional context dict with keys: - domain: str - Domain context (e.g., \"medicine\") - descriptions: dict[str, str] - Variable descriptions - system_prompt: str - Custom system prompt</p>"},{"location":"api/provider/#causaliq_knowledge.llm.provider.LLMKnowledge.get_stats","title":"get_stats","text":"<pre><code>get_stats() -&gt; Dict[str, Any]\n</code></pre> <p>Get combined statistics from all clients.</p> <p>Returns:</p> <ul> <li> <code>Dict[str, Any]</code>           \u2013            <p>Dict with total_calls, total_cost, and per-model stats.</p> </li> </ul>"},{"location":"api/provider/#consensus-strategies","title":"Consensus Strategies","text":"<p>When using multiple models, responses are combined using a consensus strategy.</p>"},{"location":"api/provider/#weighted_vote","title":"weighted_vote","text":"<p>The default strategy. Combines responses by:</p> <ol> <li>Existence: Weighted vote by confidence (True, False, or None)</li> <li>Direction: Weighted majority among agreeing models</li> <li>Confidence: Average confidence of agreeing models</li> <li>Reasoning: Combined from all models</li> </ol>"},{"location":"api/provider/#causaliq_knowledge.llm.provider.weighted_vote","title":"weighted_vote","text":"<pre><code>weighted_vote(responses: list[EdgeKnowledge]) -&gt; EdgeKnowledge\n</code></pre> <p>Combine multiple responses using weighted voting.</p> <p>Strategy: - For existence: weighted vote by confidence - For direction: weighted majority among those agreeing on existence - Final confidence: average confidence of agreeing models - Reasoning: combine reasoning from all models</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>EdgeKnowledge</code>           \u2013            <p>Combined EdgeKnowledge result.</p> </li> </ul>"},{"location":"api/provider/#causaliq_knowledge.llm.provider.weighted_vote(responses)","title":"<code>responses</code>","text":"(<code>list[EdgeKnowledge]</code>)           \u2013            <p>List of EdgeKnowledge from different models.</p>"},{"location":"api/provider/#highest_confidence","title":"highest_confidence","text":"<p>Simply returns the response with the highest confidence score.</p>"},{"location":"api/provider/#causaliq_knowledge.llm.provider.highest_confidence","title":"highest_confidence","text":"<pre><code>highest_confidence(responses: list[EdgeKnowledge]) -&gt; EdgeKnowledge\n</code></pre> <p>Return the response with highest confidence.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>EdgeKnowledge</code>           \u2013            <p>EdgeKnowledge with highest confidence score.</p> </li> </ul>"},{"location":"api/provider/#causaliq_knowledge.llm.provider.highest_confidence(responses)","title":"<code>responses</code>","text":"(<code>list[EdgeKnowledge]</code>)           \u2013            <p>List of EdgeKnowledge from different models.</p>"},{"location":"api/provider/#example-multi-model-comparison","title":"Example: Multi-Model Comparison","text":"<pre><code>from causaliq_knowledge.llm import LLMKnowledge\n\n# Query multiple models (Groq + Gemini)\nprovider = LLMKnowledge(\n    models=[\"groq/llama-3.1-8b-instant\", \"gemini/gemini-2.5-flash\"],\n    consensus_strategy=\"weighted_vote\",\n    temperature=0.1,  # Low temperature for consistency\n)\n\n# Query with domain context\nresult = provider.query_edge(\n    node_a=\"interest_rate\",\n    node_b=\"inflation\",\n    context={\n        \"domain\": \"macroeconomics\",\n        \"descriptions\": {\n            \"interest_rate\": \"Central bank policy rate\",\n            \"inflation\": \"Year-over-year CPI change\",\n        },\n    },\n)\n\nprint(f\"Combined result: {result.exists} ({result.direction})\")\nprint(f\"Confidence: {result.confidence:.2f}\")\nprint(f\"Reasoning: {result.reasoning}\")\n\n# Check usage stats\nstats = provider.get_stats()\nprint(f\"Total cost: ${stats['total_cost']:.4f}\")\n</code></pre>"},{"location":"api/provider/#using-local-models-free","title":"Using Local Models (Free)","text":"<pre><code># Use Ollama for free local inference\n# First: install Ollama and run `ollama pull llama3`\nprovider = LLMKnowledge(models=[\"ollama/llama3\"])\n\n# Or mix local and cloud models\nprovider = LLMKnowledge(\n    models=[\"ollama/llama3\", \"gpt-4o-mini\"],\n    consensus_strategy=\"weighted_vote\",\n)\n</code></pre>"},{"location":"api/clients/anthropic/","title":"Anthropic Client API Reference","text":"<p>Direct Anthropic API client for Claude models. This client implements the BaseLLMClient interface using httpx to communicate directly with the Anthropic API.</p>"},{"location":"api/clients/anthropic/#overview","title":"Overview","text":"<p>The Anthropic client provides:</p> <ul> <li>Direct HTTP communication with Anthropic's API</li> <li>Implements the <code>BaseLLMClient</code> abstract interface</li> <li>JSON response parsing with error handling</li> <li>Call counting for usage tracking</li> <li>Configurable timeout and retry settings</li> <li>Proper handling of Anthropic's system prompt format</li> </ul>"},{"location":"api/clients/anthropic/#usage","title":"Usage","text":"<pre><code>from causaliq_knowledge.llm import AnthropicClient, AnthropicConfig\n\n# Create client with custom config\nconfig = AnthropicConfig(\n    model=\"claude-sonnet-4-20250514\",\n    temperature=0.1,\n    max_tokens=500,\n)\nclient = AnthropicClient(config=config)\n\n# Make a completion request\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"What is 2+2?\"},\n]\nresponse = client.completion(messages)\nprint(response.content)\n\n# Parse JSON response\njson_data = response.parse_json()\n</code></pre>"},{"location":"api/clients/anthropic/#environment-variables","title":"Environment Variables","text":"<p>The Anthropic client requires the <code>ANTHROPIC_API_KEY</code> environment variable to be set:</p> <pre><code>export ANTHROPIC_API_KEY=your_api_key_here\n</code></pre>"},{"location":"api/clients/anthropic/#anthropicconfig","title":"AnthropicConfig","text":""},{"location":"api/clients/anthropic/#causaliq_knowledge.llm.anthropic_client.AnthropicConfig","title":"AnthropicConfig  <code>dataclass</code>","text":"<pre><code>AnthropicConfig(\n    model: str = \"claude-sonnet-4-20250514\",\n    temperature: float = 0.1,\n    max_tokens: int = 500,\n    timeout: float = 30.0,\n    api_key: Optional[str] = None,\n)\n</code></pre> <p>Configuration for Anthropic API client.</p> <p>Extends LLMConfig with Anthropic-specific defaults.</p> <p>Attributes:</p> <ul> <li> <code>model</code>               (<code>str</code>)           \u2013            <p>Anthropic model identifier (default: claude-sonnet-4-20250514).</p> </li> <li> <code>temperature</code>               (<code>float</code>)           \u2013            <p>Sampling temperature (default: 0.1).</p> </li> <li> <code>max_tokens</code>               (<code>int</code>)           \u2013            <p>Maximum response tokens (default: 500).</p> </li> <li> <code>timeout</code>               (<code>float</code>)           \u2013            <p>Request timeout in seconds (default: 30.0).</p> </li> <li> <code>api_key</code>               (<code>Optional[str]</code>)           \u2013            <p>Anthropic API key (falls back to ANTHROPIC_API_KEY env var).</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>__post_init__</code>             \u2013              <p>Set API key from environment if not provided.</p> </li> </ul>"},{"location":"api/clients/anthropic/#causaliq_knowledge.llm.anthropic_client.AnthropicConfig.api_key","title":"api_key  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>api_key: Optional[str] = None\n</code></pre>"},{"location":"api/clients/anthropic/#causaliq_knowledge.llm.anthropic_client.AnthropicConfig.max_tokens","title":"max_tokens  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_tokens: int = 500\n</code></pre>"},{"location":"api/clients/anthropic/#causaliq_knowledge.llm.anthropic_client.AnthropicConfig.model","title":"model  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model: str = 'claude-sonnet-4-20250514'\n</code></pre>"},{"location":"api/clients/anthropic/#causaliq_knowledge.llm.anthropic_client.AnthropicConfig.temperature","title":"temperature  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>temperature: float = 0.1\n</code></pre>"},{"location":"api/clients/anthropic/#causaliq_knowledge.llm.anthropic_client.AnthropicConfig.timeout","title":"timeout  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>timeout: float = 30.0\n</code></pre>"},{"location":"api/clients/anthropic/#causaliq_knowledge.llm.anthropic_client.AnthropicConfig.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__() -&gt; None\n</code></pre> <p>Set API key from environment if not provided.</p>"},{"location":"api/clients/anthropic/#anthropicclient","title":"AnthropicClient","text":""},{"location":"api/clients/anthropic/#causaliq_knowledge.llm.anthropic_client.AnthropicClient","title":"AnthropicClient","text":"<pre><code>AnthropicClient(config: Optional[AnthropicConfig] = None)\n</code></pre> <p>Direct Anthropic API client.</p> <p>Implements the BaseLLMClient interface for Anthropic's Claude API. Uses httpx for HTTP requests.</p> Example <p>config = AnthropicConfig(model=\"claude-sonnet-4-20250514\") client = AnthropicClient(config) msgs = [{\"role\": \"user\", \"content\": \"Hello\"}] response = client.completion(msgs) print(response.content)</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>complete_json</code>             \u2013              <p>Make a completion request and parse response as JSON.</p> </li> <li> <code>completion</code>             \u2013              <p>Make a chat completion request to Anthropic.</p> </li> <li> <code>is_available</code>             \u2013              <p>Check if Anthropic API is available.</p> </li> <li> <code>list_models</code>             \u2013              <p>List available Claude models from Anthropic API.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>API_VERSION</code>           \u2013            </li> <li> <code>BASE_URL</code>           \u2013            </li> <li> <code>_total_calls</code>           \u2013            </li> <li> <code>call_count</code>               (<code>int</code>)           \u2013            <p>Return the number of API calls made.</p> </li> <li> <code>config</code>           \u2013            </li> <li> <code>model_name</code>               (<code>str</code>)           \u2013            <p>Return the model name being used.</p> </li> <li> <code>provider_name</code>               (<code>str</code>)           \u2013            <p>Return the provider name.</p> </li> </ul>"},{"location":"api/clients/anthropic/#causaliq_knowledge.llm.anthropic_client.AnthropicClient(config)","title":"<code>config</code>","text":"(<code>Optional[AnthropicConfig]</code>, default:                   <code>None</code> )           \u2013            <p>Anthropic configuration. If None, uses defaults with    API key from ANTHROPIC_API_KEY environment variable.</p>"},{"location":"api/clients/anthropic/#causaliq_knowledge.llm.anthropic_client.AnthropicClient.API_VERSION","title":"API_VERSION  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>API_VERSION = '2023-06-01'\n</code></pre>"},{"location":"api/clients/anthropic/#causaliq_knowledge.llm.anthropic_client.AnthropicClient.BASE_URL","title":"BASE_URL  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>BASE_URL = 'https://api.anthropic.com/v1'\n</code></pre>"},{"location":"api/clients/anthropic/#causaliq_knowledge.llm.anthropic_client.AnthropicClient._total_calls","title":"_total_calls  <code>instance-attribute</code>","text":"<pre><code>_total_calls = 0\n</code></pre>"},{"location":"api/clients/anthropic/#causaliq_knowledge.llm.anthropic_client.AnthropicClient.call_count","title":"call_count  <code>property</code>","text":"<pre><code>call_count: int\n</code></pre> <p>Return the number of API calls made.</p>"},{"location":"api/clients/anthropic/#causaliq_knowledge.llm.anthropic_client.AnthropicClient.config","title":"config  <code>instance-attribute</code>","text":"<pre><code>config = config or AnthropicConfig()\n</code></pre>"},{"location":"api/clients/anthropic/#causaliq_knowledge.llm.anthropic_client.AnthropicClient.model_name","title":"model_name  <code>property</code>","text":"<pre><code>model_name: str\n</code></pre> <p>Return the model name being used.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>Model identifier string.</p> </li> </ul>"},{"location":"api/clients/anthropic/#causaliq_knowledge.llm.anthropic_client.AnthropicClient.provider_name","title":"provider_name  <code>property</code>","text":"<pre><code>provider_name: str\n</code></pre> <p>Return the provider name.</p>"},{"location":"api/clients/anthropic/#causaliq_knowledge.llm.anthropic_client.AnthropicClient.complete_json","title":"complete_json","text":"<pre><code>complete_json(\n    messages: List[Dict[str, str]], **kwargs: Any\n) -&gt; tuple[Optional[Dict[str, Any]], LLMResponse]\n</code></pre> <p>Make a completion request and parse response as JSON.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>tuple[Optional[Dict[str, Any]], LLMResponse]</code>           \u2013            <p>Tuple of (parsed JSON dict or None, raw LLMResponse).</p> </li> </ul>"},{"location":"api/clients/anthropic/#causaliq_knowledge.llm.anthropic_client.AnthropicClient.complete_json(messages)","title":"<code>messages</code>","text":"(<code>List[Dict[str, str]]</code>)           \u2013            <p>List of message dicts with \"role\" and \"content\" keys.</p>"},{"location":"api/clients/anthropic/#causaliq_knowledge.llm.anthropic_client.AnthropicClient.complete_json(**kwargs)","title":"<code>**kwargs</code>","text":"(<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Override config options passed to completion().</p>"},{"location":"api/clients/anthropic/#causaliq_knowledge.llm.anthropic_client.AnthropicClient.completion","title":"completion","text":"<pre><code>completion(messages: List[Dict[str, str]], **kwargs: Any) -&gt; LLMResponse\n</code></pre> <p>Make a chat completion request to Anthropic.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>LLMResponse</code>           \u2013            <p>LLMResponse with the generated content and metadata.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the API request fails.</p> </li> </ul>"},{"location":"api/clients/anthropic/#causaliq_knowledge.llm.anthropic_client.AnthropicClient.completion(messages)","title":"<code>messages</code>","text":"(<code>List[Dict[str, str]]</code>)           \u2013            <p>List of message dicts with \"role\" and \"content\" keys.</p>"},{"location":"api/clients/anthropic/#causaliq_knowledge.llm.anthropic_client.AnthropicClient.completion(**kwargs)","title":"<code>**kwargs</code>","text":"(<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Override config options (temperature, max_tokens).</p>"},{"location":"api/clients/anthropic/#causaliq_knowledge.llm.anthropic_client.AnthropicClient.is_available","title":"is_available","text":"<pre><code>is_available() -&gt; bool\n</code></pre> <p>Check if Anthropic API is available.</p> <p>Returns:</p> <ul> <li> <code>bool</code>           \u2013            <p>True if ANTHROPIC_API_KEY is configured.</p> </li> </ul>"},{"location":"api/clients/anthropic/#causaliq_knowledge.llm.anthropic_client.AnthropicClient.list_models","title":"list_models","text":"<pre><code>list_models() -&gt; List[str]\n</code></pre> <p>List available Claude models from Anthropic API.</p> <p>Queries the Anthropic /v1/models endpoint to get available models.</p> <p>Returns:</p> <ul> <li> <code>List[str]</code>           \u2013            <p>List of model identifiers</p> </li> <li> <code>List[str]</code>           \u2013            <p>(e.g., ['claude-sonnet-4-20250514', ...]).</p> </li> </ul>"},{"location":"api/clients/anthropic/#supported-models","title":"Supported Models","text":"<p>Anthropic provides the Claude family of models:</p> Model Description Free Tier <code>claude-sonnet-4-20250514</code> Claude Sonnet 4 - balanced performance \u274c No <code>claude-opus-4-20250514</code> Claude Opus 4 - highest capability \u274c No <code>claude-3-5-haiku-latest</code> Claude 3.5 Haiku - fast and efficient \u274c No <p>See Anthropic documentation for the full list of available models.</p>"},{"location":"api/clients/deepseek/","title":"DeepSeek Client","text":"<p>Direct DeepSeek API client for DeepSeek-V3 and DeepSeek-R1 models.</p>"},{"location":"api/clients/deepseek/#overview","title":"Overview","text":"<p>DeepSeek is a Chinese AI company known for highly capable models at competitive prices. Their API is OpenAI-compatible, making integration straightforward.</p> <p>Key features:</p> <ul> <li>DeepSeek-V3: General purpose chat model, excellent performance</li> <li>DeepSeek-R1: Advanced reasoning model, rivals OpenAI o1 at much lower cost</li> <li>Very competitive pricing (~$0.14/1M input for chat)</li> <li>OpenAI-compatible API</li> </ul>"},{"location":"api/clients/deepseek/#configuration","title":"Configuration","text":"<p>The client requires a <code>DEEPSEEK_API_KEY</code> environment variable:</p> <pre><code># Linux/macOS\nexport DEEPSEEK_API_KEY=\"your-api-key\"\n\n# Windows PowerShell\n$env:DEEPSEEK_API_KEY=\"your-api-key\"\n\n# Windows cmd\nset DEEPSEEK_API_KEY=your-api-key\n</code></pre> <p>Get your API key from: https://platform.deepseek.com</p>"},{"location":"api/clients/deepseek/#usage","title":"Usage","text":""},{"location":"api/clients/deepseek/#basic-usage","title":"Basic Usage","text":"<pre><code>from causaliq_knowledge.llm import DeepSeekClient, DeepSeekConfig\n\n# Default config (uses DEEPSEEK_API_KEY env var)\nclient = DeepSeekClient()\n\n# Or with custom config\nconfig = DeepSeekConfig(\n    model=\"deepseek-chat\",\n    temperature=0.1,\n    max_tokens=500,\n    timeout=30.0,\n)\nclient = DeepSeekClient(config)\n\n# Make a completion request\nmessages = [{\"role\": \"user\", \"content\": \"What is 2 + 2?\"}]\nresponse = client.completion(messages)\nprint(response.content)\n</code></pre>"},{"location":"api/clients/deepseek/#using-with-cli","title":"Using with CLI","text":"<pre><code># Query with DeepSeek\ncqknow query smoking lung_cancer --model deepseek/deepseek-chat\n\n# Use reasoning model for complex queries\ncqknow query income education --model deepseek/deepseek-reasoner --domain economics\n\n# List available DeepSeek models\ncqknow models deepseek\n</code></pre>"},{"location":"api/clients/deepseek/#using-with-llmknowledge-provider","title":"Using with LLMKnowledge Provider","text":"<pre><code>from causaliq_knowledge.llm import LLMKnowledge\n\n# Single model\nprovider = LLMKnowledge(models=[\"deepseek/deepseek-chat\"])\nresult = provider.query_edge(\"smoking\", \"lung_cancer\")\n\n# Multi-model consensus\nprovider = LLMKnowledge(\n    models=[\n        \"deepseek/deepseek-chat\",\n        \"groq/llama-3.1-8b-instant\",\n    ],\n    consensus_strategy=\"weighted_vote\",\n)\n</code></pre>"},{"location":"api/clients/deepseek/#available-models","title":"Available Models","text":"Model Description Best For <code>deepseek-chat</code> DeepSeek-V3 general purpose Fast, general queries <code>deepseek-reasoner</code> DeepSeek-R1 reasoning model Complex reasoning tasks"},{"location":"api/clients/deepseek/#pricing","title":"Pricing","text":"<p>DeepSeek offers very competitive pricing (as of Jan 2025):</p> Model Input (per 1M tokens) Output (per 1M tokens) deepseek-chat $0.14 $0.28 deepseek-reasoner $0.55 $2.19 <p>Note: Cache hits are even cheaper. See DeepSeek pricing for details.</p>"},{"location":"api/clients/deepseek/#api-reference","title":"API Reference","text":""},{"location":"api/clients/deepseek/#causaliq_knowledge.llm.deepseek_client.DeepSeekConfig","title":"DeepSeekConfig  <code>dataclass</code>","text":"<pre><code>DeepSeekConfig(\n    model: str = \"deepseek-chat\",\n    temperature: float = 0.1,\n    max_tokens: int = 500,\n    timeout: float = 30.0,\n    api_key: Optional[str] = None,\n)\n</code></pre> <p>Configuration for DeepSeek API client.</p> <p>Extends OpenAICompatConfig with DeepSeek-specific defaults.</p> <p>Attributes:</p> <ul> <li> <code>model</code>               (<code>str</code>)           \u2013            <p>DeepSeek model identifier (default: deepseek-chat).</p> </li> <li> <code>temperature</code>               (<code>float</code>)           \u2013            <p>Sampling temperature (default: 0.1).</p> </li> <li> <code>max_tokens</code>               (<code>int</code>)           \u2013            <p>Maximum response tokens (default: 500).</p> </li> <li> <code>timeout</code>               (<code>float</code>)           \u2013            <p>Request timeout in seconds (default: 30.0).</p> </li> <li> <code>api_key</code>               (<code>Optional[str]</code>)           \u2013            <p>DeepSeek API key (falls back to DEEPSEEK_API_KEY env var).</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>__post_init__</code>             \u2013              <p>Set API key from environment if not provided.</p> </li> </ul>"},{"location":"api/clients/deepseek/#causaliq_knowledge.llm.deepseek_client.DeepSeekConfig.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__() -&gt; None\n</code></pre> <p>Set API key from environment if not provided.</p>"},{"location":"api/clients/deepseek/#causaliq_knowledge.llm.deepseek_client.DeepSeekClient","title":"DeepSeekClient","text":"<pre><code>DeepSeekClient(config: Optional[DeepSeekConfig] = None)\n</code></pre> <p>Direct DeepSeek API client.</p> <p>DeepSeek uses an OpenAI-compatible API, making integration straightforward. Known for excellent reasoning capabilities (R1) at low cost.</p> Available models <ul> <li>deepseek-chat: General purpose (DeepSeek-V3)</li> <li>deepseek-reasoner: Advanced reasoning (DeepSeek-R1)</li> </ul> Example <p>config = DeepSeekConfig(model=\"deepseek-chat\") client = DeepSeekClient(config) msgs = [{\"role\": \"user\", \"content\": \"Hello\"}] response = client.completion(msgs) print(response.content)</p> <p>Parameters:</p> <ul> <li> </li> </ul> <p>Methods:</p> <ul> <li> <code>complete_json</code>             \u2013              <p>Make a completion request and parse response as JSON.</p> </li> <li> <code>completion</code>             \u2013              <p>Make a chat completion request.</p> </li> <li> <code>is_available</code>             \u2013              <p>Check if the API is available.</p> </li> <li> <code>list_models</code>             \u2013              <p>List available models from the API.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>call_count</code>               (<code>int</code>)           \u2013            <p>Return the number of API calls made.</p> </li> <li> <code>model_name</code>               (<code>str</code>)           \u2013            <p>Return the model name being used.</p> </li> <li> <code>provider_name</code>               (<code>str</code>)           \u2013            <p>Return the provider name.</p> </li> </ul>"},{"location":"api/clients/deepseek/#causaliq_knowledge.llm.deepseek_client.DeepSeekClient(config)","title":"<code>config</code>","text":"(<code>Optional[DeepSeekConfig]</code>, default:                   <code>None</code> )           \u2013            <p>DeepSeek configuration. If None, uses defaults with    API key from DEEPSEEK_API_KEY environment variable.</p>"},{"location":"api/clients/deepseek/#causaliq_knowledge.llm.deepseek_client.DeepSeekClient.call_count","title":"call_count  <code>property</code>","text":"<pre><code>call_count: int\n</code></pre> <p>Return the number of API calls made.</p>"},{"location":"api/clients/deepseek/#causaliq_knowledge.llm.deepseek_client.DeepSeekClient.model_name","title":"model_name  <code>property</code>","text":"<pre><code>model_name: str\n</code></pre> <p>Return the model name being used.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>Model identifier string.</p> </li> </ul>"},{"location":"api/clients/deepseek/#causaliq_knowledge.llm.deepseek_client.DeepSeekClient.provider_name","title":"provider_name  <code>property</code>","text":"<pre><code>provider_name: str\n</code></pre> <p>Return the provider name.</p>"},{"location":"api/clients/deepseek/#causaliq_knowledge.llm.deepseek_client.DeepSeekClient.complete_json","title":"complete_json","text":"<pre><code>complete_json(\n    messages: List[Dict[str, str]], **kwargs: Any\n) -&gt; tuple[Optional[Dict[str, Any]], LLMResponse]\n</code></pre> <p>Make a completion request and parse response as JSON.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>tuple[Optional[Dict[str, Any]], LLMResponse]</code>           \u2013            <p>Tuple of (parsed JSON dict or None, raw LLMResponse).</p> </li> </ul>"},{"location":"api/clients/deepseek/#causaliq_knowledge.llm.deepseek_client.DeepSeekClient.complete_json(messages)","title":"<code>messages</code>","text":"(<code>List[Dict[str, str]]</code>)           \u2013            <p>List of message dicts with \"role\" and \"content\" keys.</p>"},{"location":"api/clients/deepseek/#causaliq_knowledge.llm.deepseek_client.DeepSeekClient.complete_json(**kwargs)","title":"<code>**kwargs</code>","text":"(<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Override config options passed to completion().</p>"},{"location":"api/clients/deepseek/#causaliq_knowledge.llm.deepseek_client.DeepSeekClient.completion","title":"completion","text":"<pre><code>completion(messages: List[Dict[str, str]], **kwargs: Any) -&gt; LLMResponse\n</code></pre> <p>Make a chat completion request.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>LLMResponse</code>           \u2013            <p>LLMResponse with the generated content and metadata.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the API request fails.</p> </li> </ul>"},{"location":"api/clients/deepseek/#causaliq_knowledge.llm.deepseek_client.DeepSeekClient.completion(messages)","title":"<code>messages</code>","text":"(<code>List[Dict[str, str]]</code>)           \u2013            <p>List of message dicts with \"role\" and \"content\" keys.</p>"},{"location":"api/clients/deepseek/#causaliq_knowledge.llm.deepseek_client.DeepSeekClient.completion(**kwargs)","title":"<code>**kwargs</code>","text":"(<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Override config options (temperature, max_tokens).</p>"},{"location":"api/clients/deepseek/#causaliq_knowledge.llm.deepseek_client.DeepSeekClient.is_available","title":"is_available","text":"<pre><code>is_available() -&gt; bool\n</code></pre> <p>Check if the API is available.</p> <p>Returns:</p> <ul> <li> <code>bool</code>           \u2013            <p>True if API key is configured.</p> </li> </ul>"},{"location":"api/clients/deepseek/#causaliq_knowledge.llm.deepseek_client.DeepSeekClient.list_models","title":"list_models","text":"<pre><code>list_models() -&gt; List[str]\n</code></pre> <p>List available models from the API.</p> <p>Queries the API to get models accessible with the current API key, then filters using _filter_models().</p> <p>Returns:</p> <ul> <li> <code>List[str]</code>           \u2013            <p>List of model identifiers.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the API request fails.</p> </li> </ul>"},{"location":"api/clients/gemini/","title":"Gemini Client API Reference","text":"<p>Direct Google Gemini API client. This client implements the BaseLLMClient interface using httpx to communicate directly with Google's Generative Language API.</p>"},{"location":"api/clients/gemini/#overview","title":"Overview","text":"<p>The Gemini client provides:</p> <ul> <li>Direct HTTP communication with Google's Generative Language API</li> <li>Implements the <code>BaseLLMClient</code> abstract interface</li> <li>Automatic conversion from OpenAI-style messages to Gemini format</li> <li>JSON response parsing with error handling</li> <li>Call counting for usage tracking</li> <li>Configurable timeout settings</li> </ul>"},{"location":"api/clients/gemini/#usage","title":"Usage","text":"<pre><code>from causaliq_knowledge.llm import GeminiClient, GeminiConfig\n\n# Create client with custom config\nconfig = GeminiConfig(\n    model=\"gemini-2.5-flash\",\n    temperature=0.1,\n    max_tokens=500,\n)\nclient = GeminiClient(config=config)\n\n# Make a completion request (OpenAI-style messages)\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"What is 2+2?\"},\n]\nresponse = client.completion(messages)\nprint(response.content)\n\n# Parse JSON response\njson_data = response.parse_json()\n</code></pre>"},{"location":"api/clients/gemini/#environment-variables","title":"Environment Variables","text":"<p>The Gemini client requires the <code>GEMINI_API_KEY</code> environment variable to be set:</p> <pre><code>export GEMINI_API_KEY=your_api_key_here\n</code></pre>"},{"location":"api/clients/gemini/#geminiconfig","title":"GeminiConfig","text":""},{"location":"api/clients/gemini/#causaliq_knowledge.llm.gemini_client.GeminiConfig","title":"GeminiConfig  <code>dataclass</code>","text":"<pre><code>GeminiConfig(\n    model: str = \"gemini-2.5-flash\",\n    temperature: float = 0.1,\n    max_tokens: int = 500,\n    timeout: float = 30.0,\n    api_key: Optional[str] = None,\n)\n</code></pre> <p>Configuration for Gemini API client.</p> <p>Extends LLMConfig with Gemini-specific defaults.</p> <p>Attributes:</p> <ul> <li> <code>model</code>               (<code>str</code>)           \u2013            <p>Gemini model identifier (default: gemini-2.5-flash).</p> </li> <li> <code>temperature</code>               (<code>float</code>)           \u2013            <p>Sampling temperature (default: 0.1).</p> </li> <li> <code>max_tokens</code>               (<code>int</code>)           \u2013            <p>Maximum response tokens (default: 500).</p> </li> <li> <code>timeout</code>               (<code>float</code>)           \u2013            <p>Request timeout in seconds (default: 30.0).</p> </li> <li> <code>api_key</code>               (<code>Optional[str]</code>)           \u2013            <p>Gemini API key (falls back to GEMINI_API_KEY env var).</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>__post_init__</code>             \u2013              <p>Set API key from environment if not provided.</p> </li> </ul>"},{"location":"api/clients/gemini/#causaliq_knowledge.llm.gemini_client.GeminiConfig.api_key","title":"api_key  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>api_key: Optional[str] = None\n</code></pre>"},{"location":"api/clients/gemini/#causaliq_knowledge.llm.gemini_client.GeminiConfig.max_tokens","title":"max_tokens  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_tokens: int = 500\n</code></pre>"},{"location":"api/clients/gemini/#causaliq_knowledge.llm.gemini_client.GeminiConfig.model","title":"model  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model: str = 'gemini-2.5-flash'\n</code></pre>"},{"location":"api/clients/gemini/#causaliq_knowledge.llm.gemini_client.GeminiConfig.temperature","title":"temperature  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>temperature: float = 0.1\n</code></pre>"},{"location":"api/clients/gemini/#causaliq_knowledge.llm.gemini_client.GeminiConfig.timeout","title":"timeout  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>timeout: float = 30.0\n</code></pre>"},{"location":"api/clients/gemini/#causaliq_knowledge.llm.gemini_client.GeminiConfig.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__() -&gt; None\n</code></pre> <p>Set API key from environment if not provided.</p>"},{"location":"api/clients/gemini/#geminiclient","title":"GeminiClient","text":""},{"location":"api/clients/gemini/#causaliq_knowledge.llm.gemini_client.GeminiClient","title":"GeminiClient","text":"<pre><code>GeminiClient(config: Optional[GeminiConfig] = None)\n</code></pre> <p>Direct Gemini API client.</p> <p>Implements the BaseLLMClient interface for Google's Gemini API. Uses httpx for HTTP requests.</p> Example <p>config = GeminiConfig(model=\"gemini-2.5-flash\") client = GeminiClient(config) msgs = [{\"role\": \"user\", \"content\": \"Hello\"}] response = client.completion(msgs) print(response.content)</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>complete_json</code>             \u2013              <p>Make a completion request and parse response as JSON.</p> </li> <li> <code>completion</code>             \u2013              <p>Make a chat completion request to Gemini.</p> </li> <li> <code>is_available</code>             \u2013              <p>Check if Gemini API is available.</p> </li> <li> <code>list_models</code>             \u2013              <p>List available models from Gemini API.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>BASE_URL</code>           \u2013            </li> <li> <code>_total_calls</code>           \u2013            </li> <li> <code>call_count</code>               (<code>int</code>)           \u2013            <p>Return the number of API calls made.</p> </li> <li> <code>config</code>           \u2013            </li> <li> <code>model_name</code>               (<code>str</code>)           \u2013            <p>Return the model name being used.</p> </li> <li> <code>provider_name</code>               (<code>str</code>)           \u2013            <p>Return the provider name.</p> </li> </ul>"},{"location":"api/clients/gemini/#causaliq_knowledge.llm.gemini_client.GeminiClient(config)","title":"<code>config</code>","text":"(<code>Optional[GeminiConfig]</code>, default:                   <code>None</code> )           \u2013            <p>Gemini configuration. If None, uses defaults with    API key from GEMINI_API_KEY environment variable.</p>"},{"location":"api/clients/gemini/#causaliq_knowledge.llm.gemini_client.GeminiClient.BASE_URL","title":"BASE_URL  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>BASE_URL = 'https://generativelanguage.googleapis.com/v1beta/models'\n</code></pre>"},{"location":"api/clients/gemini/#causaliq_knowledge.llm.gemini_client.GeminiClient._total_calls","title":"_total_calls  <code>instance-attribute</code>","text":"<pre><code>_total_calls = 0\n</code></pre>"},{"location":"api/clients/gemini/#causaliq_knowledge.llm.gemini_client.GeminiClient.call_count","title":"call_count  <code>property</code>","text":"<pre><code>call_count: int\n</code></pre> <p>Return the number of API calls made.</p>"},{"location":"api/clients/gemini/#causaliq_knowledge.llm.gemini_client.GeminiClient.config","title":"config  <code>instance-attribute</code>","text":"<pre><code>config = config or GeminiConfig()\n</code></pre>"},{"location":"api/clients/gemini/#causaliq_knowledge.llm.gemini_client.GeminiClient.model_name","title":"model_name  <code>property</code>","text":"<pre><code>model_name: str\n</code></pre> <p>Return the model name being used.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>Model identifier string.</p> </li> </ul>"},{"location":"api/clients/gemini/#causaliq_knowledge.llm.gemini_client.GeminiClient.provider_name","title":"provider_name  <code>property</code>","text":"<pre><code>provider_name: str\n</code></pre> <p>Return the provider name.</p>"},{"location":"api/clients/gemini/#causaliq_knowledge.llm.gemini_client.GeminiClient.complete_json","title":"complete_json","text":"<pre><code>complete_json(\n    messages: List[Dict[str, str]], **kwargs: Any\n) -&gt; tuple[Optional[Dict[str, Any]], LLMResponse]\n</code></pre> <p>Make a completion request and parse response as JSON.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>tuple[Optional[Dict[str, Any]], LLMResponse]</code>           \u2013            <p>Tuple of (parsed JSON dict or None, raw LLMResponse).</p> </li> </ul>"},{"location":"api/clients/gemini/#causaliq_knowledge.llm.gemini_client.GeminiClient.complete_json(messages)","title":"<code>messages</code>","text":"(<code>List[Dict[str, str]]</code>)           \u2013            <p>List of message dicts with \"role\" and \"content\" keys.</p>"},{"location":"api/clients/gemini/#causaliq_knowledge.llm.gemini_client.GeminiClient.complete_json(**kwargs)","title":"<code>**kwargs</code>","text":"(<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Override config options passed to completion().</p>"},{"location":"api/clients/gemini/#causaliq_knowledge.llm.gemini_client.GeminiClient.completion","title":"completion","text":"<pre><code>completion(messages: List[Dict[str, str]], **kwargs: Any) -&gt; LLMResponse\n</code></pre> <p>Make a chat completion request to Gemini.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>LLMResponse</code>           \u2013            <p>LLMResponse with the generated content and metadata.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the API request fails.</p> </li> </ul>"},{"location":"api/clients/gemini/#causaliq_knowledge.llm.gemini_client.GeminiClient.completion(messages)","title":"<code>messages</code>","text":"(<code>List[Dict[str, str]]</code>)           \u2013            <p>List of message dicts with \"role\" and \"content\" keys.</p>"},{"location":"api/clients/gemini/#causaliq_knowledge.llm.gemini_client.GeminiClient.completion(**kwargs)","title":"<code>**kwargs</code>","text":"(<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Override config options (temperature, max_tokens).</p>"},{"location":"api/clients/gemini/#causaliq_knowledge.llm.gemini_client.GeminiClient.is_available","title":"is_available","text":"<pre><code>is_available() -&gt; bool\n</code></pre> <p>Check if Gemini API is available.</p> <p>Returns:</p> <ul> <li> <code>bool</code>           \u2013            <p>True if GEMINI_API_KEY is configured.</p> </li> </ul>"},{"location":"api/clients/gemini/#causaliq_knowledge.llm.gemini_client.GeminiClient.list_models","title":"list_models","text":"<pre><code>list_models() -&gt; List[str]\n</code></pre> <p>List available models from Gemini API.</p> <p>Queries the Gemini API to get models accessible with the current API key. Filters to only include models that support generateContent.</p> <p>Returns:</p> <ul> <li> <code>List[str]</code>           \u2013            <p>List of model identifiers (e.g., ['gemini-2.5-flash', ...]).</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the API request fails.</p> </li> </ul>"},{"location":"api/clients/gemini/#message-format-conversion","title":"Message Format Conversion","text":"<p>The client automatically converts OpenAI-style messages to Gemini's format:</p> OpenAI Role Gemini Role <code>system</code> System instruction (separate field) <code>user</code> <code>user</code> <code>assistant</code> <code>model</code>"},{"location":"api/clients/gemini/#supported-models","title":"Supported Models","text":"<p>Google Gemini provides a generous free tier:</p> Model Description Free Tier <code>gemini-2.5-flash</code> Fast and efficient \u2705 Yes <code>gemini-2.5-pro</code> Most capable \u2705 Limited <code>gemini-1.5-flash</code> Previous generation \u2705 Yes <p>See Google AI documentation for the full list of available models.</p>"},{"location":"api/clients/groq/","title":"Groq Client API Reference","text":"<p>Direct Groq API client for fast LLM inference. This client implements the BaseLLMClient interface using httpx to communicate directly with the Groq API.</p>"},{"location":"api/clients/groq/#overview","title":"Overview","text":"<p>The Groq client provides:</p> <ul> <li>Direct HTTP communication with Groq's API</li> <li>Implements the <code>BaseLLMClient</code> abstract interface</li> <li>JSON response parsing with error handling</li> <li>Call counting for usage tracking</li> <li>Configurable timeout and retry settings</li> </ul>"},{"location":"api/clients/groq/#usage","title":"Usage","text":"<pre><code>from causaliq_knowledge.llm import GroqClient, GroqConfig\n\n# Create client with custom config\nconfig = GroqConfig(\n    model=\"llama-3.1-8b-instant\",\n    temperature=0.1,\n    max_tokens=500,\n)\nclient = GroqClient(config=config)\n\n# Make a completion request\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"What is 2+2?\"},\n]\nresponse = client.completion(messages)\nprint(response.content)\n\n# Parse JSON response\njson_data = response.parse_json()\n</code></pre>"},{"location":"api/clients/groq/#environment-variables","title":"Environment Variables","text":"<p>The Groq client requires the <code>GROQ_API_KEY</code> environment variable to be set:</p> <pre><code>export GROQ_API_KEY=your_api_key_here\n</code></pre>"},{"location":"api/clients/groq/#groqconfig","title":"GroqConfig","text":""},{"location":"api/clients/groq/#causaliq_knowledge.llm.groq_client.GroqConfig","title":"GroqConfig  <code>dataclass</code>","text":"<pre><code>GroqConfig(\n    model: str = \"llama-3.1-8b-instant\",\n    temperature: float = 0.1,\n    max_tokens: int = 500,\n    timeout: float = 30.0,\n    api_key: Optional[str] = None,\n)\n</code></pre> <p>Configuration for Groq API client.</p> <p>Extends LLMConfig with Groq-specific defaults.</p> <p>Attributes:</p> <ul> <li> <code>model</code>               (<code>str</code>)           \u2013            <p>Groq model identifier (default: llama-3.1-8b-instant).</p> </li> <li> <code>temperature</code>               (<code>float</code>)           \u2013            <p>Sampling temperature (default: 0.1).</p> </li> <li> <code>max_tokens</code>               (<code>int</code>)           \u2013            <p>Maximum response tokens (default: 500).</p> </li> <li> <code>timeout</code>               (<code>float</code>)           \u2013            <p>Request timeout in seconds (default: 30.0).</p> </li> <li> <code>api_key</code>               (<code>Optional[str]</code>)           \u2013            <p>Groq API key (falls back to GROQ_API_KEY env var).</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>__post_init__</code>             \u2013              <p>Set API key from environment if not provided.</p> </li> </ul>"},{"location":"api/clients/groq/#causaliq_knowledge.llm.groq_client.GroqConfig.api_key","title":"api_key  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>api_key: Optional[str] = None\n</code></pre>"},{"location":"api/clients/groq/#causaliq_knowledge.llm.groq_client.GroqConfig.max_tokens","title":"max_tokens  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_tokens: int = 500\n</code></pre>"},{"location":"api/clients/groq/#causaliq_knowledge.llm.groq_client.GroqConfig.model","title":"model  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model: str = 'llama-3.1-8b-instant'\n</code></pre>"},{"location":"api/clients/groq/#causaliq_knowledge.llm.groq_client.GroqConfig.temperature","title":"temperature  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>temperature: float = 0.1\n</code></pre>"},{"location":"api/clients/groq/#causaliq_knowledge.llm.groq_client.GroqConfig.timeout","title":"timeout  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>timeout: float = 30.0\n</code></pre>"},{"location":"api/clients/groq/#causaliq_knowledge.llm.groq_client.GroqConfig.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__() -&gt; None\n</code></pre> <p>Set API key from environment if not provided.</p>"},{"location":"api/clients/groq/#groqclient","title":"GroqClient","text":""},{"location":"api/clients/groq/#causaliq_knowledge.llm.groq_client.GroqClient","title":"GroqClient","text":"<pre><code>GroqClient(config: Optional[GroqConfig] = None)\n</code></pre> <p>Direct Groq API client.</p> <p>Implements the BaseLLMClient interface for Groq's API. Uses httpx for HTTP requests.</p> Example <p>config = GroqConfig(model=\"llama-3.1-8b-instant\") client = GroqClient(config) msgs = [{\"role\": \"user\", \"content\": \"Hello\"}] response = client.completion(msgs) print(response.content)</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>complete_json</code>             \u2013              <p>Make a completion request and parse response as JSON.</p> </li> <li> <code>completion</code>             \u2013              <p>Make a chat completion request to Groq.</p> </li> <li> <code>is_available</code>             \u2013              <p>Check if Groq API is available.</p> </li> <li> <code>list_models</code>             \u2013              <p>List available models from Groq API.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>BASE_URL</code>           \u2013            </li> <li> <code>_total_calls</code>           \u2013            </li> <li> <code>call_count</code>               (<code>int</code>)           \u2013            <p>Return the number of API calls made.</p> </li> <li> <code>config</code>           \u2013            </li> <li> <code>model_name</code>               (<code>str</code>)           \u2013            <p>Return the model name being used.</p> </li> <li> <code>provider_name</code>               (<code>str</code>)           \u2013            <p>Return the provider name.</p> </li> </ul>"},{"location":"api/clients/groq/#causaliq_knowledge.llm.groq_client.GroqClient(config)","title":"<code>config</code>","text":"(<code>Optional[GroqConfig]</code>, default:                   <code>None</code> )           \u2013            <p>Groq configuration. If None, uses defaults with    API key from GROQ_API_KEY environment variable.</p>"},{"location":"api/clients/groq/#causaliq_knowledge.llm.groq_client.GroqClient.BASE_URL","title":"BASE_URL  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>BASE_URL = 'https://api.groq.com/openai/v1'\n</code></pre>"},{"location":"api/clients/groq/#causaliq_knowledge.llm.groq_client.GroqClient._total_calls","title":"_total_calls  <code>instance-attribute</code>","text":"<pre><code>_total_calls = 0\n</code></pre>"},{"location":"api/clients/groq/#causaliq_knowledge.llm.groq_client.GroqClient.call_count","title":"call_count  <code>property</code>","text":"<pre><code>call_count: int\n</code></pre> <p>Return the number of API calls made.</p>"},{"location":"api/clients/groq/#causaliq_knowledge.llm.groq_client.GroqClient.config","title":"config  <code>instance-attribute</code>","text":"<pre><code>config = config or GroqConfig()\n</code></pre>"},{"location":"api/clients/groq/#causaliq_knowledge.llm.groq_client.GroqClient.model_name","title":"model_name  <code>property</code>","text":"<pre><code>model_name: str\n</code></pre> <p>Return the model name being used.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>Model identifier string.</p> </li> </ul>"},{"location":"api/clients/groq/#causaliq_knowledge.llm.groq_client.GroqClient.provider_name","title":"provider_name  <code>property</code>","text":"<pre><code>provider_name: str\n</code></pre> <p>Return the provider name.</p>"},{"location":"api/clients/groq/#causaliq_knowledge.llm.groq_client.GroqClient.complete_json","title":"complete_json","text":"<pre><code>complete_json(\n    messages: List[Dict[str, str]], **kwargs: Any\n) -&gt; tuple[Optional[Dict[str, Any]], LLMResponse]\n</code></pre> <p>Make a completion request and parse response as JSON.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>tuple[Optional[Dict[str, Any]], LLMResponse]</code>           \u2013            <p>Tuple of (parsed JSON dict or None, raw LLMResponse).</p> </li> </ul>"},{"location":"api/clients/groq/#causaliq_knowledge.llm.groq_client.GroqClient.complete_json(messages)","title":"<code>messages</code>","text":"(<code>List[Dict[str, str]]</code>)           \u2013            <p>List of message dicts with \"role\" and \"content\" keys.</p>"},{"location":"api/clients/groq/#causaliq_knowledge.llm.groq_client.GroqClient.complete_json(**kwargs)","title":"<code>**kwargs</code>","text":"(<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Override config options passed to completion().</p>"},{"location":"api/clients/groq/#causaliq_knowledge.llm.groq_client.GroqClient.completion","title":"completion","text":"<pre><code>completion(messages: List[Dict[str, str]], **kwargs: Any) -&gt; LLMResponse\n</code></pre> <p>Make a chat completion request to Groq.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>LLMResponse</code>           \u2013            <p>LLMResponse with the generated content and metadata.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the API request fails.</p> </li> </ul>"},{"location":"api/clients/groq/#causaliq_knowledge.llm.groq_client.GroqClient.completion(messages)","title":"<code>messages</code>","text":"(<code>List[Dict[str, str]]</code>)           \u2013            <p>List of message dicts with \"role\" and \"content\" keys.</p>"},{"location":"api/clients/groq/#causaliq_knowledge.llm.groq_client.GroqClient.completion(**kwargs)","title":"<code>**kwargs</code>","text":"(<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Override config options (temperature, max_tokens).</p>"},{"location":"api/clients/groq/#causaliq_knowledge.llm.groq_client.GroqClient.is_available","title":"is_available","text":"<pre><code>is_available() -&gt; bool\n</code></pre> <p>Check if Groq API is available.</p> <p>Returns:</p> <ul> <li> <code>bool</code>           \u2013            <p>True if GROQ_API_KEY is configured.</p> </li> </ul>"},{"location":"api/clients/groq/#causaliq_knowledge.llm.groq_client.GroqClient.list_models","title":"list_models","text":"<pre><code>list_models() -&gt; List[str]\n</code></pre> <p>List available models from Groq API.</p> <p>Queries the Groq API to get models accessible with the current API key. Filters to only include text generation models.</p> <p>Returns:</p> <ul> <li> <code>List[str]</code>           \u2013            <p>List of model identifiers (e.g., ['llama-3.1-8b-instant', ...]).</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the API request fails.</p> </li> </ul>"},{"location":"api/clients/groq/#supported-models","title":"Supported Models","text":"<p>Groq provides fast inference for open-source models:</p> Model Description Free Tier <code>llama-3.1-8b-instant</code> Fast Llama 3.1 8B model \u2705 Yes <code>llama-3.1-70b-versatile</code> Larger Llama 3.1 model \u2705 Yes <code>mixtral-8x7b-32768</code> Mixtral MoE model \u2705 Yes <p>See Groq documentation for the full list of available models.</p>"},{"location":"api/clients/mistral/","title":"Mistral Client","text":"<p>Direct Mistral AI API client for Mistral models.</p>"},{"location":"api/clients/mistral/#overview","title":"Overview","text":"<p>Mistral AI is a French AI company known for high-quality open-weight and proprietary models. Their API is OpenAI-compatible, making integration straightforward.</p> <p>Key features:</p> <ul> <li>Mistral Small: Fast, cost-effective for simple tasks</li> <li>Mistral Large: Most capable, best for complex reasoning</li> <li>Codestral: Optimized for code generation</li> <li>Strong EU-based option for data sovereignty</li> <li>OpenAI-compatible API</li> </ul>"},{"location":"api/clients/mistral/#configuration","title":"Configuration","text":"<p>The client requires a <code>MISTRAL_API_KEY</code> environment variable:</p> <pre><code># Linux/macOS\nexport MISTRAL_API_KEY=\"your-api-key\"\n\n# Windows PowerShell\n$env:MISTRAL_API_KEY=\"your-api-key\"\n\n# Windows cmd\nset MISTRAL_API_KEY=your-api-key\n</code></pre> <p>Get your API key from: https://console.mistral.ai</p>"},{"location":"api/clients/mistral/#usage","title":"Usage","text":""},{"location":"api/clients/mistral/#basic-usage","title":"Basic Usage","text":"<pre><code>from causaliq_knowledge.llm import MistralClient, MistralConfig\n\n# Default config (uses MISTRAL_API_KEY env var)\nclient = MistralClient()\n\n# Or with custom config\nconfig = MistralConfig(\n    model=\"mistral-small-latest\",\n    temperature=0.1,\n    max_tokens=500,\n    timeout=30.0,\n)\nclient = MistralClient(config)\n\n# Make a completion request\nmessages = [{\"role\": \"user\", \"content\": \"What is 2 + 2?\"}]\nresponse = client.completion(messages)\nprint(response.content)\n</code></pre>"},{"location":"api/clients/mistral/#using-with-cli","title":"Using with CLI","text":"<pre><code># Query with Mistral\ncqknow query smoking lung_cancer --model mistral/mistral-small-latest\n\n# Use large model for complex queries\ncqknow query income education --model mistral/mistral-large-latest --domain economics\n\n# List available Mistral models\ncqknow models mistral\n</code></pre>"},{"location":"api/clients/mistral/#using-with-llmknowledge-provider","title":"Using with LLMKnowledge Provider","text":"<pre><code>from causaliq_knowledge.llm import LLMKnowledge\n\n# Single model\nprovider = LLMKnowledge(models=[\"mistral/mistral-small-latest\"])\nresult = provider.query_edge(\"smoking\", \"lung_cancer\")\n\n# Multi-model consensus\nprovider = LLMKnowledge(\n    models=[\n        \"mistral/mistral-large-latest\",\n        \"groq/llama-3.1-8b-instant\",\n    ],\n    consensus_strategy=\"weighted_vote\",\n)\n</code></pre>"},{"location":"api/clients/mistral/#available-models","title":"Available Models","text":"Model Description Best For <code>mistral-small-latest</code> Fast, cost-effective Simple tasks <code>mistral-medium-latest</code> Balanced performance General use <code>mistral-large-latest</code> Most capable Complex reasoning <code>codestral-latest</code> Code-optimized Programming tasks <code>open-mistral-nemo</code> 12B open model Budget-friendly <code>open-mixtral-8x7b</code> MoE open model Balanced open model <code>ministral-3b-latest</code> Ultra-small Edge deployment <code>ministral-8b-latest</code> Small Resource-constrained"},{"location":"api/clients/mistral/#pricing","title":"Pricing","text":"<p>Mistral AI offers competitive pricing (as of Jan 2025):</p> Model Input (per 1M tokens) Output (per 1M tokens) mistral-small $0.20 $0.60 mistral-medium $2.70 $8.10 mistral-large $2.00 $6.00 codestral $0.20 $0.60 open-mistral-nemo $0.15 $0.15 ministral-3b $0.04 $0.04 ministral-8b $0.10 $0.10 <p>See Mistral pricing for details.</p>"},{"location":"api/clients/mistral/#api-reference","title":"API Reference","text":""},{"location":"api/clients/mistral/#causaliq_knowledge.llm.mistral_client.MistralConfig","title":"MistralConfig  <code>dataclass</code>","text":"<pre><code>MistralConfig(\n    model: str = \"mistral-small-latest\",\n    temperature: float = 0.1,\n    max_tokens: int = 500,\n    timeout: float = 30.0,\n    api_key: Optional[str] = None,\n)\n</code></pre> <p>Configuration for Mistral AI API client.</p> <p>Extends OpenAICompatConfig with Mistral-specific defaults.</p> <p>Attributes:</p> <ul> <li> <code>model</code>               (<code>str</code>)           \u2013            <p>Mistral model identifier (default: mistral-small-latest).</p> </li> <li> <code>temperature</code>               (<code>float</code>)           \u2013            <p>Sampling temperature (default: 0.1).</p> </li> <li> <code>max_tokens</code>               (<code>int</code>)           \u2013            <p>Maximum response tokens (default: 500).</p> </li> <li> <code>timeout</code>               (<code>float</code>)           \u2013            <p>Request timeout in seconds (default: 30.0).</p> </li> <li> <code>api_key</code>               (<code>Optional[str]</code>)           \u2013            <p>Mistral API key (falls back to MISTRAL_API_KEY env var).</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>__post_init__</code>             \u2013              <p>Set API key from environment if not provided.</p> </li> </ul>"},{"location":"api/clients/mistral/#causaliq_knowledge.llm.mistral_client.MistralConfig.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__() -&gt; None\n</code></pre> <p>Set API key from environment if not provided.</p>"},{"location":"api/clients/mistral/#causaliq_knowledge.llm.mistral_client.MistralClient","title":"MistralClient","text":"<pre><code>MistralClient(config: Optional[MistralConfig] = None)\n</code></pre> <p>Direct Mistral AI API client.</p> <p>Mistral AI is a French company providing high-quality LLMs with an OpenAI-compatible API.</p> Available models <ul> <li>mistral-small-latest: Fast, cost-effective</li> <li>mistral-medium-latest: Balanced performance</li> <li>mistral-large-latest: Most capable</li> <li>codestral-latest: Optimized for code</li> </ul> Example <p>config = MistralConfig(model=\"mistral-small-latest\") client = MistralClient(config) msgs = [{\"role\": \"user\", \"content\": \"Hello\"}] response = client.completion(msgs) print(response.content)</p> <p>Parameters:</p> <ul> <li> </li> </ul> <p>Methods:</p> <ul> <li> <code>complete_json</code>             \u2013              <p>Make a completion request and parse response as JSON.</p> </li> <li> <code>completion</code>             \u2013              <p>Make a chat completion request.</p> </li> <li> <code>is_available</code>             \u2013              <p>Check if the API is available.</p> </li> <li> <code>list_models</code>             \u2013              <p>List available models from the API.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>call_count</code>               (<code>int</code>)           \u2013            <p>Return the number of API calls made.</p> </li> <li> <code>model_name</code>               (<code>str</code>)           \u2013            <p>Return the model name being used.</p> </li> <li> <code>provider_name</code>               (<code>str</code>)           \u2013            <p>Return the provider name.</p> </li> </ul>"},{"location":"api/clients/mistral/#causaliq_knowledge.llm.mistral_client.MistralClient(config)","title":"<code>config</code>","text":"(<code>Optional[MistralConfig]</code>, default:                   <code>None</code> )           \u2013            <p>Mistral configuration. If None, uses defaults with    API key from MISTRAL_API_KEY environment variable.</p>"},{"location":"api/clients/mistral/#causaliq_knowledge.llm.mistral_client.MistralClient.call_count","title":"call_count  <code>property</code>","text":"<pre><code>call_count: int\n</code></pre> <p>Return the number of API calls made.</p>"},{"location":"api/clients/mistral/#causaliq_knowledge.llm.mistral_client.MistralClient.model_name","title":"model_name  <code>property</code>","text":"<pre><code>model_name: str\n</code></pre> <p>Return the model name being used.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>Model identifier string.</p> </li> </ul>"},{"location":"api/clients/mistral/#causaliq_knowledge.llm.mistral_client.MistralClient.provider_name","title":"provider_name  <code>property</code>","text":"<pre><code>provider_name: str\n</code></pre> <p>Return the provider name.</p>"},{"location":"api/clients/mistral/#causaliq_knowledge.llm.mistral_client.MistralClient.complete_json","title":"complete_json","text":"<pre><code>complete_json(\n    messages: List[Dict[str, str]], **kwargs: Any\n) -&gt; tuple[Optional[Dict[str, Any]], LLMResponse]\n</code></pre> <p>Make a completion request and parse response as JSON.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>tuple[Optional[Dict[str, Any]], LLMResponse]</code>           \u2013            <p>Tuple of (parsed JSON dict or None, raw LLMResponse).</p> </li> </ul>"},{"location":"api/clients/mistral/#causaliq_knowledge.llm.mistral_client.MistralClient.complete_json(messages)","title":"<code>messages</code>","text":"(<code>List[Dict[str, str]]</code>)           \u2013            <p>List of message dicts with \"role\" and \"content\" keys.</p>"},{"location":"api/clients/mistral/#causaliq_knowledge.llm.mistral_client.MistralClient.complete_json(**kwargs)","title":"<code>**kwargs</code>","text":"(<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Override config options passed to completion().</p>"},{"location":"api/clients/mistral/#causaliq_knowledge.llm.mistral_client.MistralClient.completion","title":"completion","text":"<pre><code>completion(messages: List[Dict[str, str]], **kwargs: Any) -&gt; LLMResponse\n</code></pre> <p>Make a chat completion request.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>LLMResponse</code>           \u2013            <p>LLMResponse with the generated content and metadata.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the API request fails.</p> </li> </ul>"},{"location":"api/clients/mistral/#causaliq_knowledge.llm.mistral_client.MistralClient.completion(messages)","title":"<code>messages</code>","text":"(<code>List[Dict[str, str]]</code>)           \u2013            <p>List of message dicts with \"role\" and \"content\" keys.</p>"},{"location":"api/clients/mistral/#causaliq_knowledge.llm.mistral_client.MistralClient.completion(**kwargs)","title":"<code>**kwargs</code>","text":"(<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Override config options (temperature, max_tokens).</p>"},{"location":"api/clients/mistral/#causaliq_knowledge.llm.mistral_client.MistralClient.is_available","title":"is_available","text":"<pre><code>is_available() -&gt; bool\n</code></pre> <p>Check if the API is available.</p> <p>Returns:</p> <ul> <li> <code>bool</code>           \u2013            <p>True if API key is configured.</p> </li> </ul>"},{"location":"api/clients/mistral/#causaliq_knowledge.llm.mistral_client.MistralClient.list_models","title":"list_models","text":"<pre><code>list_models() -&gt; List[str]\n</code></pre> <p>List available models from the API.</p> <p>Queries the API to get models accessible with the current API key, then filters using _filter_models().</p> <p>Returns:</p> <ul> <li> <code>List[str]</code>           \u2013            <p>List of model identifiers.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the API request fails.</p> </li> </ul>"},{"location":"api/clients/ollama/","title":"Ollama Client API Reference","text":"<p>Local Ollama API client for running Llama and other open-source models locally. This client implements the BaseLLMClient interface using httpx to communicate with a locally running Ollama server.</p>"},{"location":"api/clients/ollama/#overview","title":"Overview","text":"<p>The Ollama client provides:</p> <ul> <li>Local LLM inference without API keys or internet access</li> <li>Implements the <code>BaseLLMClient</code> abstract interface</li> <li>Support for Llama 3.2, Llama 3.1, Mistral, and other models</li> <li>JSON response parsing with error handling</li> <li>Call counting for usage tracking</li> <li>Availability checking via <code>is_available()</code> method</li> </ul>"},{"location":"api/clients/ollama/#prerequisites","title":"Prerequisites","text":"<ol> <li>Install Ollama from ollama.com/download</li> <li>Pull a model:    <pre><code>ollama pull llama3.2:1b    # Small, fast (~1.3GB)\nollama pull llama3.2       # Medium (~2GB)\nollama pull llama3.1:8b    # Larger, better quality (~4.7GB)\n</code></pre></li> <li>Ensure Ollama is running (it usually auto-starts after installation)</li> </ol>"},{"location":"api/clients/ollama/#usage","title":"Usage","text":"<pre><code>from causaliq_knowledge.llm import OllamaClient, OllamaConfig\n\n# Create client with default config (llama3.2:1b on localhost:11434)\nclient = OllamaClient()\n\n# Or with custom config\nconfig = OllamaConfig(\n    model=\"llama3.1:8b\",\n    temperature=0.1,\n    max_tokens=500,\n    timeout=120.0,  # Local inference can be slow\n)\nclient = OllamaClient(config=config)\n\n# Check if Ollama is available\nif client.is_available():\n    # Make a completion request\n    messages = [\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"What is 2+2?\"},\n    ]\n    response = client.completion(messages)\n    print(response.content)\nelse:\n    print(\"Ollama not running or model not installed\")\n</code></pre>"},{"location":"api/clients/ollama/#using-with-llmknowledge-provider","title":"Using with LLMKnowledge Provider","text":"<pre><code>from causaliq_knowledge.llm import LLMKnowledge\n\n# Use local Ollama for causal queries\nprovider = LLMKnowledge(models=[\"ollama/llama3.2:1b\"])\nresult = provider.query_edge(\"smoking\", \"lung_cancer\")\nprint(f\"Exists: {result.exists}, Confidence: {result.confidence}\")\n\n# Mix local and cloud models for consensus\nprovider = LLMKnowledge(\n    models=[\n        \"ollama/llama3.2:1b\",\n        \"groq/llama-3.1-8b-instant\",\n    ],\n    consensus_strategy=\"weighted_vote\"\n)\n</code></pre>"},{"location":"api/clients/ollama/#ollamaconfig","title":"OllamaConfig","text":""},{"location":"api/clients/ollama/#causaliq_knowledge.llm.ollama_client.OllamaConfig","title":"OllamaConfig  <code>dataclass</code>","text":"<pre><code>OllamaConfig(\n    model: str = \"llama3.2:1b\",\n    temperature: float = 0.1,\n    max_tokens: int = 500,\n    timeout: float = 120.0,\n    api_key: Optional[str] = None,\n    base_url: str = \"http://localhost:11434\",\n)\n</code></pre> <p>Configuration for Ollama API client.</p> <p>Extends LLMConfig with Ollama-specific defaults.</p> <p>Attributes:</p> <ul> <li> <code>model</code>               (<code>str</code>)           \u2013            <p>Ollama model identifier (default: llama3.2:1b).</p> </li> <li> <code>temperature</code>               (<code>float</code>)           \u2013            <p>Sampling temperature (default: 0.1).</p> </li> <li> <code>max_tokens</code>               (<code>int</code>)           \u2013            <p>Maximum response tokens (default: 500).</p> </li> <li> <code>timeout</code>               (<code>float</code>)           \u2013            <p>Request timeout in seconds (default: 120.0, local).</p> </li> <li> <code>api_key</code>               (<code>Optional[str]</code>)           \u2013            <p>Not used for Ollama (local server).</p> </li> <li> <code>base_url</code>               (<code>str</code>)           \u2013            <p>Ollama server URL (default: http://localhost:11434).</p> </li> </ul>"},{"location":"api/clients/ollama/#causaliq_knowledge.llm.ollama_client.OllamaConfig.api_key","title":"api_key  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>api_key: Optional[str] = None\n</code></pre>"},{"location":"api/clients/ollama/#causaliq_knowledge.llm.ollama_client.OllamaConfig.base_url","title":"base_url  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>base_url: str = 'http://localhost:11434'\n</code></pre>"},{"location":"api/clients/ollama/#causaliq_knowledge.llm.ollama_client.OllamaConfig.max_tokens","title":"max_tokens  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_tokens: int = 500\n</code></pre>"},{"location":"api/clients/ollama/#causaliq_knowledge.llm.ollama_client.OllamaConfig.model","title":"model  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model: str = 'llama3.2:1b'\n</code></pre>"},{"location":"api/clients/ollama/#causaliq_knowledge.llm.ollama_client.OllamaConfig.temperature","title":"temperature  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>temperature: float = 0.1\n</code></pre>"},{"location":"api/clients/ollama/#causaliq_knowledge.llm.ollama_client.OllamaConfig.timeout","title":"timeout  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>timeout: float = 120.0\n</code></pre>"},{"location":"api/clients/ollama/#ollamaclient","title":"OllamaClient","text":""},{"location":"api/clients/ollama/#causaliq_knowledge.llm.ollama_client.OllamaClient","title":"OllamaClient","text":"<pre><code>OllamaClient(config: Optional[OllamaConfig] = None)\n</code></pre> <p>Local Ollama API client.</p> <p>Implements the BaseLLMClient interface for locally running Ollama server. Uses httpx for HTTP requests to the local Ollama API.</p> <p>Ollama provides an OpenAI-compatible API for running open-source models like Llama locally without requiring API keys or internet access.</p> Example <p>config = OllamaConfig(model=\"llama3.2:1b\") client = OllamaClient(config) msgs = [{\"role\": \"user\", \"content\": \"Hello\"}] response = client.completion(msgs) print(response.content)</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>complete_json</code>             \u2013              <p>Make a completion request and parse response as JSON.</p> </li> <li> <code>completion</code>             \u2013              <p>Make a chat completion request to Ollama.</p> </li> <li> <code>is_available</code>             \u2013              <p>Check if Ollama server is running and model is available.</p> </li> <li> <code>list_models</code>             \u2013              <p>List installed models from Ollama.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>_total_calls</code>           \u2013            </li> <li> <code>call_count</code>               (<code>int</code>)           \u2013            <p>Return the number of API calls made.</p> </li> <li> <code>config</code>           \u2013            </li> <li> <code>model_name</code>               (<code>str</code>)           \u2013            <p>Return the model name being used.</p> </li> <li> <code>provider_name</code>               (<code>str</code>)           \u2013            <p>Return the provider name.</p> </li> </ul>"},{"location":"api/clients/ollama/#causaliq_knowledge.llm.ollama_client.OllamaClient(config)","title":"<code>config</code>","text":"(<code>Optional[OllamaConfig]</code>, default:                   <code>None</code> )           \u2013            <p>Ollama configuration. If None, uses defaults connecting    to localhost:11434 with llama3.2:1b model.</p>"},{"location":"api/clients/ollama/#causaliq_knowledge.llm.ollama_client.OllamaClient._total_calls","title":"_total_calls  <code>instance-attribute</code>","text":"<pre><code>_total_calls = 0\n</code></pre>"},{"location":"api/clients/ollama/#causaliq_knowledge.llm.ollama_client.OllamaClient.call_count","title":"call_count  <code>property</code>","text":"<pre><code>call_count: int\n</code></pre> <p>Return the number of API calls made.</p>"},{"location":"api/clients/ollama/#causaliq_knowledge.llm.ollama_client.OllamaClient.config","title":"config  <code>instance-attribute</code>","text":"<pre><code>config = config or OllamaConfig()\n</code></pre>"},{"location":"api/clients/ollama/#causaliq_knowledge.llm.ollama_client.OllamaClient.model_name","title":"model_name  <code>property</code>","text":"<pre><code>model_name: str\n</code></pre> <p>Return the model name being used.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>Model identifier string.</p> </li> </ul>"},{"location":"api/clients/ollama/#causaliq_knowledge.llm.ollama_client.OllamaClient.provider_name","title":"provider_name  <code>property</code>","text":"<pre><code>provider_name: str\n</code></pre> <p>Return the provider name.</p>"},{"location":"api/clients/ollama/#causaliq_knowledge.llm.ollama_client.OllamaClient.complete_json","title":"complete_json","text":"<pre><code>complete_json(\n    messages: List[Dict[str, str]], **kwargs: Any\n) -&gt; tuple[Optional[Dict[str, Any]], LLMResponse]\n</code></pre> <p>Make a completion request and parse response as JSON.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>tuple[Optional[Dict[str, Any]], LLMResponse]</code>           \u2013            <p>Tuple of (parsed JSON dict or None, raw LLMResponse).</p> </li> </ul>"},{"location":"api/clients/ollama/#causaliq_knowledge.llm.ollama_client.OllamaClient.complete_json(messages)","title":"<code>messages</code>","text":"(<code>List[Dict[str, str]]</code>)           \u2013            <p>List of message dicts with \"role\" and \"content\" keys.</p>"},{"location":"api/clients/ollama/#causaliq_knowledge.llm.ollama_client.OllamaClient.complete_json(**kwargs)","title":"<code>**kwargs</code>","text":"(<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Override config options passed to completion().</p>"},{"location":"api/clients/ollama/#causaliq_knowledge.llm.ollama_client.OllamaClient.completion","title":"completion","text":"<pre><code>completion(messages: List[Dict[str, str]], **kwargs: Any) -&gt; LLMResponse\n</code></pre> <p>Make a chat completion request to Ollama.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>LLMResponse</code>           \u2013            <p>LLMResponse with the generated content and metadata.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the API request fails or Ollama is not running.</p> </li> </ul>"},{"location":"api/clients/ollama/#causaliq_knowledge.llm.ollama_client.OllamaClient.completion(messages)","title":"<code>messages</code>","text":"(<code>List[Dict[str, str]]</code>)           \u2013            <p>List of message dicts with \"role\" and \"content\" keys.</p>"},{"location":"api/clients/ollama/#causaliq_knowledge.llm.ollama_client.OllamaClient.completion(**kwargs)","title":"<code>**kwargs</code>","text":"(<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Override config options (temperature, max_tokens).</p>"},{"location":"api/clients/ollama/#causaliq_knowledge.llm.ollama_client.OllamaClient.is_available","title":"is_available","text":"<pre><code>is_available() -&gt; bool\n</code></pre> <p>Check if Ollama server is running and model is available.</p> <p>Returns:</p> <ul> <li> <code>bool</code>           \u2013            <p>True if Ollama is running and the configured model exists.</p> </li> </ul>"},{"location":"api/clients/ollama/#causaliq_knowledge.llm.ollama_client.OllamaClient.list_models","title":"list_models","text":"<pre><code>list_models() -&gt; List[str]\n</code></pre> <p>List installed models from Ollama.</p> <p>Queries the local Ollama server to get installed models. Unlike cloud providers, this returns only models the user has explicitly pulled/installed.</p> <p>Returns:</p> <ul> <li> <code>List[str]</code>           \u2013            <p>List of model identifiers (e.g., ['llama3.2:1b', ...]).</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If Ollama server is not running.</p> </li> </ul>"},{"location":"api/clients/ollama/#supported-models","title":"Supported Models","text":"<p>Ollama supports many open-source models. Recommended for causal queries:</p> Model Size RAM Needed Quality <code>llama3.2:1b</code> ~1.3GB 4GB+ Good for simple queries <code>llama3.2</code> ~2GB 6GB+ Better reasoning <code>llama3.1:8b</code> ~4.7GB 10GB+ Best quality <code>mistral</code> ~4GB 8GB+ Good alternative <p>See Ollama Library for all available models.</p>"},{"location":"api/clients/ollama/#troubleshooting","title":"Troubleshooting","text":"<p>\"Could not connect to Ollama\" - Ensure Ollama is installed and running - Run <code>ollama serve</code> in a terminal, or start the Ollama app - Check that nothing else is using port 11434</p> <p>\"Model not found\" - Run <code>ollama pull &lt;model-name&gt;</code> to download the model - Run <code>ollama list</code> to see installed models</p> <p>Slow responses - Local inference is CPU/GPU bound - Use smaller models like <code>llama3.2:1b</code> - Increase the timeout in <code>OllamaConfig</code> - Consider using GPU acceleration if available</p>"},{"location":"api/clients/openai/","title":"OpenAI Client API Reference","text":"<p>Direct OpenAI API client for GPT models. This client implements the BaseLLMClient interface using httpx to communicate directly with the OpenAI API.</p>"},{"location":"api/clients/openai/#overview","title":"Overview","text":"<p>The OpenAI client provides:</p> <ul> <li>Direct HTTP communication with OpenAI's API</li> <li>Implements the <code>BaseLLMClient</code> abstract interface</li> <li>JSON response parsing with error handling</li> <li>Call counting for usage tracking</li> <li>Cost estimation for API calls</li> <li>Configurable timeout and retry settings</li> </ul>"},{"location":"api/clients/openai/#usage","title":"Usage","text":"<pre><code>from causaliq_knowledge.llm import OpenAIClient, OpenAIConfig\n\n# Create client with custom config\nconfig = OpenAIConfig(\n    model=\"gpt-4o-mini\",\n    temperature=0.1,\n    max_tokens=500,\n)\nclient = OpenAIClient(config=config)\n\n# Make a completion request\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"What is 2+2?\"},\n]\nresponse = client.completion(messages)\nprint(response.content)\n\n# Parse JSON response\njson_data = response.parse_json()\n</code></pre>"},{"location":"api/clients/openai/#environment-variables","title":"Environment Variables","text":"<p>The OpenAI client requires the <code>OPENAI_API_KEY</code> environment variable to be set:</p> <pre><code>export OPENAI_API_KEY=your_api_key_here\n</code></pre>"},{"location":"api/clients/openai/#openaiconfig","title":"OpenAIConfig","text":""},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIConfig","title":"OpenAIConfig  <code>dataclass</code>","text":"<pre><code>OpenAIConfig(\n    model: str = \"gpt-4o-mini\",\n    temperature: float = 0.1,\n    max_tokens: int = 500,\n    timeout: float = 30.0,\n    api_key: Optional[str] = None,\n)\n</code></pre> <p>Configuration for OpenAI API client.</p> <p>Extends OpenAICompatConfig with OpenAI-specific defaults.</p> <p>Attributes:</p> <ul> <li> <code>model</code>               (<code>str</code>)           \u2013            <p>OpenAI model identifier (default: gpt-4o-mini).</p> </li> <li> <code>temperature</code>               (<code>float</code>)           \u2013            <p>Sampling temperature (default: 0.1).</p> </li> <li> <code>max_tokens</code>               (<code>int</code>)           \u2013            <p>Maximum response tokens (default: 500).</p> </li> <li> <code>timeout</code>               (<code>float</code>)           \u2013            <p>Request timeout in seconds (default: 30.0).</p> </li> <li> <code>api_key</code>               (<code>Optional[str]</code>)           \u2013            <p>OpenAI API key (falls back to OPENAI_API_KEY env var).</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>__post_init__</code>             \u2013              <p>Set API key from environment if not provided.</p> </li> </ul>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIConfig.api_key","title":"api_key  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>api_key: Optional[str] = None\n</code></pre>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIConfig.max_tokens","title":"max_tokens  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_tokens: int = 500\n</code></pre>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIConfig.model","title":"model  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model: str = 'gpt-4o-mini'\n</code></pre>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIConfig.temperature","title":"temperature  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>temperature: float = 0.1\n</code></pre>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIConfig.timeout","title":"timeout  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>timeout: float = 30.0\n</code></pre>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIConfig.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__() -&gt; None\n</code></pre> <p>Set API key from environment if not provided.</p>"},{"location":"api/clients/openai/#openaiclient","title":"OpenAIClient","text":""},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIClient","title":"OpenAIClient","text":"<pre><code>OpenAIClient(config: Optional[OpenAIConfig] = None)\n</code></pre> <p>Direct OpenAI API client.</p> <p>Implements the BaseLLMClient interface for OpenAI's API. Uses httpx for HTTP requests.</p> Example <p>config = OpenAIConfig(model=\"gpt-4o-mini\") client = OpenAIClient(config) msgs = [{\"role\": \"user\", \"content\": \"Hello\"}] response = client.completion(msgs) print(response.content)</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>_calculate_cost</code>             \u2013              <p>Calculate approximate cost for API call.</p> </li> <li> <code>_default_config</code>             \u2013              <p>Return default OpenAI configuration.</p> </li> <li> <code>_filter_models</code>             \u2013              <p>Filter to OpenAI chat models only.</p> </li> <li> <code>_get_pricing</code>             \u2013              <p>Return OpenAI pricing per 1M tokens.</p> </li> <li> <code>complete_json</code>             \u2013              <p>Make a completion request and parse response as JSON.</p> </li> <li> <code>completion</code>             \u2013              <p>Make a chat completion request.</p> </li> <li> <code>is_available</code>             \u2013              <p>Check if the API is available.</p> </li> <li> <code>list_models</code>             \u2013              <p>List available models from the API.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>BASE_URL</code>           \u2013            </li> <li> <code>ENV_VAR</code>           \u2013            </li> <li> <code>PROVIDER_NAME</code>           \u2013            </li> <li> <code>_total_calls</code>           \u2013            </li> <li> <code>call_count</code>               (<code>int</code>)           \u2013            <p>Return the number of API calls made.</p> </li> <li> <code>config</code>           \u2013            </li> <li> <code>model_name</code>               (<code>str</code>)           \u2013            <p>Return the model name being used.</p> </li> <li> <code>provider_name</code>               (<code>str</code>)           \u2013            <p>Return the provider name.</p> </li> </ul>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIClient(config)","title":"<code>config</code>","text":"(<code>Optional[OpenAIConfig]</code>, default:                   <code>None</code> )           \u2013            <p>OpenAI configuration. If None, uses defaults with    API key from OPENAI_API_KEY environment variable.</p>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIClient.BASE_URL","title":"BASE_URL  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>BASE_URL = 'https://api.openai.com/v1'\n</code></pre>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIClient.ENV_VAR","title":"ENV_VAR  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ENV_VAR = 'OPENAI_API_KEY'\n</code></pre>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIClient.PROVIDER_NAME","title":"PROVIDER_NAME  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>PROVIDER_NAME = 'openai'\n</code></pre>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIClient._total_calls","title":"_total_calls  <code>instance-attribute</code>","text":"<pre><code>_total_calls = 0\n</code></pre>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIClient.call_count","title":"call_count  <code>property</code>","text":"<pre><code>call_count: int\n</code></pre> <p>Return the number of API calls made.</p>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIClient.config","title":"config  <code>instance-attribute</code>","text":"<pre><code>config = config or _default_config()\n</code></pre>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIClient.model_name","title":"model_name  <code>property</code>","text":"<pre><code>model_name: str\n</code></pre> <p>Return the model name being used.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>Model identifier string.</p> </li> </ul>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIClient.provider_name","title":"provider_name  <code>property</code>","text":"<pre><code>provider_name: str\n</code></pre> <p>Return the provider name.</p>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIClient._calculate_cost","title":"_calculate_cost","text":"<pre><code>_calculate_cost(model: str, input_tokens: int, output_tokens: int) -&gt; float\n</code></pre> <p>Calculate approximate cost for API call.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>float</code>           \u2013            <p>Estimated cost in USD.</p> </li> </ul>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIClient._calculate_cost(model)","title":"<code>model</code>","text":"(<code>str</code>)           \u2013            <p>Model identifier.</p>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIClient._calculate_cost(input_tokens)","title":"<code>input_tokens</code>","text":"(<code>int</code>)           \u2013            <p>Number of input tokens.</p>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIClient._calculate_cost(output_tokens)","title":"<code>output_tokens</code>","text":"(<code>int</code>)           \u2013            <p>Number of output tokens.</p>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIClient._default_config","title":"_default_config","text":"<pre><code>_default_config() -&gt; OpenAIConfig\n</code></pre> <p>Return default OpenAI configuration.</p>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIClient._filter_models","title":"_filter_models","text":"<pre><code>_filter_models(models: List[str]) -&gt; List[str]\n</code></pre> <p>Filter to OpenAI chat models only.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>List[str]</code>           \u2013            <p>Filtered list of GPT and o1/o3 models.</p> </li> </ul>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIClient._filter_models(models)","title":"<code>models</code>","text":"(<code>List[str]</code>)           \u2013            <p>List of all model IDs from API.</p>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIClient._get_pricing","title":"_get_pricing","text":"<pre><code>_get_pricing() -&gt; Dict[str, Dict[str, float]]\n</code></pre> <p>Return OpenAI pricing per 1M tokens.</p> <p>Returns:</p> <ul> <li> <code>Dict[str, Dict[str, float]]</code>           \u2013            <p>Dict mapping model prefixes to input/output costs.</p> </li> </ul>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIClient.complete_json","title":"complete_json","text":"<pre><code>complete_json(\n    messages: List[Dict[str, str]], **kwargs: Any\n) -&gt; tuple[Optional[Dict[str, Any]], LLMResponse]\n</code></pre> <p>Make a completion request and parse response as JSON.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>tuple[Optional[Dict[str, Any]], LLMResponse]</code>           \u2013            <p>Tuple of (parsed JSON dict or None, raw LLMResponse).</p> </li> </ul>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIClient.complete_json(messages)","title":"<code>messages</code>","text":"(<code>List[Dict[str, str]]</code>)           \u2013            <p>List of message dicts with \"role\" and \"content\" keys.</p>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIClient.complete_json(**kwargs)","title":"<code>**kwargs</code>","text":"(<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Override config options passed to completion().</p>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIClient.completion","title":"completion","text":"<pre><code>completion(messages: List[Dict[str, str]], **kwargs: Any) -&gt; LLMResponse\n</code></pre> <p>Make a chat completion request.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>LLMResponse</code>           \u2013            <p>LLMResponse with the generated content and metadata.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the API request fails.</p> </li> </ul>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIClient.completion(messages)","title":"<code>messages</code>","text":"(<code>List[Dict[str, str]]</code>)           \u2013            <p>List of message dicts with \"role\" and \"content\" keys.</p>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIClient.completion(**kwargs)","title":"<code>**kwargs</code>","text":"(<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Override config options (temperature, max_tokens).</p>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIClient.is_available","title":"is_available","text":"<pre><code>is_available() -&gt; bool\n</code></pre> <p>Check if the API is available.</p> <p>Returns:</p> <ul> <li> <code>bool</code>           \u2013            <p>True if API key is configured.</p> </li> </ul>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIClient.list_models","title":"list_models","text":"<pre><code>list_models() -&gt; List[str]\n</code></pre> <p>List available models from the API.</p> <p>Queries the API to get models accessible with the current API key, then filters using _filter_models().</p> <p>Returns:</p> <ul> <li> <code>List[str]</code>           \u2013            <p>List of model identifiers.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the API request fails.</p> </li> </ul>"},{"location":"api/clients/openai/#supported-models","title":"Supported Models","text":"<p>OpenAI provides the GPT family of models:</p> Model Description Free Tier <code>gpt-4o</code> GPT-4o - flagship multimodal model \u274c No <code>gpt-4o-mini</code> GPT-4o Mini - affordable and fast \u274c No <code>gpt-4-turbo</code> GPT-4 Turbo - high capability \u274c No <code>gpt-3.5-turbo</code> GPT-3.5 Turbo - fast and economical \u274c No <code>o1</code> o1 - reasoning model \u274c No <code>o1-mini</code> o1 Mini - efficient reasoning \u274c No <p>See OpenAI documentation for the full list of available models and pricing.</p>"},{"location":"architecture/caching/","title":"LLM (and other) Caching","text":""},{"location":"architecture/caching/#rationale","title":"Rationale","text":"<p>LLM requests and responses will be cached for the following purposes:</p> <ul> <li>reducing costs - unnecessary LLM requests are avoided;</li> <li>reducing latency - requests from cache can be serviced much more quickly than querying an external LLM API;</li> <li>functional testing - cached responses provide a mechanism for deterministic and fast functional testing both locally and in GitHub CI testing;</li> <li>reproducibility - of published results avoiding the issues of the non-deterministic nature of LLMs and their rapid evolution;</li> <li>LLM analysis - these caches provide a rich resource characterising the comparative behaviour of different models, prompt strategies etc.</li> </ul> <p>In fact, these are generic requirements that may apply to other caching requirements, including, perhaps:</p> <ul> <li>structure learning results such as learned graphs, debug traces and experiment metadata - the conservative principle in CausalIQ workflows means that experiments are generally only performed if the results for that experiment are not present on disk</li> <li>node scores during structure learning, although these are generally held in memory</li> </ul> <p>This argues for a generic caching capability, that would ultimately be provided by causaliq-core (though we may develop it first in causaliq-knowledge, as this is where the first use case arises, and then migrate it).</p>"},{"location":"architecture/caching/#design-principles","title":"Design Principles","text":"<ul> <li>Opt-out caching - caching is enabled by default; users explicitly disable it when not wanted - for instance, if non-cached experiment timings are required</li> <li>Flexible organisation - defined by the application/user</li> <li>each application defines a key which is SHA hashed with the truncated is hash used to locate the resource within a cache<ul> <li>for LLM queries this may be the model, messages, temperature and max tokens</li> <li>for structure learning results, this could be the sample size, randomisation seed and resource type</li> <li>for node scores, this could be the node and parents</li> </ul> </li> <li>each application defines the scope of each physical cache to optimise resource re-use<ul> <li>for LLM queries this is likely defined by the dataset - in this case, the cache therefore represents the \"LLM learning about a particular dataset\"</li> <li>for structure learning results, this could be the experiment series id and network</li> <li>for node scores, this could be the dataset, sample size, score type and score parameters</li> </ul> </li> <li>each application defines the data held for the cached resource<ul> <li>for LLM queries this includes the complete response, and rich metadata such as token counts, latency, cost, and timestamps to support LLM analysis</li> <li>for structure learning results this includes the learned graph, experiment metadata (execution time, graph scores, hyperparameters etc) and optionally, the structure learning iteration trace</li> <li>for node scores this is simply the score float value</li> </ul> </li> <li>Simple cache semantics - on request: check cache \u2192 if hit, return cached response \u2192 if miss, make request, cache result, return response</li> <li>Efficient storage - much of this cached data has lots of repeated \"tokens\" - JSON and XML tags, variable names, LLM query and response words - storing it verbatim would be very inefficient</li> <li>resources should be stored internally in a compressed format, with a \"dictionary\" used to compress and expand numeric tokens to tags, words etc (i.e. like ZIP?)</li> <li>Fast access - to resources</li> <li>resource existence checking and retrieval should be very fast.</li> <li>resource addition and update, and sequential scanning of all resources (for analysis) should be reasonably fast</li> <li>Import/Export - functions should be provided to import/export all or selected parts of a cache keys and resources to standards-based human-readable files:</li> <li>this is useful for functional testing, for example, where test files are human readable, and then a fixture imports the test files to the cache bfore testing starts</li> <li>for LLM queries the queries, response and metadata would be exported as JSON files</li> <li>for experiment results the learned graphs would be exported as GraphML files, the metadata as JSON and the debug trace in CSV format</li> <li>Centralised usage - access to cache data should be centralised in the application code</li> <li>for LLM queries caching logic implemented once in <code>BaseLLMClient</code>, available to all providers</li> <li>for experiment results centralised in core CausalIQ WOrkflow code?</li> <li>for node scores this might be called from the node_score function</li> <li>Persistent and in-memory - the application can opt to use the cache in-memory or from disk</li> <li>Concurrency - the cache manages multiple processes or threads retrievng and updating it</li> </ul>"},{"location":"architecture/caching/#resolving-the-format-tension","title":"Resolving the Format Tension","text":"<p>There is a tension between some of these principles, in particular, space/performance/concurrency and standards-based open formats. This tension is mitigated because these requirements arise at different phases of the project lifecycle:</p> Phase Primary Need Format Test setup Human-readable, editable fixtures JSON, GraphML Production runs Speed, concurrency, disk efficiency SQLite + encoded blobs Result archival (Zenodo) Standards-based, long-term accessible JSON, GraphML, CSV <p>This lifecycle perspective leads to a clear resolution:</p> <ul> <li>Compact, efficient formats are the norm during active experimentation</li> <li>Import converts open formats \u2192 internal cache (test fixture setup)</li> <li>Export converts internal cache \u2192 open formats (archival, sharing)</li> </ul> <p>The import/export capability is therefore not just a convenience feature - it is essential architecture that bridges incompatible requirements at different lifecycle stages.</p>"},{"location":"architecture/caching/#existing-packages","title":"Existing Packages","text":"<p>Evaluation of existing packages against our requirements:</p>"},{"location":"architecture/caching/#comparison-summary","title":"Comparison Summary","text":"Requirement zip SQLite diskcache pickle Fast key lookup \u26a0\ufe0f Need index \u2705 Indexed \u2705 Indexed \u274c Load all Concurrency \u274c Poor \u2705 Built-in \u2705 Built-in \u274c None Efficient storage \u2705 Compressed \u26a0\ufe0f Per-blob only \u274c None \u26a0\ufe0f Reasonable Cross-entry compression \u2705 Shared dict \u274c No \u274c No \u274c No In-memory mode \u274c File-based \u2705 <code>:memory:</code> \u274c Disk-based \u2705 Native Incremental updates \u274c Rewrite \u2705 Per-entry \u2705 Per-entry \u274c Rewrite Import/Export \u2705 File extract \u26a0\ufe0f Via SQL \u274c Python-only \u274c Python-only Standards-based \u2705 Universal \u2705 Universal \u274c Python-only \u274c Python-only Security \u2705 Safe \u2705 Safe \u2705 Safe \u274c Code execution risk"},{"location":"architecture/caching/#zip","title":"zip","text":"<p>Pros: - Excellent compression with shared dictionary across all files - Universal standard format - Built-in Python support (<code>zipfile</code> module) - Could use hash as filename within archive</p> <p>Cons: - No true in-memory mode (must write to file or BytesIO) - Poor incremental update support - must rewrite archive to add/modify entries - No built-in concurrency protection - Random access requires scanning central directory</p> <p>Verdict: Good for archival/export, poor for active cache with frequent updates.</p>"},{"location":"architecture/caching/#sqlite","title":"SQLite","text":"<p>Pros: - Excellent indexed key lookup - O(1) access - Built-in concurrency with locking - In-memory mode via <code>:memory:</code> - Per-entry insert/update without rewriting - Universal format, excellent tooling - SQL enables rich querying for analysis</p> <p>Cons: - No cross-entry compression (can compress individual blobs, but no shared dictionary) - Compression must be handled at application level</p> <p>Verdict: Excellent for cache infrastructure. Compression concern addressed by our pluggable encoder + token dictionary approach.</p>"},{"location":"architecture/caching/#diskcache","title":"diskcache","text":"<p>Pros: - Simple key-value API - Built-in concurrency (uses SQLite underneath) - Automatic eviction policies (LRU, etc.) - Good performance for typical caching scenarios</p> <p>Cons: - No compression - Python-specific - not portable to other languages/tools - No cross-entry compression or shared dictionaries - Limited querying capability - just key-value lookup - Export would require custom code</p> <p>Verdict: Convenient for simple caching but lacks compression and portability requirements.</p>"},{"location":"architecture/caching/#pickle","title":"pickle","text":"<p>Pros: - Native Python serialisation - handles arbitrary objects - Simple API - In-memory use is natural (just Python dicts)</p> <p>Cons: - Security risk - pickle can execute arbitrary code on load - No concurrency protection - corruption risk with multiple writers - Single-file approach requires loading entire cache into memory - Directory-of-files approach has same scaling issues as JSON - No cross-entry compression - Python-specific - cannot share caches with other tools - Not human-readable</p> <p>Verdict: Unsuitable for production caches. Security risk alone disqualifies it. Only appropriate for small, single-process, trusted, ephemeral use cases.</p>"},{"location":"architecture/caching/#recommendation","title":"Recommendation","text":"<p>SQLite + pluggable encoders with shared token dictionary combines: - SQLite's strengths: concurrency, fast lookup, in-memory mode, incremental updates - Custom compression: pluggable encoders achieve domain-specific compression, shared token dictionary provides cross-entry compression benefits - Standards compliance: SQLite is universal, export functions produce JSON/GraphML</p>"},{"location":"architecture/caching/#llm-queryresponse-caching","title":"LLM Query/Response Caching","text":"<p>This section details the specific caching requirements for LLM queries and responses.</p>"},{"location":"architecture/caching/#cache-key-construction","title":"Cache Key Construction","text":"<p>The cache key is a SHA-256 hash (truncated to 16 hex characters) of the following request parameters that affect the response:</p> Field Type Rationale <code>model</code> string Different models produce different responses <code>messages</code> list[dict] The full conversation including system prompt and user query <code>temperature</code> float Affects response variability (0.0 recommended for reproducibility) <code>max_tokens</code> int May truncate response <p>Not included in key: - <code>api_key</code> - authentication, doesn't affect response content - <code>timeout</code> - client-side concern - <code>stream</code> - delivery method, same content</p> <p>Example cache key input: <pre><code>{\n  \"model\": \"gpt-4o\",\n  \"messages\": [\n    {\"role\": \"system\", \"content\": \"You are a causal inference expert...\"},\n    {\"role\": \"user\", \"content\": \"What does variable 'BMI' typically represent in health studies?\"}\n  ],\n  \"temperature\": 0.0,\n  \"max_tokens\": 1000\n}\n</code></pre></p> <p>Resulting hash: <code>a3f7b2c1e9d4f8a2</code></p>"},{"location":"architecture/caching/#cached-data-structure","title":"Cached Data Structure","text":"<p>Each cache entry stores both the response and rich metadata for analysis:</p> <pre><code>{\n  \"cache_key\": {\n    \"model\": \"gpt-4o\",\n    \"messages\": [...],\n    \"temperature\": 0.0,\n    \"max_tokens\": 1000\n  },\n  \"response\": {\n    \"content\": \"BMI (Body Mass Index) is a measure of body fat based on height and weight...\",\n    \"finish_reason\": \"stop\",\n    \"model_version\": \"gpt-4o-2024-08-06\"\n  },\n  \"metadata\": {\n    \"provider\": \"openai\",\n    \"timestamp\": \"2026-01-11T10:15:32.456Z\",\n    \"latency_ms\": 1847,\n    \"tokens\": {\n      \"input\": 142,\n      \"output\": 203,\n      \"total\": 345\n    },\n    \"cost_usd\": 0.00518,\n    \"cache_hit\": false\n  }\n}\n</code></pre>"},{"location":"architecture/caching/#field-descriptions","title":"Field Descriptions","text":"<p>Response fields:</p> Field Description <code>content</code> The full text response from the LLM <code>finish_reason</code> Why generation stopped: <code>stop</code>, <code>length</code>, <code>content_filter</code> <code>model_version</code> Actual model version used (may differ from requested) <p>Metadata fields:</p> Field Description Use Case <code>provider</code> LLM provider name (openai, anthropic, etc.) Analysis across providers <code>timestamp</code> When the original request was made Tracking model evolution <code>latency_ms</code> Response time in milliseconds Performance analysis <code>tokens.input</code> Prompt token count Cost tracking <code>tokens.output</code> Completion token count Cost tracking <code>tokens.total</code> Total tokens Cost tracking <code>cost_usd</code> Estimated cost of the request Budget monitoring <code>cache_hit</code> Whether this was served from cache Always <code>false</code> when first stored"},{"location":"architecture/caching/#cache-scope","title":"Cache Scope","text":"<p>LLM caches are typically dataset-scoped:</p> <pre><code>datasets/\n  health_study/\n    llm_cache.db          # All LLM queries about this dataset\n  economic_data/\n    llm_cache.db          # Separate cache for different dataset\n</code></pre> <p>This organisation means: - Queries like \"What does BMI mean?\" are shared across all experiments on that dataset - Different experiment series (algorithms, parameters) reuse the same cached LLM knowledge - The cache represents \"what the LLM has learned about this dataset\"</p>"},{"location":"architecture/caching/#integration-with-basellmclient","title":"Integration with BaseLLMClient","text":"<pre><code>class BaseLLMClient:\n    def __init__(self, cache: TokenCache | None = None, use_cache: bool = True):\n        self.cache = cache\n        self.use_cache = use_cache\n\n    def query(self, messages: list[dict], **kwargs) -&gt; LLMResponse:\n        if self.use_cache and self.cache:\n            cache_key = self._build_cache_key(messages, kwargs)\n            cached = self.cache.get(cache_key, entry_type='llm')\n            if cached:\n                return LLMResponse.from_cache(cached)\n\n        # Make actual API call\n        start = time.perf_counter()\n        response = self._call_api(messages, **kwargs)\n        latency_ms = int((time.perf_counter() - start) * 1000)\n\n        # Cache the result\n        if self.use_cache and self.cache:\n            entry = self._build_cache_entry(messages, kwargs, response, latency_ms)\n            self.cache.put(cache_key, entry_type='llm', data=entry)\n\n        return response\n\n    def _build_cache_key(self, messages: list[dict], kwargs: dict) -&gt; str:\n        key_data = {\n            'model': self.model,\n            'messages': messages,\n            'temperature': kwargs.get('temperature', 0.0),\n            'max_tokens': kwargs.get('max_tokens', 1000),\n        }\n        key_json = json.dumps(key_data, sort_keys=True, separators=(',', ':'))\n        return hashlib.sha256(key_json.encode()).hexdigest()[:16]\n</code></pre>"},{"location":"architecture/caching/#proposed-solution-sqlite-pluggable-encoders","title":"Proposed Solution: SQLite + Pluggable Encoders","text":"<p>Given the requirements, we propose SQLite for storage/concurrency combined with pluggable type-specific encoders for compression. Generic tokenisation provides a good default, but domain-specific encoders achieve far better compression for well-defined structures.</p>"},{"location":"architecture/caching/#why-pluggable-encoders","title":"Why Pluggable Encoders?","text":"<p>Generic tokenisation is a lowest common denominator - it works for everything but optimises nothing. Consider learned graphs:</p> Approach Per Edge 10,000 edges GraphML (verbatim) ~80-150 bytes 800KB - 1.5MB GraphML (tokenised) ~30-50 bytes 300-500KB Domain-specific binary 4 bytes 40KB <p>Domain-specific encoding achieves 10-30x better compression for structured data.</p>"},{"location":"architecture/caching/#sqlite-schema","title":"SQLite Schema","text":"<pre><code>-- Token dictionary (grows dynamically, shared across encoders)\nCREATE TABLE tokens (\n    id INTEGER PRIMARY KEY AUTOINCREMENT,  -- uint16 range (max 65,535)\n    token TEXT UNIQUE NOT NULL,\n    frequency INTEGER DEFAULT 1\n);\n\n-- Generic cache entries\nCREATE TABLE cache_entries (\n    hash TEXT PRIMARY KEY,\n    entry_type TEXT NOT NULL,              -- 'llm', 'graph', 'score', etc.\n    data BLOB NOT NULL,                    -- encoded by type-specific encoder\n    created_at TEXT NOT NULL,\n    metadata BLOB                          -- tokenised metadata (optional)\n);\n\nCREATE INDEX idx_entry_type ON cache_entries(entry_type);\nCREATE INDEX idx_created_at ON cache_entries(created_at);\n</code></pre>"},{"location":"architecture/caching/#encoder-architecture","title":"Encoder Architecture","text":"<pre><code>from abc import ABC, abstractmethod\nfrom pathlib import Path\nfrom typing import Any\n\nclass EntryEncoder(ABC):\n    \"\"\"Type-specific encoder/decoder for cache entries.\"\"\"\n\n    @abstractmethod\n    def encode(self, data: Any, token_cache: 'TokenCache') -&gt; bytes:\n        \"\"\"Encode data to bytes, optionally using shared token dictionary.\"\"\"\n        ...\n\n    @abstractmethod\n    def decode(self, blob: bytes, token_cache: 'TokenCache') -&gt; Any:\n        \"\"\"Decode bytes back to original data structure.\"\"\"\n        ...\n\n    @abstractmethod\n    def export(self, data: Any, path: Path) -&gt; None:\n        \"\"\"Export to human-readable format (GraphML, JSON, etc.).\"\"\"\n        ...\n\n    @abstractmethod\n    def import_(self, path: Path) -&gt; Any:\n        \"\"\"Import from human-readable format.\"\"\"\n        ...\n</code></pre>"},{"location":"architecture/caching/#concrete-encoders","title":"Concrete Encoders","text":""},{"location":"architecture/caching/#graphencoder-domain-specific-binary","title":"GraphEncoder - Domain-Specific Binary","text":"<p>Achieves ~4 bytes per edge through bit-packing:</p> <pre><code>class GraphEncoder(EntryEncoder):\n    \"\"\"Compact binary encoding for learned graphs.\n\n    Format:\n    - Header: node_count (uint16)\n    - Node table: [node_name_token_id (uint16), ...] - maps index \u2192 name\n    - Edge list: [packed_edge (uint32), ...] where:\n        - bits 0-13:  source node index (up to 16,384 nodes)\n        - bits 14-27: target node index\n        - bits 28-31: edge type (4 bits for endpoint marks: -, &gt;, o)\n    \"\"\"\n\n    def encode(self, graph: nx.DiGraph, token_cache: 'TokenCache') -&gt; bytes:\n        nodes = list(graph.nodes())\n        node_to_idx = {n: i for i, n in enumerate(nodes)}\n\n        # Header\n        data = struct.pack('H', len(nodes))\n\n        # Node table - uses shared token dictionary for node names\n        for node in nodes:\n            token_id = token_cache.get_or_create_token(str(node))\n            data += struct.pack('H', token_id)\n\n        # Edge list - packed binary\n        for src, tgt, attrs in graph.edges(data=True):\n            edge_type = self._encode_edge_type(attrs)\n            packed = (node_to_idx[src] | \n                     (node_to_idx[tgt] &lt;&lt; 14) | \n                     (edge_type &lt;&lt; 28))\n            data += struct.pack('I', packed)\n\n        return data\n\n    def export(self, graph: nx.DiGraph, path: Path) -&gt; None:\n        \"\"\"Export to standard GraphML for human readability.\"\"\"\n        nx.write_graphml(graph, path)\n\n    def import_(self, path: Path) -&gt; nx.DiGraph:\n        \"\"\"Import from GraphML.\"\"\"\n        return nx.read_graphml(path)\n</code></pre>"},{"location":"architecture/caching/#llmentryencoder-generic-tokenisation","title":"LLMEntryEncoder - Generic Tokenisation","text":"<p>Good default for text-heavy, variably-structured data:</p> <pre><code>class LLMEntryEncoder(EntryEncoder):\n    \"\"\"Tokenised encoding for LLM cache entries.\n\n    Uses shared token dictionary for JSON structure and text content.\n    Numbers stored as literals. Achieves 50-70% compression on typical entries.\n    \"\"\"\n\n    TOKEN_REF = 0x00\n    LITERAL_INT = 0x01\n    LITERAL_FLOAT = 0x02\n\n    def encode(self, entry: dict, token_cache: 'TokenCache') -&gt; bytes:\n        # Tokenise JSON keys, structural chars, string words\n        # Store integers and floats as literals\n        ...\n\n    def decode(self, blob: bytes, token_cache: 'TokenCache') -&gt; dict:\n        # Reconstruct JSON from token IDs and literals\n        ...\n\n    def export(self, entry: dict, path: Path) -&gt; None:\n        path.write_text(json.dumps(entry, indent=2))\n\n    def import_(self, path: Path) -&gt; dict:\n        return json.loads(path.read_text())\n</code></pre>"},{"location":"architecture/caching/#scoreencoder-minimal-encoding","title":"ScoreEncoder - Minimal Encoding","text":"<p>For high-volume, simple data:</p> <pre><code>class ScoreEncoder(EntryEncoder):\n    \"\"\"Compact encoding for node scores - just 8 bytes per entry.\"\"\"\n\n    def encode(self, score: float, token_cache: 'TokenCache') -&gt; bytes:\n        return struct.pack('d', score)  # 8-byte double\n\n    def decode(self, blob: bytes, token_cache: 'TokenCache') -&gt; float:\n        return struct.unpack('d', blob)[0]\n\n    def export(self, score: float, path: Path) -&gt; None:\n        path.write_text(str(score))\n\n    def import_(self, path: Path) -&gt; float:\n        return float(path.read_text())\n</code></pre>"},{"location":"architecture/caching/#compression-comparison","title":"Compression Comparison","text":"Entry Type Encoder Typical Size vs Verbatim LLM response LLMEntryEncoder 500 bytes 50-70% smaller Learned graph (1000 edges) GraphEncoder 4KB 95% smaller Node score ScoreEncoder 8 bytes 50% smaller Experiment metadata LLMEntryEncoder 200 bytes 60% smaller"},{"location":"architecture/caching/#tokencache-with-pluggable-encoders","title":"TokenCache with Pluggable Encoders","text":"<pre><code>class TokenCache:\n    def __init__(self, db_path: str, encoders: dict[str, EntryEncoder] = None):\n        self.conn = sqlite3.connect(db_path)\n        self._init_schema()\n        self._load_token_dict()\n\n        # Register encoders - defaults provided, can be overridden\n        self.encoders = encoders or {\n            'llm': LLMEntryEncoder(),\n            'graph': GraphEncoder(),\n            'score': ScoreEncoder(),\n        }\n\n    def _load_token_dict(self):\n        \"\"\"Load token dictionary into memory for fast lookup.\"\"\"\n        cursor = self.conn.execute(\"SELECT token, id FROM tokens\")\n        self.token_to_id = {row[0]: row[1] for row in cursor}\n        self.id_to_token = {v: k for k, v in self.token_to_id.items()}\n\n    def get_or_create_token(self, token: str) -&gt; int:\n        \"\"\"Get token ID, creating new entry if needed. Used by encoders.\"\"\"\n        if token in self.token_to_id:\n            return self.token_to_id[token]\n        cursor = self.conn.execute(\n            \"INSERT INTO tokens (token) VALUES (?) RETURNING id\", \n            (token,)\n        )\n        token_id = cursor.fetchone()[0]\n        self.token_to_id[token] = token_id\n        self.id_to_token[token_id] = token\n        return token_id\n\n    def put(self, hash: str, entry_type: str, data: Any, metadata: dict = None):\n        \"\"\"Store entry using appropriate encoder.\"\"\"\n        encoder = self.encoders[entry_type]\n        blob = encoder.encode(data, self)\n        meta_blob = self.encoders['llm'].encode(metadata, self) if metadata else None\n        self.conn.execute(\n            \"INSERT OR REPLACE INTO cache_entries VALUES (?, ?, ?, ?, ?)\",\n            (hash, entry_type, blob, datetime.utcnow().isoformat(), meta_blob)\n        )\n        self.conn.commit()\n\n    def get(self, hash: str, entry_type: str) -&gt; Any | None:\n        \"\"\"Retrieve and decode entry.\"\"\"\n        cursor = self.conn.execute(\n            \"SELECT data FROM cache_entries WHERE hash = ? AND entry_type = ?\",\n            (hash, entry_type)\n        )\n        row = cursor.fetchone()\n        if row is None:\n            return None\n        encoder = self.encoders[entry_type]\n        return encoder.decode(row[0], self)\n\n    def register_encoder(self, entry_type: str, encoder: EntryEncoder):\n        \"\"\"Register custom encoder for new entry types.\"\"\"\n        self.encoders[entry_type] = encoder\n</code></pre>"},{"location":"architecture/caching/#tokenisation-details-llmentryencoder","title":"Tokenisation Details (LLMEntryEncoder)","text":"Element Tokenise? Rationale JSON structural chars (<code>{</code>, <code>}</code>, <code>[</code>, <code>]</code>, <code>:</code>, <code>,</code>) Yes Very frequent, 1 char \u2192 2 bytes is fine given volume JSON keys (<code>\"role\"</code>, <code>\"content\"</code>) Yes Highly repetitive String values (words) Yes Variable names, common words repeat often Integers No Store as literal bytes (type marker + value) Floats No Store as literal bytes (type marker + 8-byte double) Booleans Yes Just two tokens: <code>true</code>, <code>false</code> Null Yes Single token"},{"location":"architecture/caching/#token-boundary-rules","title":"Token Boundary Rules","text":"<p>Tokenisation splits on:</p> <ol> <li>JSON structural characters - each is its own token</li> <li>Whitespace - discarded (JSON can be reconstructed without it)</li> <li>Within strings - split on whitespace and punctuation boundaries</li> </ol> <p>Example: <pre><code>\"BMI represents Body Mass Index\" \n\u2192 [\", BMI, represents, Body, Mass, Index, \"]\n\u2192 7 tokens\n</code></pre></p>"},{"location":"architecture/caching/#encoding-format","title":"Encoding Format","text":"<p>The data blob contains a sequence of typed values:</p> Type Marker (uint8) Followed By Description 0x00 uint16 Token ID reference 0x01 int64 (8 bytes) Literal integer 0x02 float64 (8 bytes) Literal float <p>Example encoding: <pre><code>{\"score\": 3.14159, \"node\": \"BMI\"}\n\nTokens: { \" score \" : \" node \" : \" BMI \" }\n         \u2193   \u2193     \u2193 \u2193   \u2193    \u2193 \u2193   \u2193   \u2193\n        T1  T2    T3 T4  T2   T5 T4  T2  T6  T2  T1\u2192}\n\nEncoded: [0x00,T1, 0x00,T2, 0x00,T3, 0x00,T2, 0x00,T4, \n          0x02,&lt;8-byte float&gt;,\n          0x00,T4, 0x00,T2, 0x00,T5, 0x00,T2, 0x00,T4, \n          0x00,T2, 0x00,T6, 0x00,T2, 0x00,T7]\n</code></pre></p>"},{"location":"architecture/caching/#in-memory-mode","title":"In-Memory Mode","text":"<p>For node scores during structure learning (high-frequency, session-scoped):</p> <pre><code># In-memory SQLite - fast, no disk I/O\ncache = TokenCache(\":memory:\")\n\n# Can optionally persist at end of session\ncache.save_to_disk(\"node_scores.db\")\n</code></pre>"},{"location":"architecture/caching/#importexport","title":"Import/Export","text":"<p>Import/export delegates to the appropriate encoder:</p> <pre><code>class TokenCache:\n    def export_entries(self, output_dir: Path, entry_type: str, format: str = None):\n        \"\"\"Export cache entries to human-readable files.\"\"\"\n        encoder = self.encoders[entry_type]\n        query = \"SELECT hash, data FROM cache_entries WHERE entry_type = ?\"\n        for hash, blob in self.conn.execute(query, (entry_type,)):\n            data = encoder.decode(blob, self)\n            ext = format or encoder.default_export_format\n            encoder.export(data, output_dir / f\"{hash}.{ext}\")\n\n    def import_entries(self, input_dir: Path, entry_type: str):\n        \"\"\"Import human-readable files into cache.\"\"\"\n        encoder = self.encoders[entry_type]\n        for file in input_dir.iterdir():\n            if file.is_file():\n                data = encoder.import_(file)\n                hash = file.stem\n                self.put(hash, entry_type, data)\n</code></pre>"},{"location":"architecture/caching/#cache-organisation","title":"Cache Organisation","text":"<p>The cache is scoped by its physical location (database file):</p> Context Cache Location Typical Size Dataset LLM queries <code>{dataset_dir}/llm_cache.db</code> 10K-100K entries Experiment series <code>{series_dir}/results.db</code> 100K-10M entries Functional tests <code>tests/data/cache.db</code> 100-1K entries Node scores (session) <code>:memory:</code> 1M+ entries"},{"location":"architecture/caching/#collision-handling","title":"Collision Handling","text":"<p>With truncated SHA-256 hashes, collisions are possible. On cache read:</p> <ol> <li>Find entry by hash</li> <li>Decode and verify original key matches request exactly</li> <li>If mismatch, treat as cache miss</li> </ol>"},{"location":"architecture/caching/#cli-tooling","title":"CLI Tooling","text":"<pre><code># Export to human-readable format (uses encoder's export method)\ncausaliq cache export llm_cache.db --type llm --output cache_dump/\ncausaliq cache export results.db --type graph --format graphml --output graphs/\n\n# Import from human-readable format  \ncausaliq cache import cache_dump/ --into llm_cache.db --type llm\n\n# Statistics\ncausaliq cache stats results.db\n# Output: \n#   Entries: 52,341 (llm: 12,341, graph: 40,000)\n#   Tokens: 4,892\n#   Size: 45.2 MB (estimated uncompressed: 142.8 MB)\n\n# Query/inspect\ncausaliq cache query results.db --type llm --where \"model='gpt-4o'\" --limit 10\n</code></pre>"},{"location":"architecture/caching/#implementation-plan","title":"Implementation Plan","text":"<p>The caching system will be built incrementally across the following commits. Feature branch: <code>feature/llm-caching</code></p>"},{"location":"architecture/caching/#module-structure","title":"Module Structure","text":"<p>The implementation separates core infrastructure (future migration to <code>causaliq-core</code>) from LLM-specific code (remains in <code>causaliq-knowledge</code>):</p> <pre><code>src/causaliq_knowledge/\n  cache/                          # CORE - will migrate to causaliq-core\n    __init__.py\n    token_cache.py                # TokenCache class, SQLite schema\n    encoders/\n      __init__.py\n      base.py                     # EntryEncoder ABC\n      json_encoder.py             # Generic JSON tokenisation encoder\n  llm/\n    cache.py                      # LLM-SPECIFIC - stays in causaliq-knowledge\n                                  # LLMEntryEncoder, cache key building, metadata\n\ntests/\n  unit/\n    cache/                        # CORE tests - migrate with core\n      test_token_cache.py\n      test_json_encoder.py\n    llm/\n      test_llm_cache.py           # LLM-SPECIFIC tests - stay here\n  integration/\n    test_llm_caching.py           # LLM-SPECIFIC integration tests\n</code></pre> <p>This separation ensures: - Core cache infrastructure can be extracted to <code>causaliq-core</code> with its tests - LLM-specific encoder and integration remain in <code>causaliq-knowledge</code> - Clean dependency: <code>causaliq-knowledge</code> depends on <code>causaliq-core</code>, not vice versa</p>"},{"location":"architecture/caching/#phase-1-core-infrastructure-migrates-to-causaliq-core","title":"Phase 1: Core Infrastructure (migrates to causaliq-core)","text":"# Commit Description 1 Add cache module skeleton Create <code>src/causaliq_knowledge/cache/</code> with <code>__init__.py</code>, empty module structure 2 Implement SQLite schema and TokenCache base <code>TokenCache</code> class with <code>__init__</code>, <code>_init_schema()</code>, connection management, in-memory support 3 Add token dictionary management <code>get_or_create_token()</code>, <code>_load_token_dict()</code>, in-memory token lookup 4 Add basic get/put operations <code>put()</code>, <code>get()</code>, <code>exists()</code> methods (without encoding - raw blob storage) 5 Add unit tests for core TokenCache <code>tests/unit/cache/test_token_cache.py</code> - schema, tokens, CRUD, in-memory"},{"location":"architecture/caching/#phase-2-encoder-architecture-core-migrates-llm-specific-stays","title":"Phase 2: Encoder Architecture (core migrates, LLM-specific stays)","text":"# Commit Description 6 Define EntryEncoder ABC <code>cache/encoders/base.py</code> - ABC with <code>encode()</code>, <code>decode()</code>, <code>export()</code>, <code>import_()</code> (core) 7 Implement generic JSON encoder <code>cache/encoders/json_encoder.py</code> - tokenisation, literals (core) 8 Integrate encoders with TokenCache <code>register_encoder()</code>, encoder dispatch in <code>get()</code>/<code>put()</code> (core) 9 Add unit tests for encoders <code>tests/unit/cache/test_json_encoder.py</code> (core) 10 Implement LLMEntryEncoder <code>llm/cache.py</code> - extends JSON encoder with LLM-specific structure (LLM-specific) 11 Add unit tests for LLMEntryEncoder <code>tests/unit/llm/test_llm_cache.py</code> (LLM-specific)"},{"location":"architecture/caching/#phase-3-importexport-core","title":"Phase 3: Import/Export (core)","text":"# Commit Description 12 Add export functionality <code>export_entries()</code> method in TokenCache 13 Add import functionality <code>import_entries()</code> method in TokenCache 14 Add import/export tests Round-trip tests in <code>tests/unit/cache/</code>"},{"location":"architecture/caching/#phase-4-llm-client-integration-llm-specific","title":"Phase 4: LLM Client Integration (LLM-specific)","text":"# Commit Description 15 Add cache_key building to BaseLLMClient <code>_build_cache_key()</code> method with SHA-256 hashing 16 Integrate caching into query flow Cache lookup before API call, cache storage after 17 Add metadata capture Latency, token counts, cost estimation, timestamps 18 Add LLM caching integration tests <code>tests/integration/test_llm_caching.py</code> - mock API, cache hit/miss 19 Update functional tests to use cache import Fixture imports test data, tests run from cache"},{"location":"architecture/caching/#phase-5-cli-tooling-core-commands-llm-specific-options","title":"Phase 5: CLI Tooling (core commands, LLM-specific options)","text":"# Commit Description 20 Add cache CLI subcommand <code>causaliq cache</code> command group (core) 21 Add cache export command <code>causaliq cache export</code> with type/format options 22 Add cache import command <code>causaliq cache import</code> 23 Add cache stats command Entry counts, token count, size estimation"},{"location":"architecture/caching/#phase-6-documentation-polish","title":"Phase 6: Documentation &amp; Polish","text":"# Commit Description 24 Add cache API documentation Docstrings, mkdocs pages for cache module 25 Update architecture docs Mark caching design as implemented, add usage examples"},{"location":"architecture/caching/#dependency-graph","title":"Dependency Graph","text":"<pre><code>Phase 1 (core):     1 \u2192 2 \u2192 3 \u2192 4 \u2192 5\n                                  \u2193\nPhase 2 (core):               6 \u2192 7 \u2192 8 \u2192 9\n                                        \u2193\nPhase 2 (llm):                    10 \u2192 11\n                                        \u2193\nPhase 3 (core):               12 \u2192 13 \u2192 14\n                                        \u2193\nPhase 4 (llm):          15 \u2192 16 \u2192 17 \u2192 18 \u2192 19\n                                            \u2193\nPhase 5:                      20 \u2192 21 \u2192 22 \u2192 23\n                                            \u2193\nPhase 6:                                24 \u2192 25\n</code></pre>"},{"location":"architecture/caching/#milestones","title":"Milestones","text":"<ul> <li>After Phase 1 (commits 1-5): Working cache with raw blob storage, testable standalone</li> <li>After Phase 2 (commits 6-11): Compression working, LLM encoder ready</li> <li>After Phase 4 (commits 15-19): Cache fully usable for LLM queries</li> <li>After Phase 5 (commits 20-23): CLI tools available for cache management</li> </ul>"},{"location":"architecture/caching/#future-migration-to-causaliq-core","title":"Future: Migration to causaliq-core","text":"<p>When ready to migrate core infrastructure:</p> <ol> <li>Move <code>cache/</code> directory to <code>causaliq-core</code></li> <li>Move <code>tests/unit/cache/</code> to <code>causaliq-core</code></li> <li>Update imports in <code>causaliq-knowledge</code> to use <code>causaliq_core.cache</code></li> <li><code>LLMEntryEncoder</code> and LLM integration remain in <code>causaliq-knowledge</code></li> </ol>"},{"location":"architecture/llm_integration/","title":"LLM Integration Design Note","text":""},{"location":"architecture/llm_integration/#overview","title":"Overview","text":"<p>This document describes how causaliq-knowledge integrates with Large Language Models (LLMs) to provide knowledge about causal relationships. The primary use case for v0.1.0 is answering queries about edge existence and edge orientation to support graph averaging in causaliq-analysis.</p>"},{"location":"architecture/llm_integration/#how-it-works","title":"How it works","text":""},{"location":"architecture/llm_integration/#query-flow","title":"Query Flow","text":"<ol> <li>Consumer requests knowledge about a potential edge (e.g., \"Does smoking cause cancer?\")</li> <li>KnowledgeProvider receives the query with optional context</li> <li>LLM client formats the query using structured prompts</li> <li>One or more LLMs are queried (configurable)</li> <li>Responses are parsed into structured <code>EdgeKnowledge</code> objects</li> <li>Multi-LLM consensus combines responses (if multiple models used)</li> <li>Result returned with confidence score and reasoning</li> </ol>"},{"location":"architecture/llm_integration/#core-interface","title":"Core Interface","text":"<pre><code>from abc import ABC, abstractmethod\nfrom pydantic import BaseModel\n\nclass EdgeKnowledge(BaseModel):\n    \"\"\"Structured knowledge about a potential causal edge.\"\"\"\n    exists: bool | None           # True, False, or None (uncertain)\n    direction: str | None         # \"a_to_b\", \"b_to_a\", \"undirected\", None\n    confidence: float             # 0.0 to 1.0\n    reasoning: str                # Human-readable explanation\n    model: str | None = None      # Which LLM provided this (for logging)\n\nclass KnowledgeProvider(ABC):\n    \"\"\"Abstract interface for all knowledge sources.\"\"\"\n\n    @abstractmethod\n    def query_edge(\n        self,\n        node_a: str,\n        node_b: str,\n        context: dict | None = None\n    ) -&gt; EdgeKnowledge:\n        \"\"\"\n        Query whether a causal edge exists between two nodes.\n\n        Args:\n            node_a: Name of first variable\n            node_b: Name of second variable  \n            context: Optional context (domain, variable descriptions, etc.)\n\n        Returns:\n            EdgeKnowledge with existence, direction, confidence, reasoning\n        \"\"\"\n        pass\n</code></pre>"},{"location":"architecture/llm_integration/#llm-implementation","title":"LLM Implementation","text":"<pre><code>class LLMKnowledge(KnowledgeProvider):\n    \"\"\"LLM-based knowledge provider using vendor-specific API clients.\"\"\"\n\n    def __init__(\n        self,\n        models: list[str] = [\"groq/llama-3.1-8b-instant\"],\n        consensus_strategy: str = \"weighted_vote\",\n        temperature: float = 0.1,\n        max_tokens: int = 500,\n    ):\n        \"\"\"\n        Initialize LLM knowledge provider.\n\n        Args:\n            models: List of model identifiers with provider prefix.\n                   e.g., [\"groq/llama-3.1-8b-instant\", \"gemini/gemini-2.5-flash\"]\n            consensus_strategy: How to combine multi-model responses\n                               \"weighted_vote\" or \"highest_confidence\"\n            temperature: LLM temperature (0.0-1.0)\n            max_tokens: Maximum tokens in response\n        \"\"\"\n        ...\n</code></pre>"},{"location":"architecture/llm_integration/#llm-provider-configuration","title":"LLM Provider Configuration","text":""},{"location":"architecture/llm_integration/#architectural-decision-vendor-specific-apis","title":"Architectural Decision: Vendor-Specific APIs","text":"<p>We use direct vendor-specific API clients rather than wrapper libraries like LiteLLM or LangChain. Each provider has a dedicated client class that uses httpx for HTTP communication.</p> <p>Benefits of this approach:</p> <ul> <li>Reliability: No wrapper bugs or version conflicts</li> <li>Minimal dependencies: Only httpx required for HTTP</li> <li>Full control: Direct access to vendor-specific features</li> <li>Better debugging: Clear stack traces without abstraction layers</li> <li>Predictable behavior: No surprises from wrapper library updates</li> </ul>"},{"location":"architecture/llm_integration/#supported-providers","title":"Supported Providers","text":"Provider Client Class Model Examples API Key Variable Groq <code>GroqClient</code> <code>groq/llama-3.1-8b-instant</code> <code>GROQ_API_KEY</code> Google Gemini <code>GeminiClient</code> <code>gemini/gemini-2.5-flash</code> <code>GEMINI_API_KEY</code> OpenAI <code>OpenAIClient</code> <code>openai/gpt-4o-mini</code> <code>OPENAI_API_KEY</code> Anthropic <code>AnthropicClient</code> <code>anthropic/claude-sonnet-4-20250514</code> <code>ANTHROPIC_API_KEY</code> DeepSeek <code>DeepSeekClient</code> <code>deepseek/deepseek-chat</code> <code>DEEPSEEK_API_KEY</code> Mistral <code>MistralClient</code> <code>mistral/mistral-small-latest</code> <code>MISTRAL_API_KEY</code> Ollama <code>OllamaClient</code> <code>ollama/llama3</code> N/A (local) <p>Additional providers can be added by implementing new client classes following the same pattern.</p>"},{"location":"architecture/llm_integration/#cost-considerations","title":"Cost Considerations","text":"<p>For edge queries (~500 tokens each):</p> Provider Model Cost per 1000 queries Quality Speed Groq llama-3.1-8b-instant Free tier Good Very fast Google gemini-2.5-flash Free tier Good Fast Ollama llama3 Free (local) Good Depends on HW DeepSeek deepseek-chat ~$0.07 Excellent Fast Mistral mistral-small-latest ~$0.50 Good Fast OpenAI gpt-4o-mini ~$0.15 Excellent Fast Anthropic claude-sonnet-4-20250514 ~$1.50 Excellent Fast <p>Recommendation: Use Groq free tier for development and testing. Ollama is great for local development. Both Groq and Gemini offer generous free tiers suitable for most research use cases.</p>"},{"location":"architecture/llm_integration/#prompt-design","title":"Prompt Design","text":""},{"location":"architecture/llm_integration/#edge-existence-query","title":"Edge Existence Query","text":"<pre><code>System: You are an expert in causal reasoning and domain knowledge. \nYour task is to assess whether a causal relationship exists between two variables.\nRespond in JSON format with: exists (true/false/null), direction (a_to_b/b_to_a/undirected/null), confidence (0-1), reasoning (string).\n\nUser: In the domain of {domain}, does a causal relationship exist between \"{node_a}\" and \"{node_b}\"?\nConsider:\n- Direct causation (A causes B)\n- Reverse causation (B causes A)  \n- Bidirectional/feedback relationships\n- No causal relationship (correlation only or independence)\n\nVariable context:\n{variable_descriptions}\n</code></pre>"},{"location":"architecture/llm_integration/#response-format","title":"Response Format","text":"<pre><code>{\n  \"exists\": true,\n  \"direction\": \"a_to_b\",\n  \"confidence\": 0.85,\n  \"reasoning\": \"Smoking is an established cause of lung cancer through well-documented biological mechanisms including DNA damage from carcinogens in tobacco smoke.\"\n}\n</code></pre>"},{"location":"architecture/llm_integration/#multi-llm-consensus","title":"Multi-LLM Consensus","text":"<p>When multiple models are configured, responses are combined:</p>"},{"location":"architecture/llm_integration/#weighted-vote-strategy-default","title":"Weighted Vote Strategy (default)","text":"<pre><code>def weighted_vote(responses: list[EdgeKnowledge]) -&gt; EdgeKnowledge:\n    \"\"\"Combine responses weighted by confidence.\"\"\"\n    # For existence: weighted majority vote\n    # For direction: weighted majority among those agreeing on existence\n    # Final confidence: average confidence of agreeing models\n    # Reasoning: concatenate key points from each model\n</code></pre>"},{"location":"architecture/llm_integration/#highest-confidence-strategy","title":"Highest Confidence Strategy","text":"<pre><code>def highest_confidence(responses: list[EdgeKnowledge]) -&gt; EdgeKnowledge:\n    \"\"\"Return response with highest confidence.\"\"\"\n    return max(responses, key=lambda r: r.confidence)\n</code></pre>"},{"location":"architecture/llm_integration/#integration-with-graph-averaging","title":"Integration with Graph Averaging","text":"<p>The primary consumer is <code>causaliq_analysis.graph.average()</code>:</p> <pre><code># Current output from average()\ndf = average(traces, sample_size=1000)\n# Returns: node_a, node_b, p_a_to_b, p_b_to_a, p_undirected, p_no_edge\n\n# Entropy calculation identifies uncertain edges\ndef edge_entropy(row):\n    probs = [row.p_a_to_b, row.p_b_to_a, row.p_undirected, row.p_no_edge]\n    probs = [p for p in probs if p &gt; 0]\n    return -sum(p * math.log2(p) for p in probs)\n\ndf[\"entropy\"] = df.apply(edge_entropy, axis=1)\nuncertain_edges = df[df[\"entropy\"] &gt; 1.5]  # High uncertainty\n\n# Query LLM for uncertain edges\nknowledge = LLMKnowledge(models=[\"groq/llama-3.1-8b-instant\"])\nfor _, row in uncertain_edges.iterrows():\n    result = knowledge.query_edge(row.node_a, row.node_b)\n    # Combine statistical and LLM probabilities...\n</code></pre>"},{"location":"architecture/llm_integration/#design-rationale","title":"Design Rationale","text":""},{"location":"architecture/llm_integration/#why-vendor-specific-apis-not-litellmlangchain","title":"Why Vendor-Specific APIs (not LiteLLM/LangChain)?","text":"<ol> <li>Minimal dependencies: Only httpx for HTTP, no wrapper libraries</li> <li>Reliability: No wrapper bugs or version conflicts to debug</li> <li>Full control: Direct access to vendor-specific features and error handling</li> <li>Predictable: Behavior doesn't change when wrapper library updates</li> <li>Debuggable: Clear stack traces without abstraction layers</li> <li>Lightweight: ~5KB of client code vs ~50MB of wrapper dependencies</li> </ol>"},{"location":"architecture/llm_integration/#why-structured-json-responses","title":"Why structured JSON responses?","text":"<ol> <li>Reliable parsing: Avoids regex/heuristic extraction</li> <li>Validation: Pydantic ensures response integrity</li> <li>Consistency: Same structure regardless of model</li> </ol>"},{"location":"architecture/llm_integration/#why-multi-model-consensus","title":"Why multi-model consensus?","text":"<ol> <li>Reduced hallucination: Multiple models catch individual errors</li> <li>Confidence calibration: Agreement increases confidence</li> <li>Robustness: Not dependent on single provider availability</li> </ol>"},{"location":"architecture/llm_integration/#error-handling-and-resilience","title":"Error Handling and Resilience","text":""},{"location":"architecture/llm_integration/#api-failures","title":"API Failures","text":"<ul> <li>Automatic retry with timeout handling</li> <li>Fallback to next model in list if primary fails</li> <li>Return <code>EdgeKnowledge(exists=None, confidence=0.0)</code> if all fail</li> </ul>"},{"location":"architecture/llm_integration/#invalid-responses","title":"Invalid Responses","text":"<ul> <li>Pydantic validation catches malformed JSON</li> <li>Default to <code>exists=None</code> if parsing fails</li> <li>Log warnings for debugging</li> </ul>"},{"location":"architecture/llm_integration/#rate-limiting","title":"Rate Limiting","text":"<ul> <li>Vendor clients handle rate limit errors gracefully</li> <li>Configure timeout per client</li> </ul>"},{"location":"architecture/llm_integration/#performance","title":"Performance","text":""},{"location":"architecture/llm_integration/#latency","title":"Latency","text":"<ul> <li>Single query: 0.5-2s depending on model/provider</li> <li>Batch queries: Can parallelize across edges (async)</li> <li>Cached queries: &lt;10ms</li> </ul>"},{"location":"architecture/llm_integration/#throughput-v030-with-caching","title":"Throughput (v0.3.0 with caching)","text":"<ul> <li>First query to new edge: 1-2s</li> <li>Cached query: &lt;10ms</li> <li>1000 unique edges: ~20-30 minutes (sequential), ~5 min (parallel)</li> </ul>"},{"location":"architecture/llm_integration/#future-extensions","title":"Future Extensions","text":""},{"location":"architecture/llm_integration/#v030-caching","title":"v0.3.0: Caching","text":"<ul> <li>Disk-based cache keyed by (node_a, node_b, context_hash)</li> <li>Semantic similarity cache for similar variable names</li> </ul>"},{"location":"architecture/llm_integration/#v040-rich-context","title":"v0.4.0: Rich Context","text":"<ul> <li>Variable descriptions and roles</li> <li>Domain-specific literature retrieval (RAG)</li> <li>Conversation history for follow-up queries</li> </ul>"},{"location":"architecture/llm_integration/#v050-algorithm-integration","title":"v0.5.0: Algorithm Integration","text":"<ul> <li>Direct integration with structure learning search</li> <li>Knowledge-guided constraint generation</li> </ul>"},{"location":"architecture/overview/","title":"Architecture Vision for causaliq-knowledge","text":""},{"location":"architecture/overview/#causaliq-ecosystem","title":"CausalIQ Ecosystem","text":"<p>causaliq-knowledge is a component of the overall CausalIQ ecosystem architecture.</p> <p>This package provides knowledge services to other CausalIQ packages, enabling them to incorporate LLM-derived and human-specified knowledge into causal discovery and inference workflows.</p>"},{"location":"architecture/overview/#architectural-principles","title":"Architectural Principles","text":""},{"location":"architecture/overview/#simplicity-first","title":"Simplicity First","text":"<ul> <li>Use lightweight libraries over heavy frameworks</li> <li>Start with minimal viable features, extend incrementally</li> <li>Prefer explicit code over framework \"magic\"</li> <li>Use vendor-specific APIs rather than abstraction wrappers</li> </ul>"},{"location":"architecture/overview/#cost-efficiency","title":"Cost Efficiency","text":"<ul> <li>Built-in cost tracking and budget management (critical for independent research)</li> <li>Caching of LLM queries and responses to avoid redundant API calls</li> <li>Support for cheap/free providers (Groq, Gemini free tiers)</li> </ul>"},{"location":"architecture/overview/#transparency-and-reproducibility","title":"Transparency and Reproducibility","text":"<ul> <li>Cache all LLM interactions for experiment reproducibility</li> <li>Provide reasoning/explanations with all knowledge outputs</li> <li>Log confidence levels to enable uncertainty-aware decisions</li> </ul>"},{"location":"architecture/overview/#clean-interfaces","title":"Clean Interfaces","text":"<ul> <li>Abstract <code>KnowledgeProvider</code> interface allows multiple implementations</li> <li>LLM-based, rule-based, and human-input knowledge sources use same interface</li> <li>Easy integration with causaliq-analysis and causaliq-discovery</li> </ul>"},{"location":"architecture/overview/#architecture-components","title":"Architecture Components","text":""},{"location":"architecture/overview/#core-components-v010","title":"Core Components (v0.1.0)","text":"<pre><code>causaliq_knowledge/\n\u251c\u2500\u2500 __init__.py              # Package exports\n\u251c\u2500\u2500 cli.py                   # Command-line interface\n\u251c\u2500\u2500 base.py                  # Abstract KnowledgeProvider interface\n\u251c\u2500\u2500 models.py                # Pydantic models (EdgeKnowledge, etc.)\n\u2514\u2500\u2500 llm/\n    \u251c\u2500\u2500 __init__.py          # LLM module exports\n    \u251c\u2500\u2500 groq_client.py       # Direct Groq API client\n    \u251c\u2500\u2500 gemini_client.py     # Direct Google Gemini API client\n    \u251c\u2500\u2500 prompts.py           # Prompt templates for edge queries\n    \u2514\u2500\u2500 provider.py          # LLMKnowledge implementation\n</code></pre>"},{"location":"architecture/overview/#data-flow","title":"Data Flow","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                 Consuming Package (e.g., causaliq-analysis)     \u2502\n\u2502                                                                 \u2502\n\u2502   uncertain_edges = df[df[\"entropy\"] &gt; threshold]               \u2502\n\u2502                          \u2502                                      \u2502\n\u2502                          \u25bc                                      \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502   \u2502              causaliq-knowledge                          \u2502   \u2502\n\u2502   \u2502                                                          \u2502   \u2502\n\u2502   \u2502   knowledge.query_edge(\"smoking\", \"cancer\")              \u2502   \u2502\n\u2502   \u2502       \u2502                                                  \u2502   \u2502\n\u2502   \u2502       \u25bc                                                  \u2502   \u2502\n\u2502   \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u2502   \u2502\n\u2502   \u2502   \u2502  LLM 1    \u2502    \u2502  LLM 2    \u2502    \u2502  Cache    \u2502       \u2502   \u2502\n\u2502   \u2502   \u2502 (GPT-4o)  \u2502    \u2502 (Llama3)  \u2502    \u2502 (disk)    \u2502       \u2502   \u2502\n\u2502   \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       \u2502   \u2502\n\u2502   \u2502       \u2502                 \u2502                \u2502               \u2502   \u2502\n\u2502   \u2502       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518               \u2502   \u2502\n\u2502   \u2502                         \u2502                                \u2502   \u2502\n\u2502   \u2502                         \u25bc                                \u2502   \u2502\n\u2502   \u2502               EdgeKnowledge(                             \u2502   \u2502\n\u2502   \u2502                   exists=True,                           \u2502   \u2502\n\u2502   \u2502                   direction=\"a_to_b\",                    \u2502   \u2502\n\u2502   \u2502                   confidence=0.85,                       \u2502   \u2502\n\u2502   \u2502                   reasoning=\"Established medical...\"     \u2502   \u2502\n\u2502   \u2502               )                                          \u2502   \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2502                          \u2502                                      \u2502\n\u2502                          \u25bc                                      \u2502\n\u2502   Combine with statistical probabilities                        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture/overview/#technology-choices","title":"Technology Choices","text":""},{"location":"architecture/overview/#vendor-specific-apis-over-wrapper-libraries","title":"Vendor-Specific APIs over Wrapper Libraries","text":"<p>We use direct vendor-specific API clients rather than wrapper libraries like LiteLLM or LangChain. This architectural decision provides:</p> Aspect Direct APIs Wrapper Libraries Reliability \u2705 Full control, predictable \u274c Wrapper bugs, version drift Debugging \u2705 Clear stack traces \u274c Abstraction layers Dependencies \u2705 Minimal (httpx only) \u274c Heavy transitive deps API Coverage \u2705 Full vendor features \u274c Lowest common denominator Maintenance \u2705 We control updates \u274c Wait for wrapper updates <p>Why Not LiteLLM?</p> <ul> <li>Adds 50+ transitive dependencies</li> <li>Version conflicts with other packages</li> <li>Wrapper bugs mask vendor API issues</li> <li>We only need 2-3 providers, not 100+</li> </ul> <p>Why Not LangChain?</p> <ul> <li>Massive dependency footprint (~100MB+)</li> <li>Over-engineered for simple structured queries  </li> <li>Rapid breaking changes between versions</li> <li>May reconsider for v0.4.0+ RAG features only</li> </ul>"},{"location":"architecture/overview/#current-provider-clients","title":"Current Provider Clients","text":"<ul> <li>GroqClient: Direct Groq API via httpx (free tier, fast inference)</li> <li>GeminiClient: Direct Google Gemini API via httpx (generous free tier)</li> </ul>"},{"location":"architecture/overview/#key-dependencies","title":"Key Dependencies","text":"<ul> <li>httpx: HTTP client for API calls</li> <li>pydantic: Structured response validation</li> <li>click: Command-line interface</li> <li>diskcache (v0.3.0): Persistent query caching</li> </ul>"},{"location":"architecture/overview/#integration-points","title":"Integration Points","text":""},{"location":"architecture/overview/#with-causaliq-analysis","title":"With causaliq-analysis","text":"<p>The primary integration point is the <code>average()</code> function which produces edge probability tables. Future versions will accept a <code>knowledge</code> parameter:</p> <pre><code># Future usage (v0.5.0 of causaliq-analysis)\nfrom causaliq_knowledge import LLMKnowledge\n\nknowledge = LLMKnowledge(models=[\"gpt-4o-mini\"])\ndf = average(traces, sample_size=1000, knowledge=knowledge)\n</code></pre>"},{"location":"architecture/overview/#with-causaliq-discovery","title":"With causaliq-discovery","text":"<p>Structure learning algorithms will use knowledge to guide search in uncertain areas of the graph space.</p>"},{"location":"architecture/overview/#see-also","title":"See Also","text":"<ul> <li>LLM Integration Design Note - Detailed design for LLM queries</li> <li>Roadmap - Release planning</li> </ul>"},{"location":"architecture/testing_strategy/","title":"Testing Strategy Design Note","text":""},{"location":"architecture/testing_strategy/#overview","title":"Overview","text":"<p>Testing LLM-dependent code presents unique challenges: API calls cost money, responses are non-deterministic, and external services may be unavailable. This document describes the testing strategy for causaliq-knowledge.</p>"},{"location":"architecture/testing_strategy/#testing-pyramid","title":"Testing Pyramid","text":"<pre><code>                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502   Functional    \u2502  \u2190 Cached responses (v0.3.0+)\n                    \u2502     Tests       \u2502     Real scenarios, reproducible\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n               \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n               \u2502     Integration Tests     \u2502  \u2190 Real API calls (optional)\n               \u2502   (with live LLM APIs)    \u2502     Expensive, non-deterministic\n               \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502                  Unit Tests                      \u2502  \u2190 Mocked LLM responses\n    \u2502           (mocked LLM responses)                 \u2502     Fast, free, deterministic\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture/testing_strategy/#test-categories","title":"Test Categories","text":""},{"location":"architecture/testing_strategy/#1-unit-tests-always-run-in-ci","title":"1. Unit Tests (Always Run in CI)","text":"<p>Unit tests mock all LLM calls, making them:</p> <ul> <li>Fast: No network latency</li> <li>Free: No API costs</li> <li>Deterministic: Same result every time</li> <li>Isolated: No external dependencies</li> </ul> <pre><code># tests/unit/test_llm_providers.py\nimport pytest\nfrom unittest.mock import MagicMock\n\n\ndef test_query_edge_parses_valid_response(monkeypatch):\n    \"\"\"Test that valid LLM JSON is correctly parsed.\"\"\"\n    from causaliq_knowledge.llm import LLMKnowledge\n    from causaliq_knowledge.llm.groq_client import GroqClient\n\n    # Mock the Groq client's complete_json method\n    mock_json = {\n        \"exists\": True,\n        \"direction\": \"a_to_b\",\n        \"confidence\": 0.85,\n        \"reasoning\": \"Smoking causes lung cancer via carcinogens.\"\n    }\n\n    mock_client = MagicMock(spec=GroqClient)\n    mock_client.complete_json.return_value = (mock_json, MagicMock())\n\n    knowledge = LLMKnowledge(models=[\"groq/llama-3.1-8b-instant\"])\n    knowledge._clients[\"groq/llama-3.1-8b-instant\"] = mock_client\n\n    result = knowledge.query_edge(\"smoking\", \"lung_cancer\")\n\n    assert result.exists is True\n    assert result.direction.value == \"a_to_b\"\n    assert result.confidence == 0.85\n\n\ndef test_query_edge_handles_malformed_json(monkeypatch):\n    \"\"\"Test graceful handling of invalid LLM response.\"\"\"\n    from causaliq_knowledge.llm import LLMKnowledge\n    from causaliq_knowledge.llm.groq_client import GroqClient\n\n    # Mock returning None (failed parse)\n    mock_client = MagicMock(spec=GroqClient)\n    mock_client.complete_json.return_value = (None, MagicMock())\n\n    knowledge = LLMKnowledge(models=[\"groq/llama-3.1-8b-instant\"])\n    knowledge._clients[\"groq/llama-3.1-8b-instant\"] = mock_client\n\n    result = knowledge.query_edge(\"A\", \"B\")\n\n    assert result.exists is None  # Uncertain\n    assert result.confidence == 0.0\n</code></pre>"},{"location":"architecture/testing_strategy/#2-integration-tests-optional-manual-or-ci-with-secrets","title":"2. Integration Tests (Optional, Manual or CI with Secrets)","text":"<p>Integration tests use real LLM APIs to validate actual behavior:</p> <ul> <li>Expensive: May cost money per call (though free tiers available)</li> <li>Non-deterministic: LLM responses vary</li> <li>Slow: Network latency</li> <li>Validates real integration: Catches API changes</li> </ul> <pre><code># tests/integration/test_llm_live.py\nimport pytest\nimport os\n\npytestmark = pytest.mark.skipif(\n    not os.getenv(\"GROQ_API_KEY\"),\n    reason=\"GROQ_API_KEY not set\"\n)\n\n@pytest.mark.slow\n@pytest.mark.integration\ndef test_groq_returns_valid_response():\n    \"\"\"Validate real Groq API returns parseable response.\"\"\"\n    from causaliq_knowledge.llm import LLMKnowledge\n\n    knowledge = LLMKnowledge(models=[\"groq/llama-3.1-8b-instant\"])\n    result = knowledge.query_edge(\"smoking\", \"lung_cancer\")\n\n    # Don't assert specific values - LLM may vary\n    # Just validate structure and reasonable bounds\n    assert result.exists in [True, False, None]\n    assert 0.0 &lt;= result.confidence &lt;= 1.0\n    assert len(result.reasoning) &gt; 0\n</code></pre>"},{"location":"architecture/testing_strategy/#3-functional-tests-with-cached-responses-v030","title":"3. Functional Tests with Cached Responses (v0.3.0+)","text":"<p>Once response caching is implemented, we can create reproducible functional tests using cached LLM responses. This is the best of both worlds:</p> <ul> <li>Realistic: Uses actual LLM responses (captured once)</li> <li>Deterministic: Same cached response every time</li> <li>Free: No API calls after initial capture</li> <li>Fast: Disk read instead of network call</li> </ul>"},{"location":"architecture/testing_strategy/#how-it-works","title":"How It Works","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                     Test Fixture Generation                      \u2502\n\u2502                      (run once, manually)                        \u2502\n\u2502                                                                  \u2502\n\u2502   1. Run queries against real LLMs                               \u2502\n\u2502   2. Cache stores responses in tests/data/functional/cache/     \u2502\n\u2502   3. Commit cache files to git                                   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                     Functional Tests (CI)                        \u2502\n\u2502                                                                  \u2502\n\u2502   1. Load cached responses from tests/data/functional/cache/    \u2502\n\u2502   2. LLMKnowledge configured to use cache-only mode             \u2502\n\u2502   3. Tests run with real LLM responses, no API calls            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture/testing_strategy/#example-functional-test","title":"Example Functional Test","text":"<pre><code># tests/functional/test_edge_queries.py\nimport pytest\nfrom pathlib import Path\n\nCACHE_DIR = Path(__file__).parent / \"data\" / \"cache\"\n\n@pytest.fixture\ndef cached_knowledge():\n    \"\"\"LLMKnowledge using only cached responses.\"\"\"\n    return LLMKnowledge(\n        models=[\"groq/llama-3.1-8b-instant\"],\n        cache_dir=str(CACHE_DIR),\n        cache_only=True  # Fail if cache miss, don't call API\n    )\n\ndef test_smoking_cancer_relationship(cached_knowledge):\n    \"\"\"Test with cached response for smoking-&gt;cancer query.\"\"\"\n    result = cached_knowledge.query_edge(\"smoking\", \"lung_cancer\")\n\n    # Can assert specific values since response is cached\n    assert result.exists is True\n    assert result.direction == \"a_to_b\"\n    assert result.confidence &gt; 0.8\n\ndef test_consensus_across_models(cached_knowledge):\n    \"\"\"Test multi-model consensus with cached responses.\"\"\"\n    knowledge = LLMKnowledge(\n        models=[\"groq/llama-3.1-8b-instant\", \"gemini/gemini-2.5-flash\"],\n        cache_dir=str(CACHE_DIR),\n        cache_only=True\n    )\n    result = knowledge.query_edge(\"exercise\", \"heart_health\")\n\n    assert result.exists is True\n</code></pre>"},{"location":"architecture/testing_strategy/#generating-test-fixtures","title":"Generating Test Fixtures","text":"<pre><code># scripts/generate_test_fixtures.py\n\"\"\"\nRun this script manually to generate/update cached responses for functional tests.\nRequires API keys for all models being tested.\n\"\"\"\nfrom causaliq_knowledge.llm import LLMKnowledge\nfrom pathlib import Path\n\nCACHE_DIR = Path(\"tests/data/functional/cache\")\nTEST_EDGES = [\n    (\"smoking\", \"lung_cancer\"),\n    (\"exercise\", \"heart_health\"),\n    (\"education\", \"income\"),\n    (\"rain\", \"wet_ground\"),\n]\n\ndef generate_fixtures():\n    knowledge = LLMKnowledge(\n        models=[\"groq/llama-3.1-8b-instant\", \"gemini/gemini-2.5-flash\"],\n        cache_dir=str(CACHE_DIR)\n    )\n\n    for node_a, node_b in TEST_EDGES:\n        print(f\"Caching: {node_a} -&gt; {node_b}\")\n        knowledge.query_edge(node_a, node_b)\n\n    print(f\"Fixtures saved to {CACHE_DIR}\")\n\nif __name__ == \"__main__\":\n    generate_fixtures()\n</code></pre>"},{"location":"architecture/testing_strategy/#ci-configuration","title":"CI Configuration","text":""},{"location":"architecture/testing_strategy/#pytest-markers","title":"pytest Markers","text":"<pre><code># pyproject.toml\n[tool.pytest.ini_options]\nmarkers = [\n    \"slow: marks tests as slow (deselect with '-m \\\"not slow\\\"')\",\n    \"integration: marks tests requiring live external APIs\",\n    \"functional: marks functional tests using cached responses\",\n]\naddopts = \"-ra -q --strict-markers -m 'not slow and not integration'\"\n</code></pre>"},{"location":"architecture/testing_strategy/#github-actions-strategy","title":"GitHub Actions Strategy","text":"Test Type When API Keys Cost Unit Every push/PR \u274c Not needed Free Functional Every push/PR \u274c Uses cache Free Integration Main branch only, optional \u2705 GitHub Secrets ~$0.01/run <pre><code># .github/workflows/ci.yml (conceptual addition)\njobs:\n  unit-and-functional:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - name: Run unit and functional tests\n        run: pytest tests/unit tests/functional\n\n  integration:\n    runs-on: ubuntu-latest\n    if: github.ref == 'refs/heads/main'  # Only on main\n    steps:\n      - uses: actions/checkout@v4\n      - name: Run integration tests\n        env:\n          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}\n        run: pytest tests/integration -m integration\n</code></pre>"},{"location":"architecture/testing_strategy/#test-data-management","title":"Test Data Management","text":""},{"location":"architecture/testing_strategy/#directory-structure","title":"Directory Structure","text":"<pre><code>tests/\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 unit/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 test_models.py          # EdgeKnowledge, etc.\n\u2502   \u251c\u2500\u2500 test_prompts.py         # Prompt formatting\n\u2502   \u2514\u2500\u2500 test_llm_providers.py   # Mocked LLM calls\n\u251c\u2500\u2500 integration/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2514\u2500\u2500 test_llm_live.py        # Real API calls\n\u251c\u2500\u2500 functional/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2514\u2500\u2500 test_edge_queries.py    # Using cached responses\n\u2514\u2500\u2500 data/\n    \u2514\u2500\u2500 functional/\n        \u2514\u2500\u2500 cache/              # Committed to git\n            \u251c\u2500\u2500 groq/\n            \u2502   \u251c\u2500\u2500 smoking_lung_cancer.json\n            \u2502   \u2514\u2500\u2500 exercise_heart_health.json\n            \u2514\u2500\u2500 gemini/\n                \u2514\u2500\u2500 ...\n</code></pre>"},{"location":"architecture/testing_strategy/#cache-file-format","title":"Cache File Format","text":"<pre><code>{\n  \"query\": {\n    \"node_a\": \"smoking\",\n    \"node_b\": \"lung_cancer\",\n    \"context\": {\"domain\": \"epidemiology\"}\n  },\n  \"model\": \"groq/llama-3.1-8b-instant\",\n  \"timestamp\": \"2026-01-05T10:30:00Z\",\n  \"response\": {\n    \"exists\": true,\n    \"direction\": \"a_to_b\",\n    \"confidence\": 0.92,\n    \"reasoning\": \"Smoking is an established cause of lung cancer...\"\n  }\n}\n</code></pre>"},{"location":"architecture/testing_strategy/#benefits-of-this-strategy","title":"Benefits of This Strategy","text":"Benefit How Achieved Fast CI Unit tests are mocked, functional use cache Low cost Only integration tests (optional) call APIs Reproducible Cached responses are deterministic Realistic Functional tests use real LLM responses Stable experiments Same cache = same results across runs Version controlled Cache files in git track response changes"},{"location":"architecture/testing_strategy/#future-considerations","title":"Future Considerations","text":""},{"location":"architecture/testing_strategy/#cache-invalidation-for-tests","title":"Cache Invalidation for Tests","text":"<p>When updating test fixtures:</p> <ol> <li>Delete relevant cache files</li> <li>Run fixture generation script</li> <li>Review new responses</li> <li>Commit updated cache files</li> </ol>"},{"location":"architecture/testing_strategy/#model-version-tracking","title":"Model Version Tracking","text":"<p>Cache files should include model version to detect when responses might change due to model updates.</p>"},{"location":"architecture/testing_strategy/#semantic-similarity-testing","title":"Semantic Similarity Testing","text":"<p>For v0.4.0+, consider testing that semantically similar queries hit cache (e.g., \"smoking\" vs \"tobacco use\" \u2192 \"cancer\" vs \"lung cancer\").</p>"},{"location":"userguide/introduction/","title":"CausalIQ Knowledge User Guide","text":""},{"location":"userguide/introduction/#what-is-causaliq-knowledge","title":"What is CausalIQ Knowledge?","text":"<p>CausalIQ Knowledge is a Python package that provides knowledge services for causal discovery workflows. It enables you to query Large Language Models (LLMs) about potential causal relationships between variables, helping to resolve uncertainty in learned causal graphs.</p>"},{"location":"userguide/introduction/#primary-use-case","title":"Primary Use Case","text":"<p>When averaging multiple causal graphs learned from data subsamples, some edges may be uncertain - appearing in some graphs but not others, or with inconsistent directions. CausalIQ Knowledge helps resolve this uncertainty by querying LLMs about whether:</p> <ol> <li>A causal relationship exists between two variables</li> <li>What the direction of causation is (A\u2192B or B\u2192A)</li> </ol>"},{"location":"userguide/introduction/#quick-start","title":"Quick Start","text":""},{"location":"userguide/introduction/#installation","title":"Installation","text":"<pre><code>pip install causaliq-knowledge\n</code></pre>"},{"location":"userguide/introduction/#basic-usage","title":"Basic Usage","text":"<pre><code>from causaliq_knowledge.llm import LLMKnowledge\n\n# Initialize with Groq (default, free tier)\nknowledge = LLMKnowledge(models=[\"groq/llama-3.1-8b-instant\"])\n\n# Query about a potential edge\nresult = knowledge.query_edge(\n    node_a=\"smoking\",\n    node_b=\"lung_cancer\",\n    context={\"domain\": \"epidemiology\"}\n)\n\nprint(f\"Exists: {result.exists}\")\nprint(f\"Direction: {result.direction}\")\nprint(f\"Confidence: {result.confidence}\")\nprint(f\"Reasoning: {result.reasoning}\")\n</code></pre>"},{"location":"userguide/introduction/#using-gemini-free-tier","title":"Using Gemini (Free Tier)","text":"<pre><code># Use Google Gemini for inference\nknowledge = LLMKnowledge(models=[\"gemini/gemini-2.5-flash\"])\n</code></pre>"},{"location":"userguide/introduction/#multi-model-consensus","title":"Multi-Model Consensus","text":"<pre><code># Query multiple models for more robust answers\nknowledge = LLMKnowledge(\n    models=[\"groq/llama-3.1-8b-instant\", \"gemini/gemini-2.5-flash\"],\n    consensus_strategy=\"weighted_vote\"\n)\n</code></pre>"},{"location":"userguide/introduction/#llm-provider-setup","title":"LLM Provider Setup","text":"<p>CausalIQ Knowledge uses direct vendor-specific API clients (not wrapper libraries) to communicate with LLM providers. This approach provides reliability and minimal dependencies. Currently supported:</p> <ul> <li>Groq: Free tier with fast inference</li> <li>Google Gemini: Generous free tier</li> <li>OpenAI: GPT-4o and other models</li> <li>Anthropic: Claude models</li> <li>DeepSeek: DeepSeek-V3 and R1 models</li> <li>Mistral: Mistral AI models</li> <li>Ollama: Local LLMs (free, runs locally)</li> </ul>"},{"location":"userguide/introduction/#free-options","title":"Free Options","text":""},{"location":"userguide/introduction/#groq-free-tier-very-fast","title":"Groq (Free Tier - Very Fast)","text":"<p>Groq offers a generous free tier with extremely fast inference:</p> <ol> <li>Sign up at console.groq.com</li> <li>Create an API key</li> <li>Set the environment variable (see Storing API Keys)</li> <li>Use in code:</li> </ol> <pre><code>from causaliq_knowledge.llm import LLMKnowledge\n\nknowledge = LLMKnowledge(models=[\"groq/llama-3.1-8b-instant\"])\nresult = knowledge.query_edge(\"smoking\", \"lung_cancer\")\n</code></pre> <p>Available Groq models: <code>groq/llama-3.1-8b-instant</code>, <code>groq/llama-3.1-70b-versatile</code>, <code>groq/mixtral-8x7b-32768</code></p>"},{"location":"userguide/introduction/#google-gemini-free-tier","title":"Google Gemini (Free Tier)","text":"<p>Google offers free access to Gemini models:</p> <ol> <li>Sign up at aistudio.google.com/apikey</li> <li>Create an API key</li> <li>Set <code>GEMINI_API_KEY</code> environment variable</li> <li>Use in code:</li> </ol> <pre><code>knowledge = LLMKnowledge(models=[\"gemini/gemini-2.5-flash\"])\n</code></pre>"},{"location":"userguide/introduction/#openai","title":"OpenAI","text":"<p>OpenAI provides GPT-4o and other models:</p> <ol> <li>Sign up at platform.openai.com</li> <li>Create an API key</li> <li>Set <code>OPENAI_API_KEY</code> environment variable</li> </ol> <pre><code>knowledge = LLMKnowledge(models=[\"openai/gpt-4o-mini\"])\n</code></pre>"},{"location":"userguide/introduction/#anthropic","title":"Anthropic","text":"<p>Anthropic provides Claude models:</p> <ol> <li>Sign up at console.anthropic.com</li> <li>Create an API key</li> <li>Set <code>ANTHROPIC_API_KEY</code> environment variable</li> </ol> <pre><code>knowledge = LLMKnowledge(models=[\"anthropic/claude-sonnet-4-20250514\"])\n</code></pre>"},{"location":"userguide/introduction/#deepseek","title":"DeepSeek","text":"<p>DeepSeek offers high-quality models at competitive prices:</p> <ol> <li>Sign up at platform.deepseek.com</li> <li>Create an API key</li> <li>Set <code>DEEPSEEK_API_KEY</code> environment variable</li> </ol> <pre><code>knowledge = LLMKnowledge(models=[\"deepseek/deepseek-chat\"])\n</code></pre>"},{"location":"userguide/introduction/#mistral","title":"Mistral","text":"<p>Mistral AI provides models with EU data sovereignty:</p> <ol> <li>Sign up at console.mistral.ai</li> <li>Create an API key</li> <li>Set <code>MISTRAL_API_KEY</code> environment variable</li> </ol> <pre><code>knowledge = LLMKnowledge(models=[\"mistral/mistral-small-latest\"])\n</code></pre>"},{"location":"userguide/introduction/#ollama-local","title":"Ollama (Local)","text":"<p>Run models locally with Ollama (no API key needed):</p> <ol> <li>Install Ollama from ollama.ai</li> <li>Pull a model: <code>ollama pull llama3</code></li> <li>Use in code:</li> </ol> <pre><code>knowledge = LLMKnowledge(models=[\"ollama/llama3\"])\n</code></pre>"},{"location":"userguide/introduction/#storing-api-keys","title":"Storing API Keys","text":""},{"location":"userguide/introduction/#option-1-user-environment-variables-recommended","title":"Option 1: User Environment Variables (Recommended)","text":"<p>Set permanently for your user account on Windows:</p> <pre><code>[Environment]::SetEnvironmentVariable(\"GROQ_API_KEY\", \"your-key\", \"User\")\n[Environment]::SetEnvironmentVariable(\"GEMINI_API_KEY\", \"your-key\", \"User\")\n</code></pre> <p>On Linux/macOS, add to your <code>~/.bashrc</code> or <code>~/.zshrc</code>:</p> <pre><code>export GROQ_API_KEY=\"your-key\"\nexport GEMINI_API_KEY=\"your-key\"\n</code></pre> <p>Restart your terminal for changes to take effect.</p>"},{"location":"userguide/introduction/#option-2-project-env-file","title":"Option 2: Project <code>.env</code> File","text":"<p>Create a <code>.env</code> file in your project root:</p> <pre><code>GROQ_API_KEY=your-key-here\nGEMINI_API_KEY=your-key-here\n</code></pre> <p>Important: Add <code>.env</code> to your <code>.gitignore</code> so keys aren't committed to version control!</p>"},{"location":"userguide/introduction/#option-3-password-manager","title":"Option 3: Password Manager","text":"<p>Store API keys in a password manager (LastPass, 1Password, Bitwarden, etc.) as a Secure Note. This provides:</p> <ul> <li>Encrypted backup of your keys</li> <li>Access from any machine</li> <li>Secure sharing with team members if needed</li> </ul> <p>Copy keys from your password manager when setting environment variables.</p>"},{"location":"userguide/introduction/#verifying-your-setup","title":"Verifying Your Setup","text":"<p>Test your configuration with the CLI:</p> <pre><code># Using the installed CLI\ncqknow query smoking lung_cancer --model groq/llama-3.1-8b-instant\n\n# Or with Python module\npython -m causaliq_knowledge.cli query smoking lung_cancer --model ollama/llama3\n</code></pre>"},{"location":"userguide/introduction/#whats-next","title":"What's Next?","text":"<ul> <li>Architecture Overview - Understand how the package works</li> <li>LLM Integration Design - Detailed design documentation</li> <li>API Reference - Full API documentation</li> </ul>"}]}