{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"CausalIQ Knowledge","text":""},{"location":"#welcome","title":"Welcome","text":"<p>Welcome to the documentation for CausalIQ Knowledge, which combines traditional statistical structure learning algorithms with the contextual understanding and reasoning capabilities of Large Language Models. This integration enables more interpretable, domain-aware, and human-friendly causal discovery workflows. It is part of the CausalIQ ecosystem for intelligent causal discovery.</p>"},{"location":"#overview","title":"Overview","text":"<p>This site provides detailed documentation, including:</p> <ul> <li>Development roadmap</li> <li>User guide</li> <li>Architectural vision</li> <li>Design notes</li> <li>API reference for users and contributors</li> </ul>"},{"location":"#quickstart-installation","title":"Quickstart &amp; Installation","text":"<p>For a quickstart guide and installation instructions, see the README on GitHub.</p>"},{"location":"#documentation-contents","title":"Documentation Contents","text":"<ul> <li>Development Roadmap: Roadmap of upcoming features</li> <li>User Guide: Comprehensive user guide</li> <li>Architecture: Overall architecture and design notes</li> <li>API Reference: Complete reference for Python code</li> <li>Development Guidelines: CausalIQ guidelines for developers</li> <li>Changelog</li> <li>License</li> </ul>"},{"location":"#support-community","title":"Support &amp; Community","text":"<ul> <li>GitHub Issues: Report bugs or request features.</li> <li>GitHub Discussions: Ask questions and join the community.</li> </ul> <p>Tip: Use the navigation sidebar to explore the documentation. For the latest code and releases, visit the causaliq-knowledge GitHub repository.</p> <p>Supported Python Versions: 3.9, 3.10, 3.11, 3.12, 3.13 Default Python Version: 3.11</p>"},{"location":"roadmap/","title":"CausalIQ Knowledge - Development Roadmap","text":"<p>Last updated: January 10, 2026  </p> <p>This project roadmap fits into the overall ecosystem roadmap</p>"},{"location":"roadmap/#current-release","title":"\ud83c\udfaf Current Release","text":""},{"location":"roadmap/#release-v020-additional-llms-january-2026","title":"Release v0.2.0 - Additional LLMs [January 2026]","text":"<p>Expanded LLM provider support from 2 to 7 providers.</p> <p>Scope:</p> <ul> <li>OpenAI client for GPT-4o and GPT-4o-mini models</li> <li>Anthropic client for Claude models</li> <li>DeepSeek client for DeepSeek-V3 and R1 models</li> <li>Mistral client for Mistral AI models</li> <li>Ollama client for local LLM inference</li> <li>OpenAI-compatible base client for API-compatible providers</li> <li>Integration tests for all providers</li> <li>Cost estimation utilities for each provider</li> </ul>"},{"location":"roadmap/#previous-releases","title":"\u2705 Previous Releases","text":""},{"location":"roadmap/#release-v010-foundation-llm-january-2026","title":"Release v0.1.0 - Foundation LLM [January 2026]","text":"<p>Simple LLM queries to 1 or 2 LLMs about edge existence and orientation to support graph averaging.</p> <p>Delivered:</p> <ul> <li>Abstract <code>KnowledgeProvider</code> interface</li> <li><code>EdgeKnowledge</code> Pydantic model for structured responses</li> <li><code>LLMKnowledge</code> implementation using vendor-specific API clients</li> <li>Direct API clients for Groq and Google Gemini</li> <li>Single-model and multi-model consensus queries</li> <li>Basic prompt templates for edge existence/orientation</li> <li>CLI for testing queries</li> <li>100% test coverage</li> <li>Comprehensive documentation</li> </ul>"},{"location":"roadmap/#upcoming-releases","title":"\ud83d\udee3\ufe0f Upcoming Releases","text":""},{"location":"roadmap/#release-v030-llm-caching","title":"Release v0.3.0 - LLM Caching","text":"<ul> <li>Disk-based response caching (diskcache)</li> <li>Cache key: (node_a, node_b, context_hash, model)</li> <li>Cache invalidation strategies</li> <li>Semantic similarity caching (optional)</li> </ul>"},{"location":"roadmap/#release-v040-llm-context","title":"Release v0.4.0 - LLM Context","text":"<ul> <li>Variable descriptions and roles</li> <li>Domain context specification</li> <li>Literature retrieval (RAG) - evaluate lightweight alternatives to LangChain</li> <li>Vector store integration for document search</li> </ul>"},{"location":"roadmap/#release-v050-algorithm-integration","title":"Release v0.5.0 - Algorithm Integration","text":"<ul> <li>Integration hooks for structure learning algorithms</li> <li>Knowledge-guided constraint generation</li> <li>Integration with causaliq-analysis <code>average()</code> function</li> <li>Entropy-based automatic query triggering</li> </ul>"},{"location":"roadmap/#release-v060-legacy-reference","title":"Release v0.6.0 - Legacy Reference","text":"<ul> <li>Support for deriving knowledge from reference networks</li> <li>Migration of functionality from legacy discovery repo</li> </ul>"},{"location":"roadmap/#dependencies-evolution","title":"\ud83d\udce6 Dependencies Evolution","text":"<pre><code># v0.1.0 (current)\ndependencies = [\n    \"click&gt;=8.0.0\",\n    \"httpx&gt;=0.24.0\",\n    \"pydantic&gt;=2.0.0\",\n]\n\n# v0.3.0 (add)\n\"diskcache&gt;=5.0.0\"\n\n# v0.4.0 (evaluate - prefer lightweight solutions)\n# Consider: llama-index, simple RAG, or custom implementation\n# Avoid: langchain (heavy dependencies)\n</code></pre>"},{"location":"api/base/","title":"Base API Reference","text":"<p>Abstract base classes defining the knowledge provider interface.</p>"},{"location":"api/base/#knowledgeprovider","title":"KnowledgeProvider","text":""},{"location":"api/base/#causaliq_knowledge.base.KnowledgeProvider","title":"KnowledgeProvider","text":"<p>Abstract interface for all knowledge sources.</p> <p>This is the base class that all knowledge providers must implement. Knowledge providers can be LLM-based, rule-based, human-input based, or any other source of causal knowledge.</p> <p>The primary method is <code>query_edge()</code> which asks about the causal relationship between two variables.</p> Example <p>class MyKnowledgeProvider(KnowledgeProvider): ...     def query_edge(self, node_a, node_b, context=None): ...         # Implementation here ...         return EdgeKnowledge(exists=True, confidence=0.8, ...) ... provider = MyKnowledgeProvider() result = provider.query_edge(\"smoking\", \"cancer\")</p> <p>Methods:</p> <ul> <li> <code>query_edge</code>             \u2013              <p>Query whether a causal edge exists between two nodes.</p> </li> <li> <code>query_edges</code>             \u2013              <p>Query multiple edges at once.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>Return the name of this knowledge provider.</p> </li> </ul>"},{"location":"api/base/#causaliq_knowledge.base.KnowledgeProvider.name","title":"name  <code>property</code>","text":"<pre><code>name: str\n</code></pre> <p>Return the name of this knowledge provider.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>Class name by default. Subclasses may override.</p> </li> </ul>"},{"location":"api/base/#causaliq_knowledge.base.KnowledgeProvider.query_edge","title":"query_edge  <code>abstractmethod</code>","text":"<pre><code>query_edge(node_a: str, node_b: str, context: Optional[dict] = None) -&gt; EdgeKnowledge\n</code></pre> <p>Query whether a causal edge exists between two nodes.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>EdgeKnowledge</code>           \u2013            <p>EdgeKnowledge with: - exists: True, False, or None (uncertain) - direction: \"a_to_b\", \"b_to_a\", \"undirected\", or None - confidence: 0.0 to 1.0 - reasoning: Human-readable explanation - model: Source identifier (optional)</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>NotImplementedError</code>             \u2013            <p>If not implemented by subclass.</p> </li> </ul>"},{"location":"api/base/#causaliq_knowledge.base.KnowledgeProvider.query_edge(node_a)","title":"<code>node_a</code>","text":"(<code>str</code>)           \u2013            <p>Name of the first variable.</p>"},{"location":"api/base/#causaliq_knowledge.base.KnowledgeProvider.query_edge(node_b)","title":"<code>node_b</code>","text":"(<code>str</code>)           \u2013            <p>Name of the second variable.</p>"},{"location":"api/base/#causaliq_knowledge.base.KnowledgeProvider.query_edge(context)","title":"<code>context</code>","text":"(<code>Optional[dict]</code>, default:                   <code>None</code> )           \u2013            <p>Optional context dictionary that may include: - domain: The domain (e.g., \"medicine\", \"economics\") - descriptions: Dict mapping variable names to descriptions - additional_info: Any other relevant context</p>"},{"location":"api/base/#causaliq_knowledge.base.KnowledgeProvider.query_edges","title":"query_edges","text":"<pre><code>query_edges(\n    edges: list[tuple[str, str]], context: Optional[dict] = None\n) -&gt; list[EdgeKnowledge]\n</code></pre> <p>Query multiple edges at once.</p> <p>Default implementation calls query_edge for each pair. Subclasses may override for batch optimization.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>list[EdgeKnowledge]</code>           \u2013            <p>List of EdgeKnowledge results, one per edge pair.</p> </li> </ul>"},{"location":"api/base/#causaliq_knowledge.base.KnowledgeProvider.query_edges(edges)","title":"<code>edges</code>","text":"(<code>list[tuple[str, str]]</code>)           \u2013            <p>List of (node_a, node_b) tuples to query.</p>"},{"location":"api/base/#causaliq_knowledge.base.KnowledgeProvider.query_edges(context)","title":"<code>context</code>","text":"(<code>Optional[dict]</code>, default:                   <code>None</code> )           \u2013            <p>Optional context dictionary (shared across all queries).</p>"},{"location":"api/base_client/","title":"LLM Client Base Interface","text":"<p>Abstract base class and common types for all LLM vendor clients. This module defines the interface that all vendor-specific clients must implement, ensuring consistent behavior across different LLM providers.</p>"},{"location":"api/base_client/#overview","title":"Overview","text":"<p>The base client module provides:</p> <ul> <li>BaseLLMClient - Abstract base class defining the client interface</li> <li>LLMConfig - Base configuration dataclass for all clients</li> <li>LLMResponse - Unified response format from any LLM provider</li> </ul>"},{"location":"api/base_client/#design-philosophy","title":"Design Philosophy","text":"<p>We use vendor-specific API clients rather than wrapper libraries like LiteLLM or LangChain. This provides:</p> <ul> <li>Minimal dependencies (httpx only for HTTP)</li> <li>Reliable and predictable behavior</li> <li>Easy debugging without abstraction layers</li> <li>Full control over API interactions</li> </ul> <p>The abstract interface ensures that all vendor clients behave consistently, making it easy to swap providers or add new ones.</p>"},{"location":"api/base_client/#usage","title":"Usage","text":"<p>Vendor-specific clients inherit from <code>BaseLLMClient</code>:</p> <pre><code>from causaliq_knowledge.llm import (\n    BaseLLMClient,\n    LLMConfig,\n    LLMResponse,\n    GroqClient,\n    GeminiClient,\n)\n\n# All clients share the same interface\ndef query_llm(client: BaseLLMClient, prompt: str) -&gt; str:\n    messages = [{\"role\": \"user\", \"content\": prompt}]\n    response = client.completion(messages)\n    return response.content\n\n# Works with any client\ngroq = GroqClient()\ngemini = GeminiClient()\n\nresult1 = query_llm(groq, \"What is 2+2?\")\nresult2 = query_llm(gemini, \"What is 2+2?\")\n</code></pre>"},{"location":"api/base_client/#creating-a-custom-client","title":"Creating a Custom Client","text":"<p>To add support for a new LLM provider, implement the <code>BaseLLMClient</code> interface:</p> <pre><code>from causaliq_knowledge.llm import BaseLLMClient, LLMConfig, LLMResponse\n\nclass MyCustomClient(BaseLLMClient):\n    def __init__(self, config: LLMConfig) -&gt; None:\n        self.config = config\n        self._total_calls = 0\n\n    @property\n    def provider_name(self) -&gt; str:\n        return \"my_provider\"\n\n    def completion(self, messages, **kwargs) -&gt; LLMResponse:\n        # Implement API call here\n        ...\n        return LLMResponse(\n            content=\"response text\",\n            model=self.config.model,\n            input_tokens=10,\n            output_tokens=20,\n        )\n\n    @property\n    def call_count(self) -&gt; int:\n        return self._total_calls\n</code></pre>"},{"location":"api/base_client/#llmconfig","title":"LLMConfig","text":""},{"location":"api/base_client/#causaliq_knowledge.llm.base_client.LLMConfig","title":"LLMConfig  <code>dataclass</code>","text":"<pre><code>LLMConfig(\n    model: str,\n    temperature: float = 0.1,\n    max_tokens: int = 500,\n    timeout: float = 30.0,\n    api_key: Optional[str] = None,\n)\n</code></pre> <p>Base configuration for all LLM clients.</p> <p>This dataclass defines common configuration options shared by all LLM provider clients. Vendor-specific clients may extend this with additional options.</p> <p>Attributes:</p> <ul> <li> <code>model</code>               (<code>str</code>)           \u2013            <p>Model identifier (provider-specific format).</p> </li> <li> <code>temperature</code>               (<code>float</code>)           \u2013            <p>Sampling temperature (0.0=deterministic, 1.0=creative).</p> </li> <li> <code>max_tokens</code>               (<code>int</code>)           \u2013            <p>Maximum tokens in the response.</p> </li> <li> <code>timeout</code>               (<code>float</code>)           \u2013            <p>Request timeout in seconds.</p> </li> <li> <code>api_key</code>               (<code>Optional[str]</code>)           \u2013            <p>API key for authentication (optional, can use env var).</p> </li> </ul>"},{"location":"api/base_client/#llmresponse","title":"LLMResponse","text":""},{"location":"api/base_client/#causaliq_knowledge.llm.base_client.LLMResponse","title":"LLMResponse  <code>dataclass</code>","text":"<pre><code>LLMResponse(\n    content: str,\n    model: str,\n    input_tokens: int = 0,\n    output_tokens: int = 0,\n    cost: float = 0.0,\n    raw_response: Optional[Dict[str, Any]] = None,\n)\n</code></pre> <p>Standard response from any LLM client.</p> <p>This dataclass provides a unified response format across all LLM providers, abstracting away provider-specific response structures.</p> <p>Attributes:</p> <ul> <li> <code>content</code>               (<code>str</code>)           \u2013            <p>The text content of the response.</p> </li> <li> <code>model</code>               (<code>str</code>)           \u2013            <p>The model that generated the response.</p> </li> <li> <code>input_tokens</code>               (<code>int</code>)           \u2013            <p>Number of input/prompt tokens used.</p> </li> <li> <code>output_tokens</code>               (<code>int</code>)           \u2013            <p>Number of output/completion tokens generated.</p> </li> <li> <code>cost</code>               (<code>float</code>)           \u2013            <p>Estimated cost of the request (if available).</p> </li> <li> <code>raw_response</code>               (<code>Optional[Dict[str, Any]]</code>)           \u2013            <p>The original provider-specific response (for debugging).</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>parse_json</code>             \u2013              <p>Parse content as JSON, handling common formatting issues.</p> </li> </ul>"},{"location":"api/base_client/#causaliq_knowledge.llm.base_client.LLMResponse.parse_json","title":"parse_json","text":"<pre><code>parse_json() -&gt; Optional[Dict[str, Any]]\n</code></pre> <p>Parse content as JSON, handling common formatting issues.</p> <p>LLMs sometimes wrap JSON in markdown code blocks. This method handles those cases and attempts to extract valid JSON.</p> <p>Returns:</p> <ul> <li> <code>Optional[Dict[str, Any]]</code>           \u2013            <p>Parsed JSON as dict, or None if parsing fails.</p> </li> </ul>"},{"location":"api/base_client/#basellmclient","title":"BaseLLMClient","text":""},{"location":"api/base_client/#causaliq_knowledge.llm.base_client.BaseLLMClient","title":"BaseLLMClient","text":"<pre><code>BaseLLMClient(config: LLMConfig)\n</code></pre> <p>Abstract base class for LLM clients.</p> <p>All LLM vendor clients (OpenAI, Anthropic, Groq, Gemini, Llama, etc.) must implement this interface to ensure consistent behavior across the codebase.</p> <p>This abstraction allows: - Easy addition of new LLM providers - Consistent API for all providers - Provider-agnostic code in higher-level modules - Simplified testing with mock implementations</p> Example <p>class MyClient(BaseLLMClient): ...     def completion(self, messages, **kwargs): ...         # Implementation here ...         pass ... client = MyClient(config) msgs = [{\"role\": \"user\", \"content\": \"Hello\"}] response = client.completion(msgs) print(response.content)</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>complete_json</code>             \u2013              <p>Make a completion request and parse response as JSON.</p> </li> <li> <code>completion</code>             \u2013              <p>Make a chat completion request.</p> </li> <li> <code>is_available</code>             \u2013              <p>Check if the LLM provider is available and configured.</p> </li> <li> <code>list_models</code>             \u2013              <p>List available models from the provider.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>call_count</code>               (<code>int</code>)           \u2013            <p>Return the number of API calls made by this client.</p> </li> <li> <code>model_name</code>               (<code>str</code>)           \u2013            <p>Return the model name being used.</p> </li> <li> <code>provider_name</code>               (<code>str</code>)           \u2013            <p>Return the name of the LLM provider.</p> </li> </ul>"},{"location":"api/base_client/#causaliq_knowledge.llm.base_client.BaseLLMClient(config)","title":"<code>config</code>","text":"(<code>LLMConfig</code>)           \u2013            <p>Configuration for the LLM client.</p>"},{"location":"api/base_client/#causaliq_knowledge.llm.base_client.BaseLLMClient.call_count","title":"call_count  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>call_count: int\n</code></pre> <p>Return the number of API calls made by this client.</p> <p>Returns:</p> <ul> <li> <code>int</code>           \u2013            <p>Total number of completion calls made.</p> </li> </ul>"},{"location":"api/base_client/#causaliq_knowledge.llm.base_client.BaseLLMClient.model_name","title":"model_name  <code>property</code>","text":"<pre><code>model_name: str\n</code></pre> <p>Return the model name being used.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>Model identifier string.</p> </li> </ul>"},{"location":"api/base_client/#causaliq_knowledge.llm.base_client.BaseLLMClient.provider_name","title":"provider_name  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>provider_name: str\n</code></pre> <p>Return the name of the LLM provider.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>Provider name (e.g., \"openai\", \"anthropic\", \"groq\").</p> </li> </ul>"},{"location":"api/base_client/#causaliq_knowledge.llm.base_client.BaseLLMClient.complete_json","title":"complete_json","text":"<pre><code>complete_json(\n    messages: List[Dict[str, str]], **kwargs: Any\n) -&gt; tuple[Optional[Dict[str, Any]], LLMResponse]\n</code></pre> <p>Make a completion request and parse response as JSON.</p> <p>Convenience method that calls completion() and attempts to parse the response content as JSON.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>tuple[Optional[Dict[str, Any]], LLMResponse]</code>           \u2013            <p>Tuple of (parsed JSON dict or None, raw LLMResponse).</p> </li> </ul>"},{"location":"api/base_client/#causaliq_knowledge.llm.base_client.BaseLLMClient.complete_json(messages)","title":"<code>messages</code>","text":"(<code>List[Dict[str, str]]</code>)           \u2013            <p>List of message dicts with \"role\" and \"content\" keys.</p>"},{"location":"api/base_client/#causaliq_knowledge.llm.base_client.BaseLLMClient.complete_json(**kwargs)","title":"<code>**kwargs</code>","text":"(<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Provider-specific options passed to completion().</p>"},{"location":"api/base_client/#causaliq_knowledge.llm.base_client.BaseLLMClient.completion","title":"completion  <code>abstractmethod</code>","text":"<pre><code>completion(messages: List[Dict[str, str]], **kwargs: Any) -&gt; LLMResponse\n</code></pre> <p>Make a chat completion request.</p> <p>This is the core method that sends a request to the LLM provider and returns a standardized response.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>LLMResponse</code>           \u2013            <p>LLMResponse with the generated content and metadata.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the API request fails or returns an error.</p> </li> </ul>"},{"location":"api/base_client/#causaliq_knowledge.llm.base_client.BaseLLMClient.completion(messages)","title":"<code>messages</code>","text":"(<code>List[Dict[str, str]]</code>)           \u2013            <p>List of message dicts with \"role\" and \"content\" keys. Roles can be: \"system\", \"user\", \"assistant\".</p>"},{"location":"api/base_client/#causaliq_knowledge.llm.base_client.BaseLLMClient.completion(**kwargs)","title":"<code>**kwargs</code>","text":"(<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Provider-specific options (temperature, max_tokens, etc.) that override the config defaults.</p>"},{"location":"api/base_client/#causaliq_knowledge.llm.base_client.BaseLLMClient.is_available","title":"is_available  <code>abstractmethod</code>","text":"<pre><code>is_available() -&gt; bool\n</code></pre> <p>Check if the LLM provider is available and configured.</p> <p>This method checks whether the client can make API calls: - For cloud providers: checks if API key is set - For local providers: checks if server is running</p> <p>Returns:</p> <ul> <li> <code>bool</code>           \u2013            <p>True if the provider is available and ready for requests.</p> </li> </ul>"},{"location":"api/base_client/#causaliq_knowledge.llm.base_client.BaseLLMClient.list_models","title":"list_models  <code>abstractmethod</code>","text":"<pre><code>list_models() -&gt; List[str]\n</code></pre> <p>List available models from the provider.</p> <p>Queries the provider's API to get the list of models accessible with the current API key or configuration. Results are filtered by the user's subscription/access level.</p> <p>Returns:</p> <ul> <li> <code>List[str]</code>           \u2013            <p>List of model identifiers available for use.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the API request fails.</p> </li> </ul>"},{"location":"api/cli/","title":"CausalIQ Knowledge CLI","text":"<p>The command-line interface provides a quick way to test LLM queries about causal relationships.</p>"},{"location":"api/cli/#installation","title":"Installation","text":"<p>The CLI is automatically installed when you install the package:</p> <pre><code>pip install causaliq-knowledge\n</code></pre>"},{"location":"api/cli/#usage","title":"Usage","text":""},{"location":"api/cli/#basic-query","title":"Basic Query","text":"<pre><code># Query using default model (Groq)\ncqknow query smoking lung_cancer\n\n# With domain context\ncqknow query smoking lung_cancer --domain medicine\n</code></pre>"},{"location":"api/cli/#multiple-models","title":"Multiple Models","text":"<pre><code># Query multiple models for consensus\ncqknow query X Y --model groq/llama-3.1-8b-instant --model gemini/gemini-2.5-flash\n</code></pre>"},{"location":"api/cli/#json-output","title":"JSON Output","text":"<pre><code># Get structured JSON output\ncqknow query smoking lung_cancer --json\n</code></pre>"},{"location":"api/cli/#options","title":"Options","text":"Option Short Description <code>--model</code> <code>-m</code> LLM model to query (can be repeated) <code>--domain</code> <code>-d</code> Domain context (e.g., \"medicine\") <code>--strategy</code> <code>-s</code> Consensus strategy: weighted_vote or highest_confidence <code>--json</code> Output as JSON <code>--temperature</code> <code>-t</code> LLM temperature (0.0-1.0)"},{"location":"api/cli/#cli-entry-point","title":"CLI Entry Point","text":""},{"location":"api/cli/#causaliq_knowledge.cli","title":"cli","text":"<p>Command-line interface for causaliq-knowledge.</p> <p>Functions:</p> <ul> <li> <code>cli</code>             \u2013              <p>CausalIQ Knowledge - LLM knowledge for causal discovery.</p> </li> <li> <code>list_models</code>             \u2013              <p>List available LLM models from each provider.</p> </li> <li> <code>main</code>             \u2013              <p>Entry point for the CLI.</p> </li> <li> <code>query_edge</code>             \u2013              <p>Query LLMs about a causal relationship between two variables.</p> </li> </ul>"},{"location":"api/cli/#causaliq_knowledge.cli.cli","title":"cli","text":"<pre><code>cli() -&gt; None\n</code></pre> <p>CausalIQ Knowledge - LLM knowledge for causal discovery.</p> <p>Query LLMs about causal relationships between variables.</p>"},{"location":"api/cli/#causaliq_knowledge.cli.list_models","title":"list_models","text":"<pre><code>list_models(provider: Optional[str]) -&gt; None\n</code></pre> <p>List available LLM models from each provider.</p> <p>Queries each provider's API to show models accessible with your current configuration. Results are filtered by your API key's access level or locally installed models.</p> <p>Optionally specify PROVIDER to list models from a single provider: groq, anthropic, gemini, ollama, openai, deepseek, or mistral.</p> <p>Examples:</p> <pre><code>cqknow models              # List all providers\n\ncqknow models groq         # List only Groq models\n\ncqknow models mistral      # List only Mistral models\n</code></pre>"},{"location":"api/cli/#causaliq_knowledge.cli.main","title":"main","text":"<pre><code>main() -&gt; None\n</code></pre> <p>Entry point for the CLI.</p>"},{"location":"api/cli/#causaliq_knowledge.cli.query_edge","title":"query_edge","text":"<pre><code>query_edge(\n    node_a: str,\n    node_b: str,\n    model: tuple[str, ...],\n    domain: Optional[str],\n    strategy: str,\n    output_json: bool,\n    temperature: float,\n) -&gt; None\n</code></pre> <p>Query LLMs about a causal relationship between two variables.</p> <p>NODE_A and NODE_B are the variable names to query about.</p> <p>Examples:</p> <pre><code>cqknow query smoking lung_cancer\n\ncqknow query smoking lung_cancer --domain medicine\n\ncqknow query X Y --model groq/llama-3.1-8b-instant                          --model gemini/gemini-2.5-flash\n</code></pre>"},{"location":"api/models/","title":"Models API Reference","text":"<p>Core Pydantic models for representing causal knowledge.</p>"},{"location":"api/models/#edgedirection","title":"EdgeDirection","text":""},{"location":"api/models/#causaliq_knowledge.models.EdgeDirection","title":"EdgeDirection","text":"<p>Direction of a causal edge between two nodes.</p> <p>Attributes:</p> <ul> <li> <code>A_TO_B</code>           \u2013            </li> <li> <code>B_TO_A</code>           \u2013            </li> <li> <code>UNDIRECTED</code>           \u2013            </li> </ul>"},{"location":"api/models/#causaliq_knowledge.models.EdgeDirection.A_TO_B","title":"A_TO_B  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>A_TO_B = 'a_to_b'\n</code></pre>"},{"location":"api/models/#causaliq_knowledge.models.EdgeDirection.B_TO_A","title":"B_TO_A  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>B_TO_A = 'b_to_a'\n</code></pre>"},{"location":"api/models/#causaliq_knowledge.models.EdgeDirection.UNDIRECTED","title":"UNDIRECTED  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>UNDIRECTED = 'undirected'\n</code></pre>"},{"location":"api/models/#edgeknowledge","title":"EdgeKnowledge","text":""},{"location":"api/models/#causaliq_knowledge.models.EdgeKnowledge","title":"EdgeKnowledge","text":"<p>Structured knowledge about a potential causal edge.</p> <p>This model represents the result of querying a knowledge source about whether a causal relationship exists between two variables.</p> <p>Attributes:</p> <ul> <li> <code>exists</code>               (<code>Optional[bool]</code>)           \u2013            <p>Whether a causal edge exists. True, False, or None (uncertain).</p> </li> <li> <code>direction</code>               (<code>Optional[EdgeDirection]</code>)           \u2013            <p>The direction of the causal relationship if it exists. \"a_to_b\" means node_a causes node_b, \"b_to_a\" means the reverse, \"undirected\" means bidirectional or direction unknown.</p> </li> <li> <code>confidence</code>               (<code>float</code>)           \u2013            <p>Confidence score from 0.0 (no confidence) to 1.0 (certain).</p> </li> <li> <code>reasoning</code>               (<code>str</code>)           \u2013            <p>Human-readable explanation for the knowledge assessment.</p> </li> <li> <code>model</code>               (<code>Optional[str]</code>)           \u2013            <p>The LLM or knowledge source that provided this response.</p> </li> </ul> Example <p>knowledge = EdgeKnowledge( ...     exists=True, ...     direction=\"a_to_b\", ...     confidence=0.85, ...     reasoning=\"Smoking causes lung cancer.\", ...     model=\"gpt-4o-mini\" ... )</p> <p>Methods:</p> <ul> <li> <code>is_uncertain</code>             \u2013              <p>Check if this knowledge is uncertain.</p> </li> <li> <code>to_dict</code>             \u2013              <p>Convert to dictionary with string direction.</p> </li> <li> <code>uncertain</code>             \u2013              <p>Create an uncertain EdgeKnowledge instance.</p> </li> <li> <code>validate_direction</code>             \u2013              <p>Convert string direction to EdgeDirection enum.</p> </li> </ul>"},{"location":"api/models/#causaliq_knowledge.models.EdgeKnowledge.confidence","title":"confidence  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>confidence: float = Field(\n    default=0.0, ge=0.0, le=1.0, description=\"Confidence score from 0.0 to 1.0.\"\n)\n</code></pre>"},{"location":"api/models/#causaliq_knowledge.models.EdgeKnowledge.direction","title":"direction  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>direction: Optional[EdgeDirection] = Field(\n    default=None, description=\"Direction of the causal relationship if it exists.\"\n)\n</code></pre>"},{"location":"api/models/#causaliq_knowledge.models.EdgeKnowledge.exists","title":"exists  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>exists: Optional[bool] = Field(\n    default=None, description=\"Whether a causal edge exists. None = uncertain.\"\n)\n</code></pre>"},{"location":"api/models/#causaliq_knowledge.models.EdgeKnowledge.model","title":"model  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model: Optional[str] = Field(\n    default=None, description=\"The model/source that provided this knowledge.\"\n)\n</code></pre>"},{"location":"api/models/#causaliq_knowledge.models.EdgeKnowledge.reasoning","title":"reasoning  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>reasoning: str = Field(\n    default=\"\", description=\"Human-readable explanation for the assessment.\"\n)\n</code></pre>"},{"location":"api/models/#causaliq_knowledge.models.EdgeKnowledge.is_uncertain","title":"is_uncertain","text":"<pre><code>is_uncertain() -&gt; bool\n</code></pre> <p>Check if this knowledge is uncertain.</p> <p>Returns:</p> <ul> <li> <code>bool</code>           \u2013            <p>True if exists is None or confidence is below 0.5.</p> </li> </ul>"},{"location":"api/models/#causaliq_knowledge.models.EdgeKnowledge.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; dict\n</code></pre> <p>Convert to dictionary with string direction.</p> <p>Returns:</p> <ul> <li> <code>dict</code>           \u2013            <p>Dictionary representation suitable for JSON serialization.</p> </li> </ul>"},{"location":"api/models/#causaliq_knowledge.models.EdgeKnowledge.uncertain","title":"uncertain  <code>classmethod</code>","text":"<pre><code>uncertain(\n    reasoning: str = \"Unable to determine\", model: Optional[str] = None\n) -&gt; EdgeKnowledge\n</code></pre> <p>Create an uncertain EdgeKnowledge instance.</p> <p>Useful for error cases or when knowledge source cannot provide an answer.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>EdgeKnowledge</code>           \u2013            <p>EdgeKnowledge with exists=None and confidence=0.0.</p> </li> </ul>"},{"location":"api/models/#causaliq_knowledge.models.EdgeKnowledge.uncertain(reasoning)","title":"<code>reasoning</code>","text":"(<code>str</code>, default:                   <code>'Unable to determine'</code> )           \u2013            <p>Explanation for why the result is uncertain.</p>"},{"location":"api/models/#causaliq_knowledge.models.EdgeKnowledge.uncertain(model)","title":"<code>model</code>","text":"(<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>The model/source that was queried.</p>"},{"location":"api/models/#causaliq_knowledge.models.EdgeKnowledge.validate_direction","title":"validate_direction  <code>classmethod</code>","text":"<pre><code>validate_direction(v: Optional[str]) -&gt; Optional[EdgeDirection]\n</code></pre> <p>Convert string direction to EdgeDirection enum.</p>"},{"location":"api/overview/","title":"CausalIQ Knowledge API Reference","text":"<p>API documentation for causaliq-knowledge, organized by module.</p>"},{"location":"api/overview/#import-patterns","title":"Import Patterns","text":"<p>Core models are available from the top-level package:</p> <pre><code>from causaliq_knowledge import EdgeKnowledge, EdgeDirection, KnowledgeProvider\n</code></pre> <p>LLM-specific classes should be imported from the <code>llm</code> submodule:</p> <pre><code>from causaliq_knowledge.llm import (\n    # Abstract base interface\n    BaseLLMClient,\n    LLMConfig,\n    LLMResponse,\n    # Main provider\n    LLMKnowledge,\n    # Vendor clients\n    GroqClient,\n    GroqConfig,\n    GeminiClient,\n    GeminiConfig,\n    OpenAIClient,\n    OpenAIConfig,\n    AnthropicClient,\n    AnthropicConfig,\n    DeepSeekClient,\n    DeepSeekConfig,\n    MistralClient,\n    MistralConfig,\n    OllamaClient,\n    OllamaConfig,\n    # Prompts\n    EdgeQueryPrompt,\n    parse_edge_response,\n)\n</code></pre>"},{"location":"api/overview/#modules","title":"Modules","text":""},{"location":"api/overview/#models","title":"Models","text":"<p>Core Pydantic models for representing causal knowledge:</p> <ul> <li>EdgeDirection - Enum for causal edge direction (a_to_b, b_to_a, undirected)</li> <li>EdgeKnowledge - Structured knowledge about a potential causal edge</li> </ul>"},{"location":"api/overview/#base","title":"Base","text":"<p>Abstract interfaces for knowledge providers:</p> <ul> <li>KnowledgeProvider - Abstract base class that all knowledge sources implement</li> </ul>"},{"location":"api/overview/#llm-provider","title":"LLM Provider","text":"<p>Main entry point for LLM-based knowledge queries:</p> <ul> <li>LLMKnowledge - KnowledgeProvider implementation using vendor-specific API clients</li> <li>weighted_vote - Multi-model consensus by weighted voting</li> <li>highest_confidence - Select response with highest confidence</li> </ul>"},{"location":"api/overview/#llm-client-interface","title":"LLM Client Interface","text":"<p>Abstract base class and common types for LLM vendor clients:</p> <ul> <li>BaseLLMClient - Abstract interface all vendor clients implement</li> <li>LLMConfig - Base configuration dataclass</li> <li>LLMResponse - Unified response format</li> </ul>"},{"location":"api/overview/#vendor-api-clients","title":"Vendor API Clients","text":"<p>Direct API clients for specific LLM providers. All implement the <code>BaseLLMClient</code> interface.</p> <ul> <li>Groq Client - Fast inference via Groq API</li> <li>Gemini Client - Google Gemini API</li> <li>OpenAI Client - OpenAI GPT models</li> <li>Anthropic Client - Anthropic Claude models</li> <li>DeepSeek Client - DeepSeek models</li> <li>Mistral Client - Mistral AI models</li> <li>Ollama Client - Local LLMs via Ollama</li> </ul>"},{"location":"api/overview/#prompts","title":"Prompts","text":"<p>Prompt templates for LLM edge queries:</p> <ul> <li>EdgeQueryPrompt - Builder for edge existence/orientation prompts</li> <li>parse_edge_response - Parse LLM JSON responses to EdgeKnowledge</li> </ul>"},{"location":"api/overview/#cli","title":"CLI","text":"<p>Command-line interface for testing and querying.</p>"},{"location":"api/prompts/","title":"Prompts Module","text":"<p>The <code>prompts</code> module provides prompt templates and utilities for querying LLMs about causal relationships between variables.</p>"},{"location":"api/prompts/#overview","title":"Overview","text":"<p>This module contains:</p> <ul> <li>EdgeQueryPrompt: A dataclass for building prompts to query edge existence and orientation</li> <li>parse_edge_response: A function to parse LLM JSON responses into <code>EdgeKnowledge</code> objects</li> <li>Template constants: Pre-defined prompt templates for system and user messages</li> </ul>"},{"location":"api/prompts/#edgequeryprompt","title":"EdgeQueryPrompt","text":""},{"location":"api/prompts/#causaliq_knowledge.llm.prompts.EdgeQueryPrompt","title":"EdgeQueryPrompt  <code>dataclass</code>","text":"<pre><code>EdgeQueryPrompt(\n    node_a: str,\n    node_b: str,\n    domain: Optional[str] = None,\n    descriptions: Optional[dict[str, str]] = None,\n    system_prompt: Optional[str] = None,\n)\n</code></pre> <p>Builder for edge existence/orientation query prompts.</p> <p>This class constructs system and user prompts for querying an LLM about causal relationships between variables.</p> <p>Attributes:</p> <ul> <li> <code>node_a</code>               (<code>str</code>)           \u2013            <p>Name of the first variable.</p> </li> <li> <code>node_b</code>               (<code>str</code>)           \u2013            <p>Name of the second variable.</p> </li> <li> <code>domain</code>               (<code>Optional[str]</code>)           \u2013            <p>Optional domain context (e.g., \"medicine\", \"economics\").</p> </li> <li> <code>descriptions</code>               (<code>Optional[dict[str, str]]</code>)           \u2013            <p>Optional dict mapping variable names to descriptions.</p> </li> <li> <code>system_prompt</code>               (<code>Optional[str]</code>)           \u2013            <p>Custom system prompt (uses default if None).</p> </li> </ul> Example <p>prompt = EdgeQueryPrompt(\"smoking\", \"cancer\", domain=\"medicine\") system, user = prompt.build()</p> <p>Methods:</p> <ul> <li> <code>build</code>             \u2013              <p>Build the system and user prompts.</p> </li> <li> <code>from_context</code>             \u2013              <p>Create an EdgeQueryPrompt from a context dictionary.</p> </li> </ul>"},{"location":"api/prompts/#causaliq_knowledge.llm.prompts.EdgeQueryPrompt--use-with-llmclient","title":"Use with LLMClient","text":"<p>response = client.complete(system=system, user=user)</p>"},{"location":"api/prompts/#causaliq_knowledge.llm.prompts.EdgeQueryPrompt.build","title":"build","text":"<pre><code>build() -&gt; tuple[str, str]\n</code></pre> <p>Build the system and user prompts.</p> <p>Returns:</p> <ul> <li> <code>tuple[str, str]</code>           \u2013            <p>Tuple of (system_prompt, user_prompt).</p> </li> </ul> Source code in <code>src\\causaliq_knowledge\\llm\\prompts.py</code> <pre><code>def build(self) -&gt; tuple[str, str]:\n    \"\"\"Build the system and user prompts.\n\n    Returns:\n        Tuple of (system_prompt, user_prompt).\n    \"\"\"\n    system = self.system_prompt or DEFAULT_SYSTEM_PROMPT\n\n    # Build user prompt\n    if self.domain:\n        user = USER_PROMPT_WITH_DOMAIN_TEMPLATE.format(\n            domain=self.domain,\n            node_a=self.node_a,\n            node_b=self.node_b,\n        )\n    else:\n        user = USER_PROMPT_TEMPLATE.format(\n            node_a=self.node_a,\n            node_b=self.node_b,\n        )\n\n    # Add variable descriptions if provided\n    if self.descriptions:\n        desc_a = self.descriptions.get(self.node_a, \"No description\")\n        desc_b = self.descriptions.get(self.node_b, \"No description\")\n        user += VARIABLE_DESCRIPTIONS_TEMPLATE.format(\n            node_a=self.node_a,\n            desc_a=desc_a,\n            node_b=self.node_b,\n            desc_b=desc_b,\n        )\n\n    return system, user\n</code></pre>"},{"location":"api/prompts/#causaliq_knowledge.llm.prompts.EdgeQueryPrompt.from_context","title":"from_context  <code>classmethod</code>","text":"<pre><code>from_context(\n    node_a: str, node_b: str, context: Optional[dict] = None\n) -&gt; EdgeQueryPrompt\n</code></pre> <p>Create an EdgeQueryPrompt from a context dictionary.</p> <p>This is a convenience method for creating prompts from the context dict used by KnowledgeProvider.query_edge().</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>EdgeQueryPrompt</code>           \u2013            <p>EdgeQueryPrompt instance.</p> </li> </ul> Source code in <code>src\\causaliq_knowledge\\llm\\prompts.py</code> <pre><code>@classmethod\ndef from_context(\n    cls,\n    node_a: str,\n    node_b: str,\n    context: Optional[dict] = None,\n) -&gt; \"EdgeQueryPrompt\":\n    \"\"\"Create an EdgeQueryPrompt from a context dictionary.\n\n    This is a convenience method for creating prompts from the\n    context dict used by KnowledgeProvider.query_edge().\n\n    Args:\n        node_a: Name of the first variable.\n        node_b: Name of the second variable.\n        context: Optional context dict with keys:\n            - domain: str\n            - descriptions: dict[str, str]\n            - system_prompt: str\n\n    Returns:\n        EdgeQueryPrompt instance.\n    \"\"\"\n    if context is None:\n        return cls(node_a=node_a, node_b=node_b)\n\n    return cls(\n        node_a=node_a,\n        node_b=node_b,\n        domain=context.get(\"domain\"),\n        descriptions=context.get(\"descriptions\"),\n        system_prompt=context.get(\"system_prompt\"),\n    )\n</code></pre>"},{"location":"api/prompts/#causaliq_knowledge.llm.prompts.EdgeQueryPrompt.from_context(node_a)","title":"<code>node_a</code>","text":"(<code>str</code>)           \u2013            <p>Name of the first variable.</p>"},{"location":"api/prompts/#causaliq_knowledge.llm.prompts.EdgeQueryPrompt.from_context(node_b)","title":"<code>node_b</code>","text":"(<code>str</code>)           \u2013            <p>Name of the second variable.</p>"},{"location":"api/prompts/#causaliq_knowledge.llm.prompts.EdgeQueryPrompt.from_context(context)","title":"<code>context</code>","text":"(<code>Optional[dict]</code>, default:                   <code>None</code> )           \u2013            <p>Optional context dict with keys: - domain: str - descriptions: dict[str, str] - system_prompt: str</p>"},{"location":"api/prompts/#usage-example","title":"Usage Example","text":"<pre><code>from causaliq_knowledge.llm import EdgeQueryPrompt\nfrom causaliq_knowledge.llm import GroqClient, GroqConfig\n\n# Create a prompt for querying the relationship between two variables\nprompt = EdgeQueryPrompt(\n    node_a=\"smoking\",\n    node_b=\"lung_cancer\",\n    domain=\"medicine\",\n    descriptions={\n        \"smoking\": \"Tobacco consumption frequency\",\n        \"lung_cancer\": \"Diagnosis of lung cancer\",\n    },\n)\n\n# Build the system and user prompts\nsystem_prompt, user_prompt = prompt.build()\n\n# Use with GroqClient\nconfig = GroqConfig(model=\"llama-3.1-8b-instant\")\nclient = GroqClient(config=config)\nmessages = [\n    {\"role\": \"system\", \"content\": system_prompt},\n    {\"role\": \"user\", \"content\": user_prompt},\n]\njson_data, response = client.complete_json(messages)\n</code></pre>"},{"location":"api/prompts/#using-from_context","title":"Using from_context","text":"<p>The <code>from_context</code> class method provides a convenient way to create prompts from a context dictionary, which is the format used by <code>KnowledgeProvider.query_edge()</code>:</p> <pre><code>context = {\n    \"domain\": \"economics\",\n    \"descriptions\": {\n        \"interest_rate\": \"Central bank interest rate\",\n        \"inflation\": \"Consumer price index change\",\n    },\n}\n\nprompt = EdgeQueryPrompt.from_context(\n    node_a=\"interest_rate\",\n    node_b=\"inflation\",\n    context=context,\n)\n</code></pre>"},{"location":"api/prompts/#parse_edge_response","title":"parse_edge_response","text":""},{"location":"api/prompts/#causaliq_knowledge.llm.prompts.parse_edge_response","title":"parse_edge_response","text":"<pre><code>parse_edge_response(\n    json_data: Optional[dict], model: Optional[str] = None\n) -&gt; EdgeKnowledge\n</code></pre> <p>Parse a JSON response dict into an EdgeKnowledge object.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>EdgeKnowledge</code>           \u2013            <p>EdgeKnowledge object. Returns uncertain result if json_data is None</p> </li> <li> <code>EdgeKnowledge</code>           \u2013            <p>or missing required fields.</p> </li> </ul> Source code in <code>src\\causaliq_knowledge\\llm\\prompts.py</code> <pre><code>def parse_edge_response(\n    json_data: Optional[dict],\n    model: Optional[str] = None,\n) -&gt; EdgeKnowledge:\n    \"\"\"Parse a JSON response dict into an EdgeKnowledge object.\n\n    Args:\n        json_data: Parsed JSON dict from LLM response, or None if parsing\n            failed.\n        model: Optional model identifier to include in the result.\n\n    Returns:\n        EdgeKnowledge object. Returns uncertain result if json_data is None\n        or missing required fields.\n    \"\"\"\n    if json_data is None:\n        return EdgeKnowledge.uncertain(\n            reasoning=\"Failed to parse LLM response as JSON\",\n            model=model,\n        )\n\n    # Extract fields with defaults\n    exists = json_data.get(\"exists\")\n    direction_str = json_data.get(\"direction\")\n    confidence = json_data.get(\"confidence\", 0.0)\n    reasoning = json_data.get(\"reasoning\", \"\")\n\n    # Validate confidence is a number\n    try:\n        confidence = float(confidence)\n        confidence = max(0.0, min(1.0, confidence))\n    except (TypeError, ValueError):\n        confidence = 0.0\n\n    # Convert direction string to enum\n    direction = None\n    if direction_str:\n        try:\n            direction = EdgeDirection(direction_str.lower())\n        except ValueError:\n            # Invalid direction, leave as None\n            pass\n\n    return EdgeKnowledge(\n        exists=exists,\n        direction=direction,\n        confidence=confidence,\n        reasoning=str(reasoning),\n        model=model,\n    )\n</code></pre>"},{"location":"api/prompts/#causaliq_knowledge.llm.prompts.parse_edge_response(json_data)","title":"<code>json_data</code>","text":"(<code>Optional[dict]</code>)           \u2013            <p>Parsed JSON dict from LLM response, or None if parsing failed.</p>"},{"location":"api/prompts/#causaliq_knowledge.llm.prompts.parse_edge_response(model)","title":"<code>model</code>","text":"(<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional model identifier to include in the result.</p>"},{"location":"api/prompts/#usage-example_1","title":"Usage Example","text":"<pre><code>from causaliq_knowledge.llm import (\n    EdgeQueryPrompt,\n    GroqClient,\n    GroqConfig,\n    parse_edge_response,\n)\n\n# Create client and prompt\nconfig = GroqConfig(model=\"llama-3.1-8b-instant\")\nclient = GroqClient(config=config)\nprompt = EdgeQueryPrompt(\"X\", \"Y\", domain=\"statistics\")\nsystem, user = prompt.build()\n\n# Query the LLM\nmessages = [\n    {\"role\": \"system\", \"content\": system},\n    {\"role\": \"user\", \"content\": user},\n]\njson_data, response = client.complete_json(messages)\n\n# Parse the response into EdgeKnowledge\nknowledge = parse_edge_response(json_data, model=\"groq/llama-3.1-8b-instant\")\n\nprint(f\"Edge exists: {knowledge.exists}\")\nprint(f\"Direction: {knowledge.direction}\")\nprint(f\"Confidence: {knowledge.confidence}\")\nprint(f\"Reasoning: {knowledge.reasoning}\")\n</code></pre>"},{"location":"api/prompts/#prompt-templates","title":"Prompt Templates","text":"<p>The module exports several template constants that can be customized:</p>"},{"location":"api/prompts/#default_system_prompt","title":"DEFAULT_SYSTEM_PROMPT","text":"<p>The default system prompt instructs the LLM to act as a causal reasoning expert and respond with structured JSON.</p>"},{"location":"api/prompts/#user_prompt_template","title":"USER_PROMPT_TEMPLATE","text":"<p>Basic user prompt template for querying edge relationships without domain context.</p>"},{"location":"api/prompts/#user_prompt_with_domain_template","title":"USER_PROMPT_WITH_DOMAIN_TEMPLATE","text":"<p>User prompt template that includes domain context for more accurate responses.</p>"},{"location":"api/prompts/#variable_descriptions_template","title":"VARIABLE_DESCRIPTIONS_TEMPLATE","text":"<p>Template addition for including variable descriptions in the prompt.</p>"},{"location":"api/prompts/#custom-system-prompts","title":"Custom System Prompts","text":"<p>You can provide a custom system prompt to <code>EdgeQueryPrompt</code>:</p> <pre><code>custom_system = \"\"\"You are a biomedical expert.\nAssess causal relationships based on established medical literature.\nRespond with JSON: {\"exists\": bool, \"direction\": str, \"confidence\": float, \"reasoning\": str}\n\"\"\"\n\nprompt = EdgeQueryPrompt(\n    node_a=\"gene_X\",\n    node_b=\"protein_Y\",\n    domain=\"molecular_biology\",\n    system_prompt=custom_system,\n)\n</code></pre>"},{"location":"api/provider/","title":"LLM Knowledge Provider","text":"<p>The <code>LLMKnowledge</code> class is the main entry point for querying LLMs about causal relationships. It implements the <code>KnowledgeProvider</code> interface and supports multi-model consensus using vendor-specific API clients.</p>"},{"location":"api/provider/#architecture","title":"Architecture","text":"<p><code>LLMKnowledge</code> uses direct vendor-specific API clients rather than wrapper libraries like LiteLLM or LangChain. Currently supported providers:</p> <ul> <li>Groq: Fast inference for open-source models (free tier)</li> <li>Gemini: Google's Gemini models (generous free tier)</li> <li>OpenAI: GPT-4o and other OpenAI models</li> <li>Anthropic: Claude models</li> <li>DeepSeek: DeepSeek-V3 and DeepSeek-R1 models</li> <li>Mistral: Mistral AI models</li> <li>Ollama: Local LLMs (Llama, Mistral, Phi, etc.)</li> </ul>"},{"location":"api/provider/#usage","title":"Usage","text":"<pre><code>from causaliq_knowledge.llm import LLMKnowledge\n\n# Single model (default: Groq)\nprovider = LLMKnowledge()\n\n# Query about a potential edge\nresult = provider.query_edge(\"smoking\", \"lung_cancer\")\nprint(f\"Exists: {result.exists}\")\nprint(f\"Direction: {result.direction}\")\nprint(f\"Confidence: {result.confidence}\")\n\n# Multi-model consensus\nprovider = LLMKnowledge(\n    models=[\"groq/llama-3.1-8b-instant\", \"gemini/gemini-2.5-flash\"],\n    consensus_strategy=\"weighted_vote\",\n)\nresult = provider.query_edge(\n    \"exercise\",\n    \"heart_health\",\n    context={\"domain\": \"medicine\"},\n)\n</code></pre>"},{"location":"api/provider/#model-identifiers","title":"Model Identifiers","text":"<p>Models are specified with a provider prefix:</p> Provider Format Example Groq <code>groq/&lt;model&gt;</code> <code>groq/llama-3.1-8b-instant</code> Gemini <code>gemini/&lt;model&gt;</code> <code>gemini/gemini-2.5-flash</code> OpenAI <code>openai/&lt;model&gt;</code> <code>openai/gpt-4o-mini</code> Anthropic <code>anthropic/&lt;model&gt;</code> <code>anthropic/claude-sonnet-4-20250514</code> DeepSeek <code>deepseek/&lt;model&gt;</code> <code>deepseek/deepseek-chat</code> Mistral <code>mistral/&lt;model&gt;</code> <code>mistral/mistral-small-latest</code> Ollama <code>ollama/&lt;model&gt;</code> <code>ollama/llama3</code>"},{"location":"api/provider/#llmknowledge","title":"LLMKnowledge","text":""},{"location":"api/provider/#causaliq_knowledge.llm.provider.LLMKnowledge","title":"LLMKnowledge","text":"<pre><code>LLMKnowledge(\n    models: Optional[list[str]] = None,\n    consensus_strategy: str = \"weighted_vote\",\n    temperature: float = 0.1,\n    max_tokens: int = 500,\n    timeout: float = 30.0,\n    max_retries: int = 3,\n)\n</code></pre> <p>LLM-based knowledge provider using direct API clients.</p> <p>This provider queries one or more LLMs about causal relationships and combines their responses using a configurable consensus strategy. Uses direct API clients for reliability and control.</p> <p>Attributes:</p> <ul> <li> <code>models</code>               (<code>list[str]</code>)           \u2013            <p>List of model identifiers (e.g., \"groq/llama-3.1-8b-instant\").</p> </li> <li> <code>consensus_strategy</code>               (<code>str</code>)           \u2013            <p>Strategy for combining multi-model responses.</p> </li> <li> <code>clients</code>               (<code>str</code>)           \u2013            <p>Dict mapping model names to direct client instances.</p> </li> </ul> Example <p>provider = LLMKnowledge(models=[\"groq/llama-3.1-8b-instant\"]) result = provider.query_edge(\"smoking\", \"lung_cancer\") print(f\"Exists: {result.exists}, Confidence: {result.confidence}\")</p> <p>Parameters:</p> <ul> <li> </li> <li> </li> <li> </li> <li> </li> <li> </li> <li> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If consensus_strategy is not recognized or        unsupported model.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>query_edge</code>             \u2013              <p>Query LLMs about a potential causal edge.</p> </li> <li> <code>get_stats</code>             \u2013              <p>Get combined statistics from all clients.</p> </li> </ul>"},{"location":"api/provider/#causaliq_knowledge.llm.provider.LLMKnowledge--multi-model-consensus","title":"Multi-model consensus","text":"<p>provider = LLMKnowledge( ...     models=[ ...         \"groq/llama-3.1-8b-instant\", ...         \"gemini/gemini-2.5-flash\" ...     ], ...     consensus_strategy=\"weighted_vote\" ... )</p>"},{"location":"api/provider/#causaliq_knowledge.llm.provider.LLMKnowledge(models)","title":"<code>models</code>","text":"(<code>Optional[list[str]]</code>, default:                   <code>None</code> )           \u2013            <p>List of model identifiers. Supported formats: - \"groq/llama-3.1-8b-instant\" (Groq API) - \"gemini/gemini-2.5-flash\" (Google Gemini API) - \"ollama/llama3.2:1b\" (Local Ollama server) Defaults to [\"groq/llama-3.1-8b-instant\"].</p>"},{"location":"api/provider/#causaliq_knowledge.llm.provider.LLMKnowledge(consensus_strategy)","title":"<code>consensus_strategy</code>","text":"(<code>str</code>, default:                   <code>'weighted_vote'</code> )           \u2013            <p>How to combine multi-model responses. Options: \"weighted_vote\", \"highest_confidence\".</p>"},{"location":"api/provider/#causaliq_knowledge.llm.provider.LLMKnowledge(temperature)","title":"<code>temperature</code>","text":"(<code>float</code>, default:                   <code>0.1</code> )           \u2013            <p>LLM temperature (lower = more deterministic).</p>"},{"location":"api/provider/#causaliq_knowledge.llm.provider.LLMKnowledge(max_tokens)","title":"<code>max_tokens</code>","text":"(<code>int</code>, default:                   <code>500</code> )           \u2013            <p>Maximum tokens in LLM response.</p>"},{"location":"api/provider/#causaliq_knowledge.llm.provider.LLMKnowledge(timeout)","title":"<code>timeout</code>","text":"(<code>float</code>, default:                   <code>30.0</code> )           \u2013            <p>Request timeout in seconds.</p>"},{"location":"api/provider/#causaliq_knowledge.llm.provider.LLMKnowledge(max_retries)","title":"<code>max_retries</code>","text":"(<code>int</code>, default:                   <code>3</code> )           \u2013            <p>Number of retries on failure (unused for direct APIs).</p>"},{"location":"api/provider/#causaliq_knowledge.llm.provider.LLMKnowledge.name","title":"name  <code>property</code>","text":"<pre><code>name: str\n</code></pre> <p>Return provider name with model list.</p>"},{"location":"api/provider/#causaliq_knowledge.llm.provider.LLMKnowledge.models","title":"models  <code>property</code>","text":"<pre><code>models: list[str]\n</code></pre> <p>Return list of model identifiers.</p>"},{"location":"api/provider/#causaliq_knowledge.llm.provider.LLMKnowledge.consensus_strategy","title":"consensus_strategy  <code>property</code>","text":"<pre><code>consensus_strategy: str\n</code></pre> <p>Return the consensus strategy name.</p>"},{"location":"api/provider/#causaliq_knowledge.llm.provider.LLMKnowledge.query_edge","title":"query_edge","text":"<pre><code>query_edge(node_a: str, node_b: str, context: Optional[dict] = None) -&gt; EdgeKnowledge\n</code></pre> <p>Query LLMs about a potential causal edge.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>EdgeKnowledge</code>           \u2013            <p>EdgeKnowledge with combined result from all models.</p> </li> </ul>"},{"location":"api/provider/#causaliq_knowledge.llm.provider.LLMKnowledge.query_edge(node_a)","title":"<code>node_a</code>","text":"(<code>str</code>)           \u2013            <p>Name of the first variable.</p>"},{"location":"api/provider/#causaliq_knowledge.llm.provider.LLMKnowledge.query_edge(node_b)","title":"<code>node_b</code>","text":"(<code>str</code>)           \u2013            <p>Name of the second variable.</p>"},{"location":"api/provider/#causaliq_knowledge.llm.provider.LLMKnowledge.query_edge(context)","title":"<code>context</code>","text":"(<code>Optional[dict]</code>, default:                   <code>None</code> )           \u2013            <p>Optional context dict with keys: - domain: str - Domain context (e.g., \"medicine\") - descriptions: dict[str, str] - Variable descriptions - system_prompt: str - Custom system prompt</p>"},{"location":"api/provider/#causaliq_knowledge.llm.provider.LLMKnowledge.get_stats","title":"get_stats","text":"<pre><code>get_stats() -&gt; Dict[str, Any]\n</code></pre> <p>Get combined statistics from all clients.</p> <p>Returns:</p> <ul> <li> <code>Dict[str, Any]</code>           \u2013            <p>Dict with total_calls, total_cost, and per-model stats.</p> </li> </ul>"},{"location":"api/provider/#consensus-strategies","title":"Consensus Strategies","text":"<p>When using multiple models, responses are combined using a consensus strategy.</p>"},{"location":"api/provider/#weighted_vote","title":"weighted_vote","text":"<p>The default strategy. Combines responses by:</p> <ol> <li>Existence: Weighted vote by confidence (True, False, or None)</li> <li>Direction: Weighted majority among agreeing models</li> <li>Confidence: Average confidence of agreeing models</li> <li>Reasoning: Combined from all models</li> </ol>"},{"location":"api/provider/#causaliq_knowledge.llm.provider.weighted_vote","title":"weighted_vote","text":"<pre><code>weighted_vote(responses: list[EdgeKnowledge]) -&gt; EdgeKnowledge\n</code></pre> <p>Combine multiple responses using weighted voting.</p> <p>Strategy: - For existence: weighted vote by confidence - For direction: weighted majority among those agreeing on existence - Final confidence: average confidence of agreeing models - Reasoning: combine reasoning from all models</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>EdgeKnowledge</code>           \u2013            <p>Combined EdgeKnowledge result.</p> </li> </ul>"},{"location":"api/provider/#causaliq_knowledge.llm.provider.weighted_vote(responses)","title":"<code>responses</code>","text":"(<code>list[EdgeKnowledge]</code>)           \u2013            <p>List of EdgeKnowledge from different models.</p>"},{"location":"api/provider/#highest_confidence","title":"highest_confidence","text":"<p>Simply returns the response with the highest confidence score.</p>"},{"location":"api/provider/#causaliq_knowledge.llm.provider.highest_confidence","title":"highest_confidence","text":"<pre><code>highest_confidence(responses: list[EdgeKnowledge]) -&gt; EdgeKnowledge\n</code></pre> <p>Return the response with highest confidence.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>EdgeKnowledge</code>           \u2013            <p>EdgeKnowledge with highest confidence score.</p> </li> </ul>"},{"location":"api/provider/#causaliq_knowledge.llm.provider.highest_confidence(responses)","title":"<code>responses</code>","text":"(<code>list[EdgeKnowledge]</code>)           \u2013            <p>List of EdgeKnowledge from different models.</p>"},{"location":"api/provider/#example-multi-model-comparison","title":"Example: Multi-Model Comparison","text":"<pre><code>from causaliq_knowledge.llm import LLMKnowledge\n\n# Query multiple models (Groq + Gemini)\nprovider = LLMKnowledge(\n    models=[\"groq/llama-3.1-8b-instant\", \"gemini/gemini-2.5-flash\"],\n    consensus_strategy=\"weighted_vote\",\n    temperature=0.1,  # Low temperature for consistency\n)\n\n# Query with domain context\nresult = provider.query_edge(\n    node_a=\"interest_rate\",\n    node_b=\"inflation\",\n    context={\n        \"domain\": \"macroeconomics\",\n        \"descriptions\": {\n            \"interest_rate\": \"Central bank policy rate\",\n            \"inflation\": \"Year-over-year CPI change\",\n        },\n    },\n)\n\nprint(f\"Combined result: {result.exists} ({result.direction})\")\nprint(f\"Confidence: {result.confidence:.2f}\")\nprint(f\"Reasoning: {result.reasoning}\")\n\n# Check usage stats\nstats = provider.get_stats()\nprint(f\"Total cost: ${stats['total_cost']:.4f}\")\n</code></pre>"},{"location":"api/provider/#using-local-models-free","title":"Using Local Models (Free)","text":"<pre><code># Use Ollama for free local inference\n# First: install Ollama and run `ollama pull llama3`\nprovider = LLMKnowledge(models=[\"ollama/llama3\"])\n\n# Or mix local and cloud models\nprovider = LLMKnowledge(\n    models=[\"ollama/llama3\", \"gpt-4o-mini\"],\n    consensus_strategy=\"weighted_vote\",\n)\n</code></pre>"},{"location":"api/clients/anthropic/","title":"Anthropic Client API Reference","text":"<p>Direct Anthropic API client for Claude models. This client implements the BaseLLMClient interface using httpx to communicate directly with the Anthropic API.</p>"},{"location":"api/clients/anthropic/#overview","title":"Overview","text":"<p>The Anthropic client provides:</p> <ul> <li>Direct HTTP communication with Anthropic's API</li> <li>Implements the <code>BaseLLMClient</code> abstract interface</li> <li>JSON response parsing with error handling</li> <li>Call counting for usage tracking</li> <li>Configurable timeout and retry settings</li> <li>Proper handling of Anthropic's system prompt format</li> </ul>"},{"location":"api/clients/anthropic/#usage","title":"Usage","text":"<pre><code>from causaliq_knowledge.llm import AnthropicClient, AnthropicConfig\n\n# Create client with custom config\nconfig = AnthropicConfig(\n    model=\"claude-sonnet-4-20250514\",\n    temperature=0.1,\n    max_tokens=500,\n)\nclient = AnthropicClient(config=config)\n\n# Make a completion request\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"What is 2+2?\"},\n]\nresponse = client.completion(messages)\nprint(response.content)\n\n# Parse JSON response\njson_data = response.parse_json()\n</code></pre>"},{"location":"api/clients/anthropic/#environment-variables","title":"Environment Variables","text":"<p>The Anthropic client requires the <code>ANTHROPIC_API_KEY</code> environment variable to be set:</p> <pre><code>export ANTHROPIC_API_KEY=your_api_key_here\n</code></pre>"},{"location":"api/clients/anthropic/#anthropicconfig","title":"AnthropicConfig","text":""},{"location":"api/clients/anthropic/#causaliq_knowledge.llm.anthropic_client.AnthropicConfig","title":"AnthropicConfig  <code>dataclass</code>","text":"<pre><code>AnthropicConfig(\n    model: str = \"claude-sonnet-4-20250514\",\n    temperature: float = 0.1,\n    max_tokens: int = 500,\n    timeout: float = 30.0,\n    api_key: Optional[str] = None,\n)\n</code></pre> <p>Configuration for Anthropic API client.</p> <p>Extends LLMConfig with Anthropic-specific defaults.</p> <p>Attributes:</p> <ul> <li> <code>model</code>               (<code>str</code>)           \u2013            <p>Anthropic model identifier (default: claude-sonnet-4-20250514).</p> </li> <li> <code>temperature</code>               (<code>float</code>)           \u2013            <p>Sampling temperature (default: 0.1).</p> </li> <li> <code>max_tokens</code>               (<code>int</code>)           \u2013            <p>Maximum response tokens (default: 500).</p> </li> <li> <code>timeout</code>               (<code>float</code>)           \u2013            <p>Request timeout in seconds (default: 30.0).</p> </li> <li> <code>api_key</code>               (<code>Optional[str]</code>)           \u2013            <p>Anthropic API key (falls back to ANTHROPIC_API_KEY env var).</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>__post_init__</code>             \u2013              <p>Set API key from environment if not provided.</p> </li> </ul>"},{"location":"api/clients/anthropic/#causaliq_knowledge.llm.anthropic_client.AnthropicConfig.api_key","title":"api_key  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>api_key: Optional[str] = None\n</code></pre>"},{"location":"api/clients/anthropic/#causaliq_knowledge.llm.anthropic_client.AnthropicConfig.max_tokens","title":"max_tokens  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_tokens: int = 500\n</code></pre>"},{"location":"api/clients/anthropic/#causaliq_knowledge.llm.anthropic_client.AnthropicConfig.model","title":"model  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model: str = 'claude-sonnet-4-20250514'\n</code></pre>"},{"location":"api/clients/anthropic/#causaliq_knowledge.llm.anthropic_client.AnthropicConfig.temperature","title":"temperature  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>temperature: float = 0.1\n</code></pre>"},{"location":"api/clients/anthropic/#causaliq_knowledge.llm.anthropic_client.AnthropicConfig.timeout","title":"timeout  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>timeout: float = 30.0\n</code></pre>"},{"location":"api/clients/anthropic/#causaliq_knowledge.llm.anthropic_client.AnthropicConfig.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__() -&gt; None\n</code></pre> <p>Set API key from environment if not provided.</p>"},{"location":"api/clients/anthropic/#anthropicclient","title":"AnthropicClient","text":""},{"location":"api/clients/anthropic/#causaliq_knowledge.llm.anthropic_client.AnthropicClient","title":"AnthropicClient","text":"<pre><code>AnthropicClient(config: Optional[AnthropicConfig] = None)\n</code></pre> <p>Direct Anthropic API client.</p> <p>Implements the BaseLLMClient interface for Anthropic's Claude API. Uses httpx for HTTP requests.</p> Example <p>config = AnthropicConfig(model=\"claude-sonnet-4-20250514\") client = AnthropicClient(config) msgs = [{\"role\": \"user\", \"content\": \"Hello\"}] response = client.completion(msgs) print(response.content)</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>complete_json</code>             \u2013              <p>Make a completion request and parse response as JSON.</p> </li> <li> <code>completion</code>             \u2013              <p>Make a chat completion request to Anthropic.</p> </li> <li> <code>is_available</code>             \u2013              <p>Check if Anthropic API is available.</p> </li> <li> <code>list_models</code>             \u2013              <p>List available Claude models from Anthropic API.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>API_VERSION</code>           \u2013            </li> <li> <code>BASE_URL</code>           \u2013            </li> <li> <code>_total_calls</code>           \u2013            </li> <li> <code>call_count</code>               (<code>int</code>)           \u2013            <p>Return the number of API calls made.</p> </li> <li> <code>config</code>           \u2013            </li> <li> <code>model_name</code>               (<code>str</code>)           \u2013            <p>Return the model name being used.</p> </li> <li> <code>provider_name</code>               (<code>str</code>)           \u2013            <p>Return the provider name.</p> </li> </ul>"},{"location":"api/clients/anthropic/#causaliq_knowledge.llm.anthropic_client.AnthropicClient(config)","title":"<code>config</code>","text":"(<code>Optional[AnthropicConfig]</code>, default:                   <code>None</code> )           \u2013            <p>Anthropic configuration. If None, uses defaults with    API key from ANTHROPIC_API_KEY environment variable.</p>"},{"location":"api/clients/anthropic/#causaliq_knowledge.llm.anthropic_client.AnthropicClient.API_VERSION","title":"API_VERSION  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>API_VERSION = '2023-06-01'\n</code></pre>"},{"location":"api/clients/anthropic/#causaliq_knowledge.llm.anthropic_client.AnthropicClient.BASE_URL","title":"BASE_URL  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>BASE_URL = 'https://api.anthropic.com/v1'\n</code></pre>"},{"location":"api/clients/anthropic/#causaliq_knowledge.llm.anthropic_client.AnthropicClient._total_calls","title":"_total_calls  <code>instance-attribute</code>","text":"<pre><code>_total_calls = 0\n</code></pre>"},{"location":"api/clients/anthropic/#causaliq_knowledge.llm.anthropic_client.AnthropicClient.call_count","title":"call_count  <code>property</code>","text":"<pre><code>call_count: int\n</code></pre> <p>Return the number of API calls made.</p>"},{"location":"api/clients/anthropic/#causaliq_knowledge.llm.anthropic_client.AnthropicClient.config","title":"config  <code>instance-attribute</code>","text":"<pre><code>config = config or AnthropicConfig()\n</code></pre>"},{"location":"api/clients/anthropic/#causaliq_knowledge.llm.anthropic_client.AnthropicClient.model_name","title":"model_name  <code>property</code>","text":"<pre><code>model_name: str\n</code></pre> <p>Return the model name being used.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>Model identifier string.</p> </li> </ul>"},{"location":"api/clients/anthropic/#causaliq_knowledge.llm.anthropic_client.AnthropicClient.provider_name","title":"provider_name  <code>property</code>","text":"<pre><code>provider_name: str\n</code></pre> <p>Return the provider name.</p>"},{"location":"api/clients/anthropic/#causaliq_knowledge.llm.anthropic_client.AnthropicClient.complete_json","title":"complete_json","text":"<pre><code>complete_json(\n    messages: List[Dict[str, str]], **kwargs: Any\n) -&gt; tuple[Optional[Dict[str, Any]], LLMResponse]\n</code></pre> <p>Make a completion request and parse response as JSON.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>tuple[Optional[Dict[str, Any]], LLMResponse]</code>           \u2013            <p>Tuple of (parsed JSON dict or None, raw LLMResponse).</p> </li> </ul>"},{"location":"api/clients/anthropic/#causaliq_knowledge.llm.anthropic_client.AnthropicClient.complete_json(messages)","title":"<code>messages</code>","text":"(<code>List[Dict[str, str]]</code>)           \u2013            <p>List of message dicts with \"role\" and \"content\" keys.</p>"},{"location":"api/clients/anthropic/#causaliq_knowledge.llm.anthropic_client.AnthropicClient.complete_json(**kwargs)","title":"<code>**kwargs</code>","text":"(<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Override config options passed to completion().</p>"},{"location":"api/clients/anthropic/#causaliq_knowledge.llm.anthropic_client.AnthropicClient.completion","title":"completion","text":"<pre><code>completion(messages: List[Dict[str, str]], **kwargs: Any) -&gt; LLMResponse\n</code></pre> <p>Make a chat completion request to Anthropic.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>LLMResponse</code>           \u2013            <p>LLMResponse with the generated content and metadata.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the API request fails.</p> </li> </ul>"},{"location":"api/clients/anthropic/#causaliq_knowledge.llm.anthropic_client.AnthropicClient.completion(messages)","title":"<code>messages</code>","text":"(<code>List[Dict[str, str]]</code>)           \u2013            <p>List of message dicts with \"role\" and \"content\" keys.</p>"},{"location":"api/clients/anthropic/#causaliq_knowledge.llm.anthropic_client.AnthropicClient.completion(**kwargs)","title":"<code>**kwargs</code>","text":"(<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Override config options (temperature, max_tokens).</p>"},{"location":"api/clients/anthropic/#causaliq_knowledge.llm.anthropic_client.AnthropicClient.is_available","title":"is_available","text":"<pre><code>is_available() -&gt; bool\n</code></pre> <p>Check if Anthropic API is available.</p> <p>Returns:</p> <ul> <li> <code>bool</code>           \u2013            <p>True if ANTHROPIC_API_KEY is configured.</p> </li> </ul>"},{"location":"api/clients/anthropic/#causaliq_knowledge.llm.anthropic_client.AnthropicClient.list_models","title":"list_models","text":"<pre><code>list_models() -&gt; List[str]\n</code></pre> <p>List available Claude models from Anthropic API.</p> <p>Queries the Anthropic /v1/models endpoint to get available models.</p> <p>Returns:</p> <ul> <li> <code>List[str]</code>           \u2013            <p>List of model identifiers</p> </li> <li> <code>List[str]</code>           \u2013            <p>(e.g., ['claude-sonnet-4-20250514', ...]).</p> </li> </ul>"},{"location":"api/clients/anthropic/#supported-models","title":"Supported Models","text":"<p>Anthropic provides the Claude family of models:</p> Model Description Free Tier <code>claude-sonnet-4-20250514</code> Claude Sonnet 4 - balanced performance \u274c No <code>claude-opus-4-20250514</code> Claude Opus 4 - highest capability \u274c No <code>claude-3-5-haiku-latest</code> Claude 3.5 Haiku - fast and efficient \u274c No <p>See Anthropic documentation for the full list of available models.</p>"},{"location":"api/clients/deepseek/","title":"DeepSeek Client","text":"<p>Direct DeepSeek API client for DeepSeek-V3 and DeepSeek-R1 models.</p>"},{"location":"api/clients/deepseek/#overview","title":"Overview","text":"<p>DeepSeek is a Chinese AI company known for highly capable models at competitive prices. Their API is OpenAI-compatible, making integration straightforward.</p> <p>Key features:</p> <ul> <li>DeepSeek-V3: General purpose chat model, excellent performance</li> <li>DeepSeek-R1: Advanced reasoning model, rivals OpenAI o1 at much lower cost</li> <li>Very competitive pricing (~$0.14/1M input for chat)</li> <li>OpenAI-compatible API</li> </ul>"},{"location":"api/clients/deepseek/#configuration","title":"Configuration","text":"<p>The client requires a <code>DEEPSEEK_API_KEY</code> environment variable:</p> <pre><code># Linux/macOS\nexport DEEPSEEK_API_KEY=\"your-api-key\"\n\n# Windows PowerShell\n$env:DEEPSEEK_API_KEY=\"your-api-key\"\n\n# Windows cmd\nset DEEPSEEK_API_KEY=your-api-key\n</code></pre> <p>Get your API key from: https://platform.deepseek.com</p>"},{"location":"api/clients/deepseek/#usage","title":"Usage","text":""},{"location":"api/clients/deepseek/#basic-usage","title":"Basic Usage","text":"<pre><code>from causaliq_knowledge.llm import DeepSeekClient, DeepSeekConfig\n\n# Default config (uses DEEPSEEK_API_KEY env var)\nclient = DeepSeekClient()\n\n# Or with custom config\nconfig = DeepSeekConfig(\n    model=\"deepseek-chat\",\n    temperature=0.1,\n    max_tokens=500,\n    timeout=30.0,\n)\nclient = DeepSeekClient(config)\n\n# Make a completion request\nmessages = [{\"role\": \"user\", \"content\": \"What is 2 + 2?\"}]\nresponse = client.completion(messages)\nprint(response.content)\n</code></pre>"},{"location":"api/clients/deepseek/#using-with-cli","title":"Using with CLI","text":"<pre><code># Query with DeepSeek\ncqknow query smoking lung_cancer --model deepseek/deepseek-chat\n\n# Use reasoning model for complex queries\ncqknow query income education --model deepseek/deepseek-reasoner --domain economics\n\n# List available DeepSeek models\ncqknow models deepseek\n</code></pre>"},{"location":"api/clients/deepseek/#using-with-llmknowledge-provider","title":"Using with LLMKnowledge Provider","text":"<pre><code>from causaliq_knowledge.llm import LLMKnowledge\n\n# Single model\nprovider = LLMKnowledge(models=[\"deepseek/deepseek-chat\"])\nresult = provider.query_edge(\"smoking\", \"lung_cancer\")\n\n# Multi-model consensus\nprovider = LLMKnowledge(\n    models=[\n        \"deepseek/deepseek-chat\",\n        \"groq/llama-3.1-8b-instant\",\n    ],\n    consensus_strategy=\"weighted_vote\",\n)\n</code></pre>"},{"location":"api/clients/deepseek/#available-models","title":"Available Models","text":"Model Description Best For <code>deepseek-chat</code> DeepSeek-V3 general purpose Fast, general queries <code>deepseek-reasoner</code> DeepSeek-R1 reasoning model Complex reasoning tasks"},{"location":"api/clients/deepseek/#pricing","title":"Pricing","text":"<p>DeepSeek offers very competitive pricing (as of Jan 2025):</p> Model Input (per 1M tokens) Output (per 1M tokens) deepseek-chat $0.14 $0.28 deepseek-reasoner $0.55 $2.19 <p>Note: Cache hits are even cheaper. See DeepSeek pricing for details.</p>"},{"location":"api/clients/deepseek/#api-reference","title":"API Reference","text":""},{"location":"api/clients/deepseek/#causaliq_knowledge.llm.deepseek_client.DeepSeekConfig","title":"DeepSeekConfig  <code>dataclass</code>","text":"<pre><code>DeepSeekConfig(\n    model: str = \"deepseek-chat\",\n    temperature: float = 0.1,\n    max_tokens: int = 500,\n    timeout: float = 30.0,\n    api_key: Optional[str] = None,\n)\n</code></pre> <p>Configuration for DeepSeek API client.</p> <p>Extends OpenAICompatConfig with DeepSeek-specific defaults.</p> <p>Attributes:</p> <ul> <li> <code>model</code>               (<code>str</code>)           \u2013            <p>DeepSeek model identifier (default: deepseek-chat).</p> </li> <li> <code>temperature</code>               (<code>float</code>)           \u2013            <p>Sampling temperature (default: 0.1).</p> </li> <li> <code>max_tokens</code>               (<code>int</code>)           \u2013            <p>Maximum response tokens (default: 500).</p> </li> <li> <code>timeout</code>               (<code>float</code>)           \u2013            <p>Request timeout in seconds (default: 30.0).</p> </li> <li> <code>api_key</code>               (<code>Optional[str]</code>)           \u2013            <p>DeepSeek API key (falls back to DEEPSEEK_API_KEY env var).</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>__post_init__</code>             \u2013              <p>Set API key from environment if not provided.</p> </li> </ul>"},{"location":"api/clients/deepseek/#causaliq_knowledge.llm.deepseek_client.DeepSeekConfig.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__() -&gt; None\n</code></pre> <p>Set API key from environment if not provided.</p>"},{"location":"api/clients/deepseek/#causaliq_knowledge.llm.deepseek_client.DeepSeekClient","title":"DeepSeekClient","text":"<pre><code>DeepSeekClient(config: Optional[DeepSeekConfig] = None)\n</code></pre> <p>Direct DeepSeek API client.</p> <p>DeepSeek uses an OpenAI-compatible API, making integration straightforward. Known for excellent reasoning capabilities (R1) at low cost.</p> Available models <ul> <li>deepseek-chat: General purpose (DeepSeek-V3)</li> <li>deepseek-reasoner: Advanced reasoning (DeepSeek-R1)</li> </ul> Example <p>config = DeepSeekConfig(model=\"deepseek-chat\") client = DeepSeekClient(config) msgs = [{\"role\": \"user\", \"content\": \"Hello\"}] response = client.completion(msgs) print(response.content)</p> <p>Parameters:</p> <ul> <li> </li> </ul> <p>Methods:</p> <ul> <li> <code>complete_json</code>             \u2013              <p>Make a completion request and parse response as JSON.</p> </li> <li> <code>completion</code>             \u2013              <p>Make a chat completion request.</p> </li> <li> <code>is_available</code>             \u2013              <p>Check if the API is available.</p> </li> <li> <code>list_models</code>             \u2013              <p>List available models from the API.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>call_count</code>               (<code>int</code>)           \u2013            <p>Return the number of API calls made.</p> </li> <li> <code>model_name</code>               (<code>str</code>)           \u2013            <p>Return the model name being used.</p> </li> <li> <code>provider_name</code>               (<code>str</code>)           \u2013            <p>Return the provider name.</p> </li> </ul>"},{"location":"api/clients/deepseek/#causaliq_knowledge.llm.deepseek_client.DeepSeekClient(config)","title":"<code>config</code>","text":"(<code>Optional[DeepSeekConfig]</code>, default:                   <code>None</code> )           \u2013            <p>DeepSeek configuration. If None, uses defaults with    API key from DEEPSEEK_API_KEY environment variable.</p>"},{"location":"api/clients/deepseek/#causaliq_knowledge.llm.deepseek_client.DeepSeekClient.call_count","title":"call_count  <code>property</code>","text":"<pre><code>call_count: int\n</code></pre> <p>Return the number of API calls made.</p>"},{"location":"api/clients/deepseek/#causaliq_knowledge.llm.deepseek_client.DeepSeekClient.model_name","title":"model_name  <code>property</code>","text":"<pre><code>model_name: str\n</code></pre> <p>Return the model name being used.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>Model identifier string.</p> </li> </ul>"},{"location":"api/clients/deepseek/#causaliq_knowledge.llm.deepseek_client.DeepSeekClient.provider_name","title":"provider_name  <code>property</code>","text":"<pre><code>provider_name: str\n</code></pre> <p>Return the provider name.</p>"},{"location":"api/clients/deepseek/#causaliq_knowledge.llm.deepseek_client.DeepSeekClient.complete_json","title":"complete_json","text":"<pre><code>complete_json(\n    messages: List[Dict[str, str]], **kwargs: Any\n) -&gt; tuple[Optional[Dict[str, Any]], LLMResponse]\n</code></pre> <p>Make a completion request and parse response as JSON.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>tuple[Optional[Dict[str, Any]], LLMResponse]</code>           \u2013            <p>Tuple of (parsed JSON dict or None, raw LLMResponse).</p> </li> </ul>"},{"location":"api/clients/deepseek/#causaliq_knowledge.llm.deepseek_client.DeepSeekClient.complete_json(messages)","title":"<code>messages</code>","text":"(<code>List[Dict[str, str]]</code>)           \u2013            <p>List of message dicts with \"role\" and \"content\" keys.</p>"},{"location":"api/clients/deepseek/#causaliq_knowledge.llm.deepseek_client.DeepSeekClient.complete_json(**kwargs)","title":"<code>**kwargs</code>","text":"(<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Override config options passed to completion().</p>"},{"location":"api/clients/deepseek/#causaliq_knowledge.llm.deepseek_client.DeepSeekClient.completion","title":"completion","text":"<pre><code>completion(messages: List[Dict[str, str]], **kwargs: Any) -&gt; LLMResponse\n</code></pre> <p>Make a chat completion request.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>LLMResponse</code>           \u2013            <p>LLMResponse with the generated content and metadata.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the API request fails.</p> </li> </ul>"},{"location":"api/clients/deepseek/#causaliq_knowledge.llm.deepseek_client.DeepSeekClient.completion(messages)","title":"<code>messages</code>","text":"(<code>List[Dict[str, str]]</code>)           \u2013            <p>List of message dicts with \"role\" and \"content\" keys.</p>"},{"location":"api/clients/deepseek/#causaliq_knowledge.llm.deepseek_client.DeepSeekClient.completion(**kwargs)","title":"<code>**kwargs</code>","text":"(<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Override config options (temperature, max_tokens).</p>"},{"location":"api/clients/deepseek/#causaliq_knowledge.llm.deepseek_client.DeepSeekClient.is_available","title":"is_available","text":"<pre><code>is_available() -&gt; bool\n</code></pre> <p>Check if the API is available.</p> <p>Returns:</p> <ul> <li> <code>bool</code>           \u2013            <p>True if API key is configured.</p> </li> </ul>"},{"location":"api/clients/deepseek/#causaliq_knowledge.llm.deepseek_client.DeepSeekClient.list_models","title":"list_models","text":"<pre><code>list_models() -&gt; List[str]\n</code></pre> <p>List available models from the API.</p> <p>Queries the API to get models accessible with the current API key, then filters using _filter_models().</p> <p>Returns:</p> <ul> <li> <code>List[str]</code>           \u2013            <p>List of model identifiers.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the API request fails.</p> </li> </ul>"},{"location":"api/clients/gemini/","title":"Gemini Client API Reference","text":"<p>Direct Google Gemini API client. This client implements the BaseLLMClient interface using httpx to communicate directly with Google's Generative Language API.</p>"},{"location":"api/clients/gemini/#overview","title":"Overview","text":"<p>The Gemini client provides:</p> <ul> <li>Direct HTTP communication with Google's Generative Language API</li> <li>Implements the <code>BaseLLMClient</code> abstract interface</li> <li>Automatic conversion from OpenAI-style messages to Gemini format</li> <li>JSON response parsing with error handling</li> <li>Call counting for usage tracking</li> <li>Configurable timeout settings</li> </ul>"},{"location":"api/clients/gemini/#usage","title":"Usage","text":"<pre><code>from causaliq_knowledge.llm import GeminiClient, GeminiConfig\n\n# Create client with custom config\nconfig = GeminiConfig(\n    model=\"gemini-2.5-flash\",\n    temperature=0.1,\n    max_tokens=500,\n)\nclient = GeminiClient(config=config)\n\n# Make a completion request (OpenAI-style messages)\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"What is 2+2?\"},\n]\nresponse = client.completion(messages)\nprint(response.content)\n\n# Parse JSON response\njson_data = response.parse_json()\n</code></pre>"},{"location":"api/clients/gemini/#environment-variables","title":"Environment Variables","text":"<p>The Gemini client requires the <code>GEMINI_API_KEY</code> environment variable to be set:</p> <pre><code>export GEMINI_API_KEY=your_api_key_here\n</code></pre>"},{"location":"api/clients/gemini/#geminiconfig","title":"GeminiConfig","text":""},{"location":"api/clients/gemini/#causaliq_knowledge.llm.gemini_client.GeminiConfig","title":"GeminiConfig  <code>dataclass</code>","text":"<pre><code>GeminiConfig(\n    model: str = \"gemini-2.5-flash\",\n    temperature: float = 0.1,\n    max_tokens: int = 500,\n    timeout: float = 30.0,\n    api_key: Optional[str] = None,\n)\n</code></pre> <p>Configuration for Gemini API client.</p> <p>Extends LLMConfig with Gemini-specific defaults.</p> <p>Attributes:</p> <ul> <li> <code>model</code>               (<code>str</code>)           \u2013            <p>Gemini model identifier (default: gemini-2.5-flash).</p> </li> <li> <code>temperature</code>               (<code>float</code>)           \u2013            <p>Sampling temperature (default: 0.1).</p> </li> <li> <code>max_tokens</code>               (<code>int</code>)           \u2013            <p>Maximum response tokens (default: 500).</p> </li> <li> <code>timeout</code>               (<code>float</code>)           \u2013            <p>Request timeout in seconds (default: 30.0).</p> </li> <li> <code>api_key</code>               (<code>Optional[str]</code>)           \u2013            <p>Gemini API key (falls back to GEMINI_API_KEY env var).</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>__post_init__</code>             \u2013              <p>Set API key from environment if not provided.</p> </li> </ul>"},{"location":"api/clients/gemini/#causaliq_knowledge.llm.gemini_client.GeminiConfig.api_key","title":"api_key  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>api_key: Optional[str] = None\n</code></pre>"},{"location":"api/clients/gemini/#causaliq_knowledge.llm.gemini_client.GeminiConfig.max_tokens","title":"max_tokens  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_tokens: int = 500\n</code></pre>"},{"location":"api/clients/gemini/#causaliq_knowledge.llm.gemini_client.GeminiConfig.model","title":"model  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model: str = 'gemini-2.5-flash'\n</code></pre>"},{"location":"api/clients/gemini/#causaliq_knowledge.llm.gemini_client.GeminiConfig.temperature","title":"temperature  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>temperature: float = 0.1\n</code></pre>"},{"location":"api/clients/gemini/#causaliq_knowledge.llm.gemini_client.GeminiConfig.timeout","title":"timeout  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>timeout: float = 30.0\n</code></pre>"},{"location":"api/clients/gemini/#causaliq_knowledge.llm.gemini_client.GeminiConfig.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__() -&gt; None\n</code></pre> <p>Set API key from environment if not provided.</p>"},{"location":"api/clients/gemini/#geminiclient","title":"GeminiClient","text":""},{"location":"api/clients/gemini/#causaliq_knowledge.llm.gemini_client.GeminiClient","title":"GeminiClient","text":"<pre><code>GeminiClient(config: Optional[GeminiConfig] = None)\n</code></pre> <p>Direct Gemini API client.</p> <p>Implements the BaseLLMClient interface for Google's Gemini API. Uses httpx for HTTP requests.</p> Example <p>config = GeminiConfig(model=\"gemini-2.5-flash\") client = GeminiClient(config) msgs = [{\"role\": \"user\", \"content\": \"Hello\"}] response = client.completion(msgs) print(response.content)</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>complete_json</code>             \u2013              <p>Make a completion request and parse response as JSON.</p> </li> <li> <code>completion</code>             \u2013              <p>Make a chat completion request to Gemini.</p> </li> <li> <code>is_available</code>             \u2013              <p>Check if Gemini API is available.</p> </li> <li> <code>list_models</code>             \u2013              <p>List available models from Gemini API.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>BASE_URL</code>           \u2013            </li> <li> <code>_total_calls</code>           \u2013            </li> <li> <code>call_count</code>               (<code>int</code>)           \u2013            <p>Return the number of API calls made.</p> </li> <li> <code>config</code>           \u2013            </li> <li> <code>model_name</code>               (<code>str</code>)           \u2013            <p>Return the model name being used.</p> </li> <li> <code>provider_name</code>               (<code>str</code>)           \u2013            <p>Return the provider name.</p> </li> </ul>"},{"location":"api/clients/gemini/#causaliq_knowledge.llm.gemini_client.GeminiClient(config)","title":"<code>config</code>","text":"(<code>Optional[GeminiConfig]</code>, default:                   <code>None</code> )           \u2013            <p>Gemini configuration. If None, uses defaults with    API key from GEMINI_API_KEY environment variable.</p>"},{"location":"api/clients/gemini/#causaliq_knowledge.llm.gemini_client.GeminiClient.BASE_URL","title":"BASE_URL  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>BASE_URL = 'https://generativelanguage.googleapis.com/v1beta/models'\n</code></pre>"},{"location":"api/clients/gemini/#causaliq_knowledge.llm.gemini_client.GeminiClient._total_calls","title":"_total_calls  <code>instance-attribute</code>","text":"<pre><code>_total_calls = 0\n</code></pre>"},{"location":"api/clients/gemini/#causaliq_knowledge.llm.gemini_client.GeminiClient.call_count","title":"call_count  <code>property</code>","text":"<pre><code>call_count: int\n</code></pre> <p>Return the number of API calls made.</p>"},{"location":"api/clients/gemini/#causaliq_knowledge.llm.gemini_client.GeminiClient.config","title":"config  <code>instance-attribute</code>","text":"<pre><code>config = config or GeminiConfig()\n</code></pre>"},{"location":"api/clients/gemini/#causaliq_knowledge.llm.gemini_client.GeminiClient.model_name","title":"model_name  <code>property</code>","text":"<pre><code>model_name: str\n</code></pre> <p>Return the model name being used.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>Model identifier string.</p> </li> </ul>"},{"location":"api/clients/gemini/#causaliq_knowledge.llm.gemini_client.GeminiClient.provider_name","title":"provider_name  <code>property</code>","text":"<pre><code>provider_name: str\n</code></pre> <p>Return the provider name.</p>"},{"location":"api/clients/gemini/#causaliq_knowledge.llm.gemini_client.GeminiClient.complete_json","title":"complete_json","text":"<pre><code>complete_json(\n    messages: List[Dict[str, str]], **kwargs: Any\n) -&gt; tuple[Optional[Dict[str, Any]], LLMResponse]\n</code></pre> <p>Make a completion request and parse response as JSON.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>tuple[Optional[Dict[str, Any]], LLMResponse]</code>           \u2013            <p>Tuple of (parsed JSON dict or None, raw LLMResponse).</p> </li> </ul>"},{"location":"api/clients/gemini/#causaliq_knowledge.llm.gemini_client.GeminiClient.complete_json(messages)","title":"<code>messages</code>","text":"(<code>List[Dict[str, str]]</code>)           \u2013            <p>List of message dicts with \"role\" and \"content\" keys.</p>"},{"location":"api/clients/gemini/#causaliq_knowledge.llm.gemini_client.GeminiClient.complete_json(**kwargs)","title":"<code>**kwargs</code>","text":"(<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Override config options passed to completion().</p>"},{"location":"api/clients/gemini/#causaliq_knowledge.llm.gemini_client.GeminiClient.completion","title":"completion","text":"<pre><code>completion(messages: List[Dict[str, str]], **kwargs: Any) -&gt; LLMResponse\n</code></pre> <p>Make a chat completion request to Gemini.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>LLMResponse</code>           \u2013            <p>LLMResponse with the generated content and metadata.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the API request fails.</p> </li> </ul>"},{"location":"api/clients/gemini/#causaliq_knowledge.llm.gemini_client.GeminiClient.completion(messages)","title":"<code>messages</code>","text":"(<code>List[Dict[str, str]]</code>)           \u2013            <p>List of message dicts with \"role\" and \"content\" keys.</p>"},{"location":"api/clients/gemini/#causaliq_knowledge.llm.gemini_client.GeminiClient.completion(**kwargs)","title":"<code>**kwargs</code>","text":"(<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Override config options (temperature, max_tokens).</p>"},{"location":"api/clients/gemini/#causaliq_knowledge.llm.gemini_client.GeminiClient.is_available","title":"is_available","text":"<pre><code>is_available() -&gt; bool\n</code></pre> <p>Check if Gemini API is available.</p> <p>Returns:</p> <ul> <li> <code>bool</code>           \u2013            <p>True if GEMINI_API_KEY is configured.</p> </li> </ul>"},{"location":"api/clients/gemini/#causaliq_knowledge.llm.gemini_client.GeminiClient.list_models","title":"list_models","text":"<pre><code>list_models() -&gt; List[str]\n</code></pre> <p>List available models from Gemini API.</p> <p>Queries the Gemini API to get models accessible with the current API key. Filters to only include models that support generateContent.</p> <p>Returns:</p> <ul> <li> <code>List[str]</code>           \u2013            <p>List of model identifiers (e.g., ['gemini-2.5-flash', ...]).</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the API request fails.</p> </li> </ul>"},{"location":"api/clients/gemini/#message-format-conversion","title":"Message Format Conversion","text":"<p>The client automatically converts OpenAI-style messages to Gemini's format:</p> OpenAI Role Gemini Role <code>system</code> System instruction (separate field) <code>user</code> <code>user</code> <code>assistant</code> <code>model</code>"},{"location":"api/clients/gemini/#supported-models","title":"Supported Models","text":"<p>Google Gemini provides a generous free tier:</p> Model Description Free Tier <code>gemini-2.5-flash</code> Fast and efficient \u2705 Yes <code>gemini-2.5-pro</code> Most capable \u2705 Limited <code>gemini-1.5-flash</code> Previous generation \u2705 Yes <p>See Google AI documentation for the full list of available models.</p>"},{"location":"api/clients/groq/","title":"Groq Client API Reference","text":"<p>Direct Groq API client for fast LLM inference. This client implements the BaseLLMClient interface using httpx to communicate directly with the Groq API.</p>"},{"location":"api/clients/groq/#overview","title":"Overview","text":"<p>The Groq client provides:</p> <ul> <li>Direct HTTP communication with Groq's API</li> <li>Implements the <code>BaseLLMClient</code> abstract interface</li> <li>JSON response parsing with error handling</li> <li>Call counting for usage tracking</li> <li>Configurable timeout and retry settings</li> </ul>"},{"location":"api/clients/groq/#usage","title":"Usage","text":"<pre><code>from causaliq_knowledge.llm import GroqClient, GroqConfig\n\n# Create client with custom config\nconfig = GroqConfig(\n    model=\"llama-3.1-8b-instant\",\n    temperature=0.1,\n    max_tokens=500,\n)\nclient = GroqClient(config=config)\n\n# Make a completion request\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"What is 2+2?\"},\n]\nresponse = client.completion(messages)\nprint(response.content)\n\n# Parse JSON response\njson_data = response.parse_json()\n</code></pre>"},{"location":"api/clients/groq/#environment-variables","title":"Environment Variables","text":"<p>The Groq client requires the <code>GROQ_API_KEY</code> environment variable to be set:</p> <pre><code>export GROQ_API_KEY=your_api_key_here\n</code></pre>"},{"location":"api/clients/groq/#groqconfig","title":"GroqConfig","text":""},{"location":"api/clients/groq/#causaliq_knowledge.llm.groq_client.GroqConfig","title":"GroqConfig  <code>dataclass</code>","text":"<pre><code>GroqConfig(\n    model: str = \"llama-3.1-8b-instant\",\n    temperature: float = 0.1,\n    max_tokens: int = 500,\n    timeout: float = 30.0,\n    api_key: Optional[str] = None,\n)\n</code></pre> <p>Configuration for Groq API client.</p> <p>Extends LLMConfig with Groq-specific defaults.</p> <p>Attributes:</p> <ul> <li> <code>model</code>               (<code>str</code>)           \u2013            <p>Groq model identifier (default: llama-3.1-8b-instant).</p> </li> <li> <code>temperature</code>               (<code>float</code>)           \u2013            <p>Sampling temperature (default: 0.1).</p> </li> <li> <code>max_tokens</code>               (<code>int</code>)           \u2013            <p>Maximum response tokens (default: 500).</p> </li> <li> <code>timeout</code>               (<code>float</code>)           \u2013            <p>Request timeout in seconds (default: 30.0).</p> </li> <li> <code>api_key</code>               (<code>Optional[str]</code>)           \u2013            <p>Groq API key (falls back to GROQ_API_KEY env var).</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>__post_init__</code>             \u2013              <p>Set API key from environment if not provided.</p> </li> </ul>"},{"location":"api/clients/groq/#causaliq_knowledge.llm.groq_client.GroqConfig.api_key","title":"api_key  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>api_key: Optional[str] = None\n</code></pre>"},{"location":"api/clients/groq/#causaliq_knowledge.llm.groq_client.GroqConfig.max_tokens","title":"max_tokens  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_tokens: int = 500\n</code></pre>"},{"location":"api/clients/groq/#causaliq_knowledge.llm.groq_client.GroqConfig.model","title":"model  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model: str = 'llama-3.1-8b-instant'\n</code></pre>"},{"location":"api/clients/groq/#causaliq_knowledge.llm.groq_client.GroqConfig.temperature","title":"temperature  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>temperature: float = 0.1\n</code></pre>"},{"location":"api/clients/groq/#causaliq_knowledge.llm.groq_client.GroqConfig.timeout","title":"timeout  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>timeout: float = 30.0\n</code></pre>"},{"location":"api/clients/groq/#causaliq_knowledge.llm.groq_client.GroqConfig.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__() -&gt; None\n</code></pre> <p>Set API key from environment if not provided.</p>"},{"location":"api/clients/groq/#groqclient","title":"GroqClient","text":""},{"location":"api/clients/groq/#causaliq_knowledge.llm.groq_client.GroqClient","title":"GroqClient","text":"<pre><code>GroqClient(config: Optional[GroqConfig] = None)\n</code></pre> <p>Direct Groq API client.</p> <p>Implements the BaseLLMClient interface for Groq's API. Uses httpx for HTTP requests.</p> Example <p>config = GroqConfig(model=\"llama-3.1-8b-instant\") client = GroqClient(config) msgs = [{\"role\": \"user\", \"content\": \"Hello\"}] response = client.completion(msgs) print(response.content)</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>complete_json</code>             \u2013              <p>Make a completion request and parse response as JSON.</p> </li> <li> <code>completion</code>             \u2013              <p>Make a chat completion request to Groq.</p> </li> <li> <code>is_available</code>             \u2013              <p>Check if Groq API is available.</p> </li> <li> <code>list_models</code>             \u2013              <p>List available models from Groq API.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>BASE_URL</code>           \u2013            </li> <li> <code>_total_calls</code>           \u2013            </li> <li> <code>call_count</code>               (<code>int</code>)           \u2013            <p>Return the number of API calls made.</p> </li> <li> <code>config</code>           \u2013            </li> <li> <code>model_name</code>               (<code>str</code>)           \u2013            <p>Return the model name being used.</p> </li> <li> <code>provider_name</code>               (<code>str</code>)           \u2013            <p>Return the provider name.</p> </li> </ul>"},{"location":"api/clients/groq/#causaliq_knowledge.llm.groq_client.GroqClient(config)","title":"<code>config</code>","text":"(<code>Optional[GroqConfig]</code>, default:                   <code>None</code> )           \u2013            <p>Groq configuration. If None, uses defaults with    API key from GROQ_API_KEY environment variable.</p>"},{"location":"api/clients/groq/#causaliq_knowledge.llm.groq_client.GroqClient.BASE_URL","title":"BASE_URL  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>BASE_URL = 'https://api.groq.com/openai/v1'\n</code></pre>"},{"location":"api/clients/groq/#causaliq_knowledge.llm.groq_client.GroqClient._total_calls","title":"_total_calls  <code>instance-attribute</code>","text":"<pre><code>_total_calls = 0\n</code></pre>"},{"location":"api/clients/groq/#causaliq_knowledge.llm.groq_client.GroqClient.call_count","title":"call_count  <code>property</code>","text":"<pre><code>call_count: int\n</code></pre> <p>Return the number of API calls made.</p>"},{"location":"api/clients/groq/#causaliq_knowledge.llm.groq_client.GroqClient.config","title":"config  <code>instance-attribute</code>","text":"<pre><code>config = config or GroqConfig()\n</code></pre>"},{"location":"api/clients/groq/#causaliq_knowledge.llm.groq_client.GroqClient.model_name","title":"model_name  <code>property</code>","text":"<pre><code>model_name: str\n</code></pre> <p>Return the model name being used.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>Model identifier string.</p> </li> </ul>"},{"location":"api/clients/groq/#causaliq_knowledge.llm.groq_client.GroqClient.provider_name","title":"provider_name  <code>property</code>","text":"<pre><code>provider_name: str\n</code></pre> <p>Return the provider name.</p>"},{"location":"api/clients/groq/#causaliq_knowledge.llm.groq_client.GroqClient.complete_json","title":"complete_json","text":"<pre><code>complete_json(\n    messages: List[Dict[str, str]], **kwargs: Any\n) -&gt; tuple[Optional[Dict[str, Any]], LLMResponse]\n</code></pre> <p>Make a completion request and parse response as JSON.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>tuple[Optional[Dict[str, Any]], LLMResponse]</code>           \u2013            <p>Tuple of (parsed JSON dict or None, raw LLMResponse).</p> </li> </ul>"},{"location":"api/clients/groq/#causaliq_knowledge.llm.groq_client.GroqClient.complete_json(messages)","title":"<code>messages</code>","text":"(<code>List[Dict[str, str]]</code>)           \u2013            <p>List of message dicts with \"role\" and \"content\" keys.</p>"},{"location":"api/clients/groq/#causaliq_knowledge.llm.groq_client.GroqClient.complete_json(**kwargs)","title":"<code>**kwargs</code>","text":"(<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Override config options passed to completion().</p>"},{"location":"api/clients/groq/#causaliq_knowledge.llm.groq_client.GroqClient.completion","title":"completion","text":"<pre><code>completion(messages: List[Dict[str, str]], **kwargs: Any) -&gt; LLMResponse\n</code></pre> <p>Make a chat completion request to Groq.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>LLMResponse</code>           \u2013            <p>LLMResponse with the generated content and metadata.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the API request fails.</p> </li> </ul>"},{"location":"api/clients/groq/#causaliq_knowledge.llm.groq_client.GroqClient.completion(messages)","title":"<code>messages</code>","text":"(<code>List[Dict[str, str]]</code>)           \u2013            <p>List of message dicts with \"role\" and \"content\" keys.</p>"},{"location":"api/clients/groq/#causaliq_knowledge.llm.groq_client.GroqClient.completion(**kwargs)","title":"<code>**kwargs</code>","text":"(<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Override config options (temperature, max_tokens).</p>"},{"location":"api/clients/groq/#causaliq_knowledge.llm.groq_client.GroqClient.is_available","title":"is_available","text":"<pre><code>is_available() -&gt; bool\n</code></pre> <p>Check if Groq API is available.</p> <p>Returns:</p> <ul> <li> <code>bool</code>           \u2013            <p>True if GROQ_API_KEY is configured.</p> </li> </ul>"},{"location":"api/clients/groq/#causaliq_knowledge.llm.groq_client.GroqClient.list_models","title":"list_models","text":"<pre><code>list_models() -&gt; List[str]\n</code></pre> <p>List available models from Groq API.</p> <p>Queries the Groq API to get models accessible with the current API key. Filters to only include text generation models.</p> <p>Returns:</p> <ul> <li> <code>List[str]</code>           \u2013            <p>List of model identifiers (e.g., ['llama-3.1-8b-instant', ...]).</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the API request fails.</p> </li> </ul>"},{"location":"api/clients/groq/#supported-models","title":"Supported Models","text":"<p>Groq provides fast inference for open-source models:</p> Model Description Free Tier <code>llama-3.1-8b-instant</code> Fast Llama 3.1 8B model \u2705 Yes <code>llama-3.1-70b-versatile</code> Larger Llama 3.1 model \u2705 Yes <code>mixtral-8x7b-32768</code> Mixtral MoE model \u2705 Yes <p>See Groq documentation for the full list of available models.</p>"},{"location":"api/clients/mistral/","title":"Mistral Client","text":"<p>Direct Mistral AI API client for Mistral models.</p>"},{"location":"api/clients/mistral/#overview","title":"Overview","text":"<p>Mistral AI is a French AI company known for high-quality open-weight and proprietary models. Their API is OpenAI-compatible, making integration straightforward.</p> <p>Key features:</p> <ul> <li>Mistral Small: Fast, cost-effective for simple tasks</li> <li>Mistral Large: Most capable, best for complex reasoning</li> <li>Codestral: Optimized for code generation</li> <li>Strong EU-based option for data sovereignty</li> <li>OpenAI-compatible API</li> </ul>"},{"location":"api/clients/mistral/#configuration","title":"Configuration","text":"<p>The client requires a <code>MISTRAL_API_KEY</code> environment variable:</p> <pre><code># Linux/macOS\nexport MISTRAL_API_KEY=\"your-api-key\"\n\n# Windows PowerShell\n$env:MISTRAL_API_KEY=\"your-api-key\"\n\n# Windows cmd\nset MISTRAL_API_KEY=your-api-key\n</code></pre> <p>Get your API key from: https://console.mistral.ai</p>"},{"location":"api/clients/mistral/#usage","title":"Usage","text":""},{"location":"api/clients/mistral/#basic-usage","title":"Basic Usage","text":"<pre><code>from causaliq_knowledge.llm import MistralClient, MistralConfig\n\n# Default config (uses MISTRAL_API_KEY env var)\nclient = MistralClient()\n\n# Or with custom config\nconfig = MistralConfig(\n    model=\"mistral-small-latest\",\n    temperature=0.1,\n    max_tokens=500,\n    timeout=30.0,\n)\nclient = MistralClient(config)\n\n# Make a completion request\nmessages = [{\"role\": \"user\", \"content\": \"What is 2 + 2?\"}]\nresponse = client.completion(messages)\nprint(response.content)\n</code></pre>"},{"location":"api/clients/mistral/#using-with-cli","title":"Using with CLI","text":"<pre><code># Query with Mistral\ncqknow query smoking lung_cancer --model mistral/mistral-small-latest\n\n# Use large model for complex queries\ncqknow query income education --model mistral/mistral-large-latest --domain economics\n\n# List available Mistral models\ncqknow models mistral\n</code></pre>"},{"location":"api/clients/mistral/#using-with-llmknowledge-provider","title":"Using with LLMKnowledge Provider","text":"<pre><code>from causaliq_knowledge.llm import LLMKnowledge\n\n# Single model\nprovider = LLMKnowledge(models=[\"mistral/mistral-small-latest\"])\nresult = provider.query_edge(\"smoking\", \"lung_cancer\")\n\n# Multi-model consensus\nprovider = LLMKnowledge(\n    models=[\n        \"mistral/mistral-large-latest\",\n        \"groq/llama-3.1-8b-instant\",\n    ],\n    consensus_strategy=\"weighted_vote\",\n)\n</code></pre>"},{"location":"api/clients/mistral/#available-models","title":"Available Models","text":"Model Description Best For <code>mistral-small-latest</code> Fast, cost-effective Simple tasks <code>mistral-medium-latest</code> Balanced performance General use <code>mistral-large-latest</code> Most capable Complex reasoning <code>codestral-latest</code> Code-optimized Programming tasks <code>open-mistral-nemo</code> 12B open model Budget-friendly <code>open-mixtral-8x7b</code> MoE open model Balanced open model <code>ministral-3b-latest</code> Ultra-small Edge deployment <code>ministral-8b-latest</code> Small Resource-constrained"},{"location":"api/clients/mistral/#pricing","title":"Pricing","text":"<p>Mistral AI offers competitive pricing (as of Jan 2025):</p> Model Input (per 1M tokens) Output (per 1M tokens) mistral-small $0.20 $0.60 mistral-medium $2.70 $8.10 mistral-large $2.00 $6.00 codestral $0.20 $0.60 open-mistral-nemo $0.15 $0.15 ministral-3b $0.04 $0.04 ministral-8b $0.10 $0.10 <p>See Mistral pricing for details.</p>"},{"location":"api/clients/mistral/#api-reference","title":"API Reference","text":""},{"location":"api/clients/mistral/#causaliq_knowledge.llm.mistral_client.MistralConfig","title":"MistralConfig  <code>dataclass</code>","text":"<pre><code>MistralConfig(\n    model: str = \"mistral-small-latest\",\n    temperature: float = 0.1,\n    max_tokens: int = 500,\n    timeout: float = 30.0,\n    api_key: Optional[str] = None,\n)\n</code></pre> <p>Configuration for Mistral AI API client.</p> <p>Extends OpenAICompatConfig with Mistral-specific defaults.</p> <p>Attributes:</p> <ul> <li> <code>model</code>               (<code>str</code>)           \u2013            <p>Mistral model identifier (default: mistral-small-latest).</p> </li> <li> <code>temperature</code>               (<code>float</code>)           \u2013            <p>Sampling temperature (default: 0.1).</p> </li> <li> <code>max_tokens</code>               (<code>int</code>)           \u2013            <p>Maximum response tokens (default: 500).</p> </li> <li> <code>timeout</code>               (<code>float</code>)           \u2013            <p>Request timeout in seconds (default: 30.0).</p> </li> <li> <code>api_key</code>               (<code>Optional[str]</code>)           \u2013            <p>Mistral API key (falls back to MISTRAL_API_KEY env var).</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>__post_init__</code>             \u2013              <p>Set API key from environment if not provided.</p> </li> </ul>"},{"location":"api/clients/mistral/#causaliq_knowledge.llm.mistral_client.MistralConfig.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__() -&gt; None\n</code></pre> <p>Set API key from environment if not provided.</p>"},{"location":"api/clients/mistral/#causaliq_knowledge.llm.mistral_client.MistralClient","title":"MistralClient","text":"<pre><code>MistralClient(config: Optional[MistralConfig] = None)\n</code></pre> <p>Direct Mistral AI API client.</p> <p>Mistral AI is a French company providing high-quality LLMs with an OpenAI-compatible API.</p> Available models <ul> <li>mistral-small-latest: Fast, cost-effective</li> <li>mistral-medium-latest: Balanced performance</li> <li>mistral-large-latest: Most capable</li> <li>codestral-latest: Optimized for code</li> </ul> Example <p>config = MistralConfig(model=\"mistral-small-latest\") client = MistralClient(config) msgs = [{\"role\": \"user\", \"content\": \"Hello\"}] response = client.completion(msgs) print(response.content)</p> <p>Parameters:</p> <ul> <li> </li> </ul> <p>Methods:</p> <ul> <li> <code>complete_json</code>             \u2013              <p>Make a completion request and parse response as JSON.</p> </li> <li> <code>completion</code>             \u2013              <p>Make a chat completion request.</p> </li> <li> <code>is_available</code>             \u2013              <p>Check if the API is available.</p> </li> <li> <code>list_models</code>             \u2013              <p>List available models from the API.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>call_count</code>               (<code>int</code>)           \u2013            <p>Return the number of API calls made.</p> </li> <li> <code>model_name</code>               (<code>str</code>)           \u2013            <p>Return the model name being used.</p> </li> <li> <code>provider_name</code>               (<code>str</code>)           \u2013            <p>Return the provider name.</p> </li> </ul>"},{"location":"api/clients/mistral/#causaliq_knowledge.llm.mistral_client.MistralClient(config)","title":"<code>config</code>","text":"(<code>Optional[MistralConfig]</code>, default:                   <code>None</code> )           \u2013            <p>Mistral configuration. If None, uses defaults with    API key from MISTRAL_API_KEY environment variable.</p>"},{"location":"api/clients/mistral/#causaliq_knowledge.llm.mistral_client.MistralClient.call_count","title":"call_count  <code>property</code>","text":"<pre><code>call_count: int\n</code></pre> <p>Return the number of API calls made.</p>"},{"location":"api/clients/mistral/#causaliq_knowledge.llm.mistral_client.MistralClient.model_name","title":"model_name  <code>property</code>","text":"<pre><code>model_name: str\n</code></pre> <p>Return the model name being used.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>Model identifier string.</p> </li> </ul>"},{"location":"api/clients/mistral/#causaliq_knowledge.llm.mistral_client.MistralClient.provider_name","title":"provider_name  <code>property</code>","text":"<pre><code>provider_name: str\n</code></pre> <p>Return the provider name.</p>"},{"location":"api/clients/mistral/#causaliq_knowledge.llm.mistral_client.MistralClient.complete_json","title":"complete_json","text":"<pre><code>complete_json(\n    messages: List[Dict[str, str]], **kwargs: Any\n) -&gt; tuple[Optional[Dict[str, Any]], LLMResponse]\n</code></pre> <p>Make a completion request and parse response as JSON.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>tuple[Optional[Dict[str, Any]], LLMResponse]</code>           \u2013            <p>Tuple of (parsed JSON dict or None, raw LLMResponse).</p> </li> </ul>"},{"location":"api/clients/mistral/#causaliq_knowledge.llm.mistral_client.MistralClient.complete_json(messages)","title":"<code>messages</code>","text":"(<code>List[Dict[str, str]]</code>)           \u2013            <p>List of message dicts with \"role\" and \"content\" keys.</p>"},{"location":"api/clients/mistral/#causaliq_knowledge.llm.mistral_client.MistralClient.complete_json(**kwargs)","title":"<code>**kwargs</code>","text":"(<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Override config options passed to completion().</p>"},{"location":"api/clients/mistral/#causaliq_knowledge.llm.mistral_client.MistralClient.completion","title":"completion","text":"<pre><code>completion(messages: List[Dict[str, str]], **kwargs: Any) -&gt; LLMResponse\n</code></pre> <p>Make a chat completion request.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>LLMResponse</code>           \u2013            <p>LLMResponse with the generated content and metadata.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the API request fails.</p> </li> </ul>"},{"location":"api/clients/mistral/#causaliq_knowledge.llm.mistral_client.MistralClient.completion(messages)","title":"<code>messages</code>","text":"(<code>List[Dict[str, str]]</code>)           \u2013            <p>List of message dicts with \"role\" and \"content\" keys.</p>"},{"location":"api/clients/mistral/#causaliq_knowledge.llm.mistral_client.MistralClient.completion(**kwargs)","title":"<code>**kwargs</code>","text":"(<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Override config options (temperature, max_tokens).</p>"},{"location":"api/clients/mistral/#causaliq_knowledge.llm.mistral_client.MistralClient.is_available","title":"is_available","text":"<pre><code>is_available() -&gt; bool\n</code></pre> <p>Check if the API is available.</p> <p>Returns:</p> <ul> <li> <code>bool</code>           \u2013            <p>True if API key is configured.</p> </li> </ul>"},{"location":"api/clients/mistral/#causaliq_knowledge.llm.mistral_client.MistralClient.list_models","title":"list_models","text":"<pre><code>list_models() -&gt; List[str]\n</code></pre> <p>List available models from the API.</p> <p>Queries the API to get models accessible with the current API key, then filters using _filter_models().</p> <p>Returns:</p> <ul> <li> <code>List[str]</code>           \u2013            <p>List of model identifiers.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the API request fails.</p> </li> </ul>"},{"location":"api/clients/ollama/","title":"Ollama Client API Reference","text":"<p>Local Ollama API client for running Llama and other open-source models locally. This client implements the BaseLLMClient interface using httpx to communicate with a locally running Ollama server.</p>"},{"location":"api/clients/ollama/#overview","title":"Overview","text":"<p>The Ollama client provides:</p> <ul> <li>Local LLM inference without API keys or internet access</li> <li>Implements the <code>BaseLLMClient</code> abstract interface</li> <li>Support for Llama 3.2, Llama 3.1, Mistral, and other models</li> <li>JSON response parsing with error handling</li> <li>Call counting for usage tracking</li> <li>Availability checking via <code>is_available()</code> method</li> </ul>"},{"location":"api/clients/ollama/#prerequisites","title":"Prerequisites","text":"<ol> <li>Install Ollama from ollama.com/download</li> <li>Pull a model:    <pre><code>ollama pull llama3.2:1b    # Small, fast (~1.3GB)\nollama pull llama3.2       # Medium (~2GB)\nollama pull llama3.1:8b    # Larger, better quality (~4.7GB)\n</code></pre></li> <li>Ensure Ollama is running (it usually auto-starts after installation)</li> </ol>"},{"location":"api/clients/ollama/#usage","title":"Usage","text":"<pre><code>from causaliq_knowledge.llm import OllamaClient, OllamaConfig\n\n# Create client with default config (llama3.2:1b on localhost:11434)\nclient = OllamaClient()\n\n# Or with custom config\nconfig = OllamaConfig(\n    model=\"llama3.1:8b\",\n    temperature=0.1,\n    max_tokens=500,\n    timeout=120.0,  # Local inference can be slow\n)\nclient = OllamaClient(config=config)\n\n# Check if Ollama is available\nif client.is_available():\n    # Make a completion request\n    messages = [\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"What is 2+2?\"},\n    ]\n    response = client.completion(messages)\n    print(response.content)\nelse:\n    print(\"Ollama not running or model not installed\")\n</code></pre>"},{"location":"api/clients/ollama/#using-with-llmknowledge-provider","title":"Using with LLMKnowledge Provider","text":"<pre><code>from causaliq_knowledge.llm import LLMKnowledge\n\n# Use local Ollama for causal queries\nprovider = LLMKnowledge(models=[\"ollama/llama3.2:1b\"])\nresult = provider.query_edge(\"smoking\", \"lung_cancer\")\nprint(f\"Exists: {result.exists}, Confidence: {result.confidence}\")\n\n# Mix local and cloud models for consensus\nprovider = LLMKnowledge(\n    models=[\n        \"ollama/llama3.2:1b\",\n        \"groq/llama-3.1-8b-instant\",\n    ],\n    consensus_strategy=\"weighted_vote\"\n)\n</code></pre>"},{"location":"api/clients/ollama/#ollamaconfig","title":"OllamaConfig","text":""},{"location":"api/clients/ollama/#causaliq_knowledge.llm.ollama_client.OllamaConfig","title":"OllamaConfig  <code>dataclass</code>","text":"<pre><code>OllamaConfig(\n    model: str = \"llama3.2:1b\",\n    temperature: float = 0.1,\n    max_tokens: int = 500,\n    timeout: float = 120.0,\n    api_key: Optional[str] = None,\n    base_url: str = \"http://localhost:11434\",\n)\n</code></pre> <p>Configuration for Ollama API client.</p> <p>Extends LLMConfig with Ollama-specific defaults.</p> <p>Attributes:</p> <ul> <li> <code>model</code>               (<code>str</code>)           \u2013            <p>Ollama model identifier (default: llama3.2:1b).</p> </li> <li> <code>temperature</code>               (<code>float</code>)           \u2013            <p>Sampling temperature (default: 0.1).</p> </li> <li> <code>max_tokens</code>               (<code>int</code>)           \u2013            <p>Maximum response tokens (default: 500).</p> </li> <li> <code>timeout</code>               (<code>float</code>)           \u2013            <p>Request timeout in seconds (default: 120.0, local).</p> </li> <li> <code>api_key</code>               (<code>Optional[str]</code>)           \u2013            <p>Not used for Ollama (local server).</p> </li> <li> <code>base_url</code>               (<code>str</code>)           \u2013            <p>Ollama server URL (default: http://localhost:11434).</p> </li> </ul>"},{"location":"api/clients/ollama/#causaliq_knowledge.llm.ollama_client.OllamaConfig.api_key","title":"api_key  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>api_key: Optional[str] = None\n</code></pre>"},{"location":"api/clients/ollama/#causaliq_knowledge.llm.ollama_client.OllamaConfig.base_url","title":"base_url  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>base_url: str = 'http://localhost:11434'\n</code></pre>"},{"location":"api/clients/ollama/#causaliq_knowledge.llm.ollama_client.OllamaConfig.max_tokens","title":"max_tokens  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_tokens: int = 500\n</code></pre>"},{"location":"api/clients/ollama/#causaliq_knowledge.llm.ollama_client.OllamaConfig.model","title":"model  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model: str = 'llama3.2:1b'\n</code></pre>"},{"location":"api/clients/ollama/#causaliq_knowledge.llm.ollama_client.OllamaConfig.temperature","title":"temperature  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>temperature: float = 0.1\n</code></pre>"},{"location":"api/clients/ollama/#causaliq_knowledge.llm.ollama_client.OllamaConfig.timeout","title":"timeout  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>timeout: float = 120.0\n</code></pre>"},{"location":"api/clients/ollama/#ollamaclient","title":"OllamaClient","text":""},{"location":"api/clients/ollama/#causaliq_knowledge.llm.ollama_client.OllamaClient","title":"OllamaClient","text":"<pre><code>OllamaClient(config: Optional[OllamaConfig] = None)\n</code></pre> <p>Local Ollama API client.</p> <p>Implements the BaseLLMClient interface for locally running Ollama server. Uses httpx for HTTP requests to the local Ollama API.</p> <p>Ollama provides an OpenAI-compatible API for running open-source models like Llama locally without requiring API keys or internet access.</p> Example <p>config = OllamaConfig(model=\"llama3.2:1b\") client = OllamaClient(config) msgs = [{\"role\": \"user\", \"content\": \"Hello\"}] response = client.completion(msgs) print(response.content)</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>complete_json</code>             \u2013              <p>Make a completion request and parse response as JSON.</p> </li> <li> <code>completion</code>             \u2013              <p>Make a chat completion request to Ollama.</p> </li> <li> <code>is_available</code>             \u2013              <p>Check if Ollama server is running and model is available.</p> </li> <li> <code>list_models</code>             \u2013              <p>List installed models from Ollama.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>_total_calls</code>           \u2013            </li> <li> <code>call_count</code>               (<code>int</code>)           \u2013            <p>Return the number of API calls made.</p> </li> <li> <code>config</code>           \u2013            </li> <li> <code>model_name</code>               (<code>str</code>)           \u2013            <p>Return the model name being used.</p> </li> <li> <code>provider_name</code>               (<code>str</code>)           \u2013            <p>Return the provider name.</p> </li> </ul>"},{"location":"api/clients/ollama/#causaliq_knowledge.llm.ollama_client.OllamaClient(config)","title":"<code>config</code>","text":"(<code>Optional[OllamaConfig]</code>, default:                   <code>None</code> )           \u2013            <p>Ollama configuration. If None, uses defaults connecting    to localhost:11434 with llama3.2:1b model.</p>"},{"location":"api/clients/ollama/#causaliq_knowledge.llm.ollama_client.OllamaClient._total_calls","title":"_total_calls  <code>instance-attribute</code>","text":"<pre><code>_total_calls = 0\n</code></pre>"},{"location":"api/clients/ollama/#causaliq_knowledge.llm.ollama_client.OllamaClient.call_count","title":"call_count  <code>property</code>","text":"<pre><code>call_count: int\n</code></pre> <p>Return the number of API calls made.</p>"},{"location":"api/clients/ollama/#causaliq_knowledge.llm.ollama_client.OllamaClient.config","title":"config  <code>instance-attribute</code>","text":"<pre><code>config = config or OllamaConfig()\n</code></pre>"},{"location":"api/clients/ollama/#causaliq_knowledge.llm.ollama_client.OllamaClient.model_name","title":"model_name  <code>property</code>","text":"<pre><code>model_name: str\n</code></pre> <p>Return the model name being used.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>Model identifier string.</p> </li> </ul>"},{"location":"api/clients/ollama/#causaliq_knowledge.llm.ollama_client.OllamaClient.provider_name","title":"provider_name  <code>property</code>","text":"<pre><code>provider_name: str\n</code></pre> <p>Return the provider name.</p>"},{"location":"api/clients/ollama/#causaliq_knowledge.llm.ollama_client.OllamaClient.complete_json","title":"complete_json","text":"<pre><code>complete_json(\n    messages: List[Dict[str, str]], **kwargs: Any\n) -&gt; tuple[Optional[Dict[str, Any]], LLMResponse]\n</code></pre> <p>Make a completion request and parse response as JSON.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>tuple[Optional[Dict[str, Any]], LLMResponse]</code>           \u2013            <p>Tuple of (parsed JSON dict or None, raw LLMResponse).</p> </li> </ul>"},{"location":"api/clients/ollama/#causaliq_knowledge.llm.ollama_client.OllamaClient.complete_json(messages)","title":"<code>messages</code>","text":"(<code>List[Dict[str, str]]</code>)           \u2013            <p>List of message dicts with \"role\" and \"content\" keys.</p>"},{"location":"api/clients/ollama/#causaliq_knowledge.llm.ollama_client.OllamaClient.complete_json(**kwargs)","title":"<code>**kwargs</code>","text":"(<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Override config options passed to completion().</p>"},{"location":"api/clients/ollama/#causaliq_knowledge.llm.ollama_client.OllamaClient.completion","title":"completion","text":"<pre><code>completion(messages: List[Dict[str, str]], **kwargs: Any) -&gt; LLMResponse\n</code></pre> <p>Make a chat completion request to Ollama.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>LLMResponse</code>           \u2013            <p>LLMResponse with the generated content and metadata.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the API request fails or Ollama is not running.</p> </li> </ul>"},{"location":"api/clients/ollama/#causaliq_knowledge.llm.ollama_client.OllamaClient.completion(messages)","title":"<code>messages</code>","text":"(<code>List[Dict[str, str]]</code>)           \u2013            <p>List of message dicts with \"role\" and \"content\" keys.</p>"},{"location":"api/clients/ollama/#causaliq_knowledge.llm.ollama_client.OllamaClient.completion(**kwargs)","title":"<code>**kwargs</code>","text":"(<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Override config options (temperature, max_tokens).</p>"},{"location":"api/clients/ollama/#causaliq_knowledge.llm.ollama_client.OllamaClient.is_available","title":"is_available","text":"<pre><code>is_available() -&gt; bool\n</code></pre> <p>Check if Ollama server is running and model is available.</p> <p>Returns:</p> <ul> <li> <code>bool</code>           \u2013            <p>True if Ollama is running and the configured model exists.</p> </li> </ul>"},{"location":"api/clients/ollama/#causaliq_knowledge.llm.ollama_client.OllamaClient.list_models","title":"list_models","text":"<pre><code>list_models() -&gt; List[str]\n</code></pre> <p>List installed models from Ollama.</p> <p>Queries the local Ollama server to get installed models. Unlike cloud providers, this returns only models the user has explicitly pulled/installed.</p> <p>Returns:</p> <ul> <li> <code>List[str]</code>           \u2013            <p>List of model identifiers (e.g., ['llama3.2:1b', ...]).</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If Ollama server is not running.</p> </li> </ul>"},{"location":"api/clients/ollama/#supported-models","title":"Supported Models","text":"<p>Ollama supports many open-source models. Recommended for causal queries:</p> Model Size RAM Needed Quality <code>llama3.2:1b</code> ~1.3GB 4GB+ Good for simple queries <code>llama3.2</code> ~2GB 6GB+ Better reasoning <code>llama3.1:8b</code> ~4.7GB 10GB+ Best quality <code>mistral</code> ~4GB 8GB+ Good alternative <p>See Ollama Library for all available models.</p>"},{"location":"api/clients/ollama/#troubleshooting","title":"Troubleshooting","text":"<p>\"Could not connect to Ollama\" - Ensure Ollama is installed and running - Run <code>ollama serve</code> in a terminal, or start the Ollama app - Check that nothing else is using port 11434</p> <p>\"Model not found\" - Run <code>ollama pull &lt;model-name&gt;</code> to download the model - Run <code>ollama list</code> to see installed models</p> <p>Slow responses - Local inference is CPU/GPU bound - Use smaller models like <code>llama3.2:1b</code> - Increase the timeout in <code>OllamaConfig</code> - Consider using GPU acceleration if available</p>"},{"location":"api/clients/openai/","title":"OpenAI Client API Reference","text":"<p>Direct OpenAI API client for GPT models. This client implements the BaseLLMClient interface using httpx to communicate directly with the OpenAI API.</p>"},{"location":"api/clients/openai/#overview","title":"Overview","text":"<p>The OpenAI client provides:</p> <ul> <li>Direct HTTP communication with OpenAI's API</li> <li>Implements the <code>BaseLLMClient</code> abstract interface</li> <li>JSON response parsing with error handling</li> <li>Call counting for usage tracking</li> <li>Cost estimation for API calls</li> <li>Configurable timeout and retry settings</li> </ul>"},{"location":"api/clients/openai/#usage","title":"Usage","text":"<pre><code>from causaliq_knowledge.llm import OpenAIClient, OpenAIConfig\n\n# Create client with custom config\nconfig = OpenAIConfig(\n    model=\"gpt-4o-mini\",\n    temperature=0.1,\n    max_tokens=500,\n)\nclient = OpenAIClient(config=config)\n\n# Make a completion request\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"What is 2+2?\"},\n]\nresponse = client.completion(messages)\nprint(response.content)\n\n# Parse JSON response\njson_data = response.parse_json()\n</code></pre>"},{"location":"api/clients/openai/#environment-variables","title":"Environment Variables","text":"<p>The OpenAI client requires the <code>OPENAI_API_KEY</code> environment variable to be set:</p> <pre><code>export OPENAI_API_KEY=your_api_key_here\n</code></pre>"},{"location":"api/clients/openai/#openaiconfig","title":"OpenAIConfig","text":""},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIConfig","title":"OpenAIConfig  <code>dataclass</code>","text":"<pre><code>OpenAIConfig(\n    model: str = \"gpt-4o-mini\",\n    temperature: float = 0.1,\n    max_tokens: int = 500,\n    timeout: float = 30.0,\n    api_key: Optional[str] = None,\n)\n</code></pre> <p>Configuration for OpenAI API client.</p> <p>Extends OpenAICompatConfig with OpenAI-specific defaults.</p> <p>Attributes:</p> <ul> <li> <code>model</code>               (<code>str</code>)           \u2013            <p>OpenAI model identifier (default: gpt-4o-mini).</p> </li> <li> <code>temperature</code>               (<code>float</code>)           \u2013            <p>Sampling temperature (default: 0.1).</p> </li> <li> <code>max_tokens</code>               (<code>int</code>)           \u2013            <p>Maximum response tokens (default: 500).</p> </li> <li> <code>timeout</code>               (<code>float</code>)           \u2013            <p>Request timeout in seconds (default: 30.0).</p> </li> <li> <code>api_key</code>               (<code>Optional[str]</code>)           \u2013            <p>OpenAI API key (falls back to OPENAI_API_KEY env var).</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>__post_init__</code>             \u2013              <p>Set API key from environment if not provided.</p> </li> </ul>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIConfig.api_key","title":"api_key  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>api_key: Optional[str] = None\n</code></pre>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIConfig.max_tokens","title":"max_tokens  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_tokens: int = 500\n</code></pre>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIConfig.model","title":"model  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model: str = 'gpt-4o-mini'\n</code></pre>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIConfig.temperature","title":"temperature  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>temperature: float = 0.1\n</code></pre>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIConfig.timeout","title":"timeout  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>timeout: float = 30.0\n</code></pre>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIConfig.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__() -&gt; None\n</code></pre> <p>Set API key from environment if not provided.</p>"},{"location":"api/clients/openai/#openaiclient","title":"OpenAIClient","text":""},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIClient","title":"OpenAIClient","text":"<pre><code>OpenAIClient(config: Optional[OpenAIConfig] = None)\n</code></pre> <p>Direct OpenAI API client.</p> <p>Implements the BaseLLMClient interface for OpenAI's API. Uses httpx for HTTP requests.</p> Example <p>config = OpenAIConfig(model=\"gpt-4o-mini\") client = OpenAIClient(config) msgs = [{\"role\": \"user\", \"content\": \"Hello\"}] response = client.completion(msgs) print(response.content)</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>_calculate_cost</code>             \u2013              <p>Calculate approximate cost for API call.</p> </li> <li> <code>_default_config</code>             \u2013              <p>Return default OpenAI configuration.</p> </li> <li> <code>_filter_models</code>             \u2013              <p>Filter to OpenAI chat models only.</p> </li> <li> <code>_get_pricing</code>             \u2013              <p>Return OpenAI pricing per 1M tokens.</p> </li> <li> <code>complete_json</code>             \u2013              <p>Make a completion request and parse response as JSON.</p> </li> <li> <code>completion</code>             \u2013              <p>Make a chat completion request.</p> </li> <li> <code>is_available</code>             \u2013              <p>Check if the API is available.</p> </li> <li> <code>list_models</code>             \u2013              <p>List available models from the API.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>BASE_URL</code>           \u2013            </li> <li> <code>ENV_VAR</code>           \u2013            </li> <li> <code>PROVIDER_NAME</code>           \u2013            </li> <li> <code>_total_calls</code>           \u2013            </li> <li> <code>call_count</code>               (<code>int</code>)           \u2013            <p>Return the number of API calls made.</p> </li> <li> <code>config</code>           \u2013            </li> <li> <code>model_name</code>               (<code>str</code>)           \u2013            <p>Return the model name being used.</p> </li> <li> <code>provider_name</code>               (<code>str</code>)           \u2013            <p>Return the provider name.</p> </li> </ul>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIClient(config)","title":"<code>config</code>","text":"(<code>Optional[OpenAIConfig]</code>, default:                   <code>None</code> )           \u2013            <p>OpenAI configuration. If None, uses defaults with    API key from OPENAI_API_KEY environment variable.</p>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIClient.BASE_URL","title":"BASE_URL  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>BASE_URL = 'https://api.openai.com/v1'\n</code></pre>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIClient.ENV_VAR","title":"ENV_VAR  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ENV_VAR = 'OPENAI_API_KEY'\n</code></pre>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIClient.PROVIDER_NAME","title":"PROVIDER_NAME  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>PROVIDER_NAME = 'openai'\n</code></pre>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIClient._total_calls","title":"_total_calls  <code>instance-attribute</code>","text":"<pre><code>_total_calls = 0\n</code></pre>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIClient.call_count","title":"call_count  <code>property</code>","text":"<pre><code>call_count: int\n</code></pre> <p>Return the number of API calls made.</p>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIClient.config","title":"config  <code>instance-attribute</code>","text":"<pre><code>config = config or _default_config()\n</code></pre>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIClient.model_name","title":"model_name  <code>property</code>","text":"<pre><code>model_name: str\n</code></pre> <p>Return the model name being used.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>Model identifier string.</p> </li> </ul>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIClient.provider_name","title":"provider_name  <code>property</code>","text":"<pre><code>provider_name: str\n</code></pre> <p>Return the provider name.</p>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIClient._calculate_cost","title":"_calculate_cost","text":"<pre><code>_calculate_cost(model: str, input_tokens: int, output_tokens: int) -&gt; float\n</code></pre> <p>Calculate approximate cost for API call.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>float</code>           \u2013            <p>Estimated cost in USD.</p> </li> </ul>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIClient._calculate_cost(model)","title":"<code>model</code>","text":"(<code>str</code>)           \u2013            <p>Model identifier.</p>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIClient._calculate_cost(input_tokens)","title":"<code>input_tokens</code>","text":"(<code>int</code>)           \u2013            <p>Number of input tokens.</p>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIClient._calculate_cost(output_tokens)","title":"<code>output_tokens</code>","text":"(<code>int</code>)           \u2013            <p>Number of output tokens.</p>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIClient._default_config","title":"_default_config","text":"<pre><code>_default_config() -&gt; OpenAIConfig\n</code></pre> <p>Return default OpenAI configuration.</p>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIClient._filter_models","title":"_filter_models","text":"<pre><code>_filter_models(models: List[str]) -&gt; List[str]\n</code></pre> <p>Filter to OpenAI chat models only.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>List[str]</code>           \u2013            <p>Filtered list of GPT and o1/o3 models.</p> </li> </ul>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIClient._filter_models(models)","title":"<code>models</code>","text":"(<code>List[str]</code>)           \u2013            <p>List of all model IDs from API.</p>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIClient._get_pricing","title":"_get_pricing","text":"<pre><code>_get_pricing() -&gt; Dict[str, Dict[str, float]]\n</code></pre> <p>Return OpenAI pricing per 1M tokens.</p> <p>Returns:</p> <ul> <li> <code>Dict[str, Dict[str, float]]</code>           \u2013            <p>Dict mapping model prefixes to input/output costs.</p> </li> </ul>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIClient.complete_json","title":"complete_json","text":"<pre><code>complete_json(\n    messages: List[Dict[str, str]], **kwargs: Any\n) -&gt; tuple[Optional[Dict[str, Any]], LLMResponse]\n</code></pre> <p>Make a completion request and parse response as JSON.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>tuple[Optional[Dict[str, Any]], LLMResponse]</code>           \u2013            <p>Tuple of (parsed JSON dict or None, raw LLMResponse).</p> </li> </ul>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIClient.complete_json(messages)","title":"<code>messages</code>","text":"(<code>List[Dict[str, str]]</code>)           \u2013            <p>List of message dicts with \"role\" and \"content\" keys.</p>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIClient.complete_json(**kwargs)","title":"<code>**kwargs</code>","text":"(<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Override config options passed to completion().</p>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIClient.completion","title":"completion","text":"<pre><code>completion(messages: List[Dict[str, str]], **kwargs: Any) -&gt; LLMResponse\n</code></pre> <p>Make a chat completion request.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>LLMResponse</code>           \u2013            <p>LLMResponse with the generated content and metadata.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the API request fails.</p> </li> </ul>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIClient.completion(messages)","title":"<code>messages</code>","text":"(<code>List[Dict[str, str]]</code>)           \u2013            <p>List of message dicts with \"role\" and \"content\" keys.</p>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIClient.completion(**kwargs)","title":"<code>**kwargs</code>","text":"(<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Override config options (temperature, max_tokens).</p>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIClient.is_available","title":"is_available","text":"<pre><code>is_available() -&gt; bool\n</code></pre> <p>Check if the API is available.</p> <p>Returns:</p> <ul> <li> <code>bool</code>           \u2013            <p>True if API key is configured.</p> </li> </ul>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIClient.list_models","title":"list_models","text":"<pre><code>list_models() -&gt; List[str]\n</code></pre> <p>List available models from the API.</p> <p>Queries the API to get models accessible with the current API key, then filters using _filter_models().</p> <p>Returns:</p> <ul> <li> <code>List[str]</code>           \u2013            <p>List of model identifiers.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the API request fails.</p> </li> </ul>"},{"location":"api/clients/openai/#supported-models","title":"Supported Models","text":"<p>OpenAI provides the GPT family of models:</p> Model Description Free Tier <code>gpt-4o</code> GPT-4o - flagship multimodal model \u274c No <code>gpt-4o-mini</code> GPT-4o Mini - affordable and fast \u274c No <code>gpt-4-turbo</code> GPT-4 Turbo - high capability \u274c No <code>gpt-3.5-turbo</code> GPT-3.5 Turbo - fast and economical \u274c No <code>o1</code> o1 - reasoning model \u274c No <code>o1-mini</code> o1 Mini - efficient reasoning \u274c No <p>See OpenAI documentation for the full list of available models and pricing.</p>"},{"location":"architecture/llm_integration/","title":"LLM Integration Design Note","text":""},{"location":"architecture/llm_integration/#overview","title":"Overview","text":"<p>This document describes how causaliq-knowledge integrates with Large Language Models (LLMs) to provide knowledge about causal relationships. The primary use case for v0.1.0 is answering queries about edge existence and edge orientation to support graph averaging in causaliq-analysis.</p>"},{"location":"architecture/llm_integration/#how-it-works","title":"How it works","text":""},{"location":"architecture/llm_integration/#query-flow","title":"Query Flow","text":"<ol> <li>Consumer requests knowledge about a potential edge (e.g., \"Does smoking cause cancer?\")</li> <li>KnowledgeProvider receives the query with optional context</li> <li>LLM client formats the query using structured prompts</li> <li>One or more LLMs are queried (configurable)</li> <li>Responses are parsed into structured <code>EdgeKnowledge</code> objects</li> <li>Multi-LLM consensus combines responses (if multiple models used)</li> <li>Result returned with confidence score and reasoning</li> </ol>"},{"location":"architecture/llm_integration/#core-interface","title":"Core Interface","text":"<pre><code>from abc import ABC, abstractmethod\nfrom pydantic import BaseModel\n\nclass EdgeKnowledge(BaseModel):\n    \"\"\"Structured knowledge about a potential causal edge.\"\"\"\n    exists: bool | None           # True, False, or None (uncertain)\n    direction: str | None         # \"a_to_b\", \"b_to_a\", \"undirected\", None\n    confidence: float             # 0.0 to 1.0\n    reasoning: str                # Human-readable explanation\n    model: str | None = None      # Which LLM provided this (for logging)\n\nclass KnowledgeProvider(ABC):\n    \"\"\"Abstract interface for all knowledge sources.\"\"\"\n\n    @abstractmethod\n    def query_edge(\n        self,\n        node_a: str,\n        node_b: str,\n        context: dict | None = None\n    ) -&gt; EdgeKnowledge:\n        \"\"\"\n        Query whether a causal edge exists between two nodes.\n\n        Args:\n            node_a: Name of first variable\n            node_b: Name of second variable  \n            context: Optional context (domain, variable descriptions, etc.)\n\n        Returns:\n            EdgeKnowledge with existence, direction, confidence, reasoning\n        \"\"\"\n        pass\n</code></pre>"},{"location":"architecture/llm_integration/#llm-implementation","title":"LLM Implementation","text":"<pre><code>class LLMKnowledge(KnowledgeProvider):\n    \"\"\"LLM-based knowledge provider using vendor-specific API clients.\"\"\"\n\n    def __init__(\n        self,\n        models: list[str] = [\"groq/llama-3.1-8b-instant\"],\n        consensus_strategy: str = \"weighted_vote\",\n        temperature: float = 0.1,\n        max_tokens: int = 500,\n    ):\n        \"\"\"\n        Initialize LLM knowledge provider.\n\n        Args:\n            models: List of model identifiers with provider prefix.\n                   e.g., [\"groq/llama-3.1-8b-instant\", \"gemini/gemini-2.5-flash\"]\n            consensus_strategy: How to combine multi-model responses\n                               \"weighted_vote\" or \"highest_confidence\"\n            temperature: LLM temperature (0.0-1.0)\n            max_tokens: Maximum tokens in response\n        \"\"\"\n        ...\n</code></pre>"},{"location":"architecture/llm_integration/#llm-provider-configuration","title":"LLM Provider Configuration","text":""},{"location":"architecture/llm_integration/#architectural-decision-vendor-specific-apis","title":"Architectural Decision: Vendor-Specific APIs","text":"<p>We use direct vendor-specific API clients rather than wrapper libraries like LiteLLM or LangChain. Each provider has a dedicated client class that uses httpx for HTTP communication.</p> <p>Benefits of this approach:</p> <ul> <li>Reliability: No wrapper bugs or version conflicts</li> <li>Minimal dependencies: Only httpx required for HTTP</li> <li>Full control: Direct access to vendor-specific features</li> <li>Better debugging: Clear stack traces without abstraction layers</li> <li>Predictable behavior: No surprises from wrapper library updates</li> </ul>"},{"location":"architecture/llm_integration/#supported-providers","title":"Supported Providers","text":"Provider Client Class Model Examples API Key Variable Groq <code>GroqClient</code> <code>groq/llama-3.1-8b-instant</code> <code>GROQ_API_KEY</code> Google Gemini <code>GeminiClient</code> <code>gemini/gemini-2.5-flash</code> <code>GEMINI_API_KEY</code> OpenAI <code>OpenAIClient</code> <code>openai/gpt-4o-mini</code> <code>OPENAI_API_KEY</code> Anthropic <code>AnthropicClient</code> <code>anthropic/claude-sonnet-4-20250514</code> <code>ANTHROPIC_API_KEY</code> DeepSeek <code>DeepSeekClient</code> <code>deepseek/deepseek-chat</code> <code>DEEPSEEK_API_KEY</code> Mistral <code>MistralClient</code> <code>mistral/mistral-small-latest</code> <code>MISTRAL_API_KEY</code> Ollama <code>OllamaClient</code> <code>ollama/llama3</code> N/A (local) <p>Additional providers can be added by implementing new client classes following the same pattern.</p>"},{"location":"architecture/llm_integration/#cost-considerations","title":"Cost Considerations","text":"<p>For edge queries (~500 tokens each):</p> Provider Model Cost per 1000 queries Quality Speed Groq llama-3.1-8b-instant Free tier Good Very fast Google gemini-2.5-flash Free tier Good Fast Ollama llama3 Free (local) Good Depends on HW DeepSeek deepseek-chat ~$0.07 Excellent Fast Mistral mistral-small-latest ~$0.50 Good Fast OpenAI gpt-4o-mini ~$0.15 Excellent Fast Anthropic claude-sonnet-4-20250514 ~$1.50 Excellent Fast <p>Recommendation: Use Groq free tier for development and testing. Ollama is great for local development. Both Groq and Gemini offer generous free tiers suitable for most research use cases.</p>"},{"location":"architecture/llm_integration/#prompt-design","title":"Prompt Design","text":""},{"location":"architecture/llm_integration/#edge-existence-query","title":"Edge Existence Query","text":"<pre><code>System: You are an expert in causal reasoning and domain knowledge. \nYour task is to assess whether a causal relationship exists between two variables.\nRespond in JSON format with: exists (true/false/null), direction (a_to_b/b_to_a/undirected/null), confidence (0-1), reasoning (string).\n\nUser: In the domain of {domain}, does a causal relationship exist between \"{node_a}\" and \"{node_b}\"?\nConsider:\n- Direct causation (A causes B)\n- Reverse causation (B causes A)  \n- Bidirectional/feedback relationships\n- No causal relationship (correlation only or independence)\n\nVariable context:\n{variable_descriptions}\n</code></pre>"},{"location":"architecture/llm_integration/#response-format","title":"Response Format","text":"<pre><code>{\n  \"exists\": true,\n  \"direction\": \"a_to_b\",\n  \"confidence\": 0.85,\n  \"reasoning\": \"Smoking is an established cause of lung cancer through well-documented biological mechanisms including DNA damage from carcinogens in tobacco smoke.\"\n}\n</code></pre>"},{"location":"architecture/llm_integration/#multi-llm-consensus","title":"Multi-LLM Consensus","text":"<p>When multiple models are configured, responses are combined:</p>"},{"location":"architecture/llm_integration/#weighted-vote-strategy-default","title":"Weighted Vote Strategy (default)","text":"<pre><code>def weighted_vote(responses: list[EdgeKnowledge]) -&gt; EdgeKnowledge:\n    \"\"\"Combine responses weighted by confidence.\"\"\"\n    # For existence: weighted majority vote\n    # For direction: weighted majority among those agreeing on existence\n    # Final confidence: average confidence of agreeing models\n    # Reasoning: concatenate key points from each model\n</code></pre>"},{"location":"architecture/llm_integration/#highest-confidence-strategy","title":"Highest Confidence Strategy","text":"<pre><code>def highest_confidence(responses: list[EdgeKnowledge]) -&gt; EdgeKnowledge:\n    \"\"\"Return response with highest confidence.\"\"\"\n    return max(responses, key=lambda r: r.confidence)\n</code></pre>"},{"location":"architecture/llm_integration/#integration-with-graph-averaging","title":"Integration with Graph Averaging","text":"<p>The primary consumer is <code>causaliq_analysis.graph.average()</code>:</p> <pre><code># Current output from average()\ndf = average(traces, sample_size=1000)\n# Returns: node_a, node_b, p_a_to_b, p_b_to_a, p_undirected, p_no_edge\n\n# Entropy calculation identifies uncertain edges\ndef edge_entropy(row):\n    probs = [row.p_a_to_b, row.p_b_to_a, row.p_undirected, row.p_no_edge]\n    probs = [p for p in probs if p &gt; 0]\n    return -sum(p * math.log2(p) for p in probs)\n\ndf[\"entropy\"] = df.apply(edge_entropy, axis=1)\nuncertain_edges = df[df[\"entropy\"] &gt; 1.5]  # High uncertainty\n\n# Query LLM for uncertain edges\nknowledge = LLMKnowledge(models=[\"groq/llama-3.1-8b-instant\"])\nfor _, row in uncertain_edges.iterrows():\n    result = knowledge.query_edge(row.node_a, row.node_b)\n    # Combine statistical and LLM probabilities...\n</code></pre>"},{"location":"architecture/llm_integration/#design-rationale","title":"Design Rationale","text":""},{"location":"architecture/llm_integration/#why-vendor-specific-apis-not-litellmlangchain","title":"Why Vendor-Specific APIs (not LiteLLM/LangChain)?","text":"<ol> <li>Minimal dependencies: Only httpx for HTTP, no wrapper libraries</li> <li>Reliability: No wrapper bugs or version conflicts to debug</li> <li>Full control: Direct access to vendor-specific features and error handling</li> <li>Predictable: Behavior doesn't change when wrapper library updates</li> <li>Debuggable: Clear stack traces without abstraction layers</li> <li>Lightweight: ~5KB of client code vs ~50MB of wrapper dependencies</li> </ol>"},{"location":"architecture/llm_integration/#why-structured-json-responses","title":"Why structured JSON responses?","text":"<ol> <li>Reliable parsing: Avoids regex/heuristic extraction</li> <li>Validation: Pydantic ensures response integrity</li> <li>Consistency: Same structure regardless of model</li> </ol>"},{"location":"architecture/llm_integration/#why-multi-model-consensus","title":"Why multi-model consensus?","text":"<ol> <li>Reduced hallucination: Multiple models catch individual errors</li> <li>Confidence calibration: Agreement increases confidence</li> <li>Robustness: Not dependent on single provider availability</li> </ol>"},{"location":"architecture/llm_integration/#error-handling-and-resilience","title":"Error Handling and Resilience","text":""},{"location":"architecture/llm_integration/#api-failures","title":"API Failures","text":"<ul> <li>Automatic retry with timeout handling</li> <li>Fallback to next model in list if primary fails</li> <li>Return <code>EdgeKnowledge(exists=None, confidence=0.0)</code> if all fail</li> </ul>"},{"location":"architecture/llm_integration/#invalid-responses","title":"Invalid Responses","text":"<ul> <li>Pydantic validation catches malformed JSON</li> <li>Default to <code>exists=None</code> if parsing fails</li> <li>Log warnings for debugging</li> </ul>"},{"location":"architecture/llm_integration/#rate-limiting","title":"Rate Limiting","text":"<ul> <li>Vendor clients handle rate limit errors gracefully</li> <li>Configure timeout per client</li> </ul>"},{"location":"architecture/llm_integration/#performance","title":"Performance","text":""},{"location":"architecture/llm_integration/#latency","title":"Latency","text":"<ul> <li>Single query: 0.5-2s depending on model/provider</li> <li>Batch queries: Can parallelize across edges (async)</li> <li>Cached queries: &lt;10ms</li> </ul>"},{"location":"architecture/llm_integration/#throughput-v030-with-caching","title":"Throughput (v0.3.0 with caching)","text":"<ul> <li>First query to new edge: 1-2s</li> <li>Cached query: &lt;10ms</li> <li>1000 unique edges: ~20-30 minutes (sequential), ~5 min (parallel)</li> </ul>"},{"location":"architecture/llm_integration/#future-extensions","title":"Future Extensions","text":""},{"location":"architecture/llm_integration/#v030-caching","title":"v0.3.0: Caching","text":"<ul> <li>Disk-based cache keyed by (node_a, node_b, context_hash)</li> <li>Semantic similarity cache for similar variable names</li> </ul>"},{"location":"architecture/llm_integration/#v040-rich-context","title":"v0.4.0: Rich Context","text":"<ul> <li>Variable descriptions and roles</li> <li>Domain-specific literature retrieval (RAG)</li> <li>Conversation history for follow-up queries</li> </ul>"},{"location":"architecture/llm_integration/#v050-algorithm-integration","title":"v0.5.0: Algorithm Integration","text":"<ul> <li>Direct integration with structure learning search</li> <li>Knowledge-guided constraint generation</li> </ul>"},{"location":"architecture/overview/","title":"Architecture Vision for causaliq-knowledge","text":""},{"location":"architecture/overview/#causaliq-ecosystem","title":"CausalIQ Ecosystem","text":"<p>causaliq-knowledge is a component of the overall CausalIQ ecosystem architecture.</p> <p>This package provides knowledge services to other CausalIQ packages, enabling them to incorporate LLM-derived and human-specified knowledge into causal discovery and inference workflows.</p>"},{"location":"architecture/overview/#architectural-principles","title":"Architectural Principles","text":""},{"location":"architecture/overview/#simplicity-first","title":"Simplicity First","text":"<ul> <li>Use lightweight libraries over heavy frameworks</li> <li>Start with minimal viable features, extend incrementally</li> <li>Prefer explicit code over framework \"magic\"</li> <li>Use vendor-specific APIs rather than abstraction wrappers</li> </ul>"},{"location":"architecture/overview/#cost-efficiency","title":"Cost Efficiency","text":"<ul> <li>Built-in cost tracking and budget management (critical for independent research)</li> <li>Caching of LLM queries and responses to avoid redundant API calls</li> <li>Support for cheap/free providers (Groq, Gemini free tiers)</li> </ul>"},{"location":"architecture/overview/#transparency-and-reproducibility","title":"Transparency and Reproducibility","text":"<ul> <li>Cache all LLM interactions for experiment reproducibility</li> <li>Provide reasoning/explanations with all knowledge outputs</li> <li>Log confidence levels to enable uncertainty-aware decisions</li> </ul>"},{"location":"architecture/overview/#clean-interfaces","title":"Clean Interfaces","text":"<ul> <li>Abstract <code>KnowledgeProvider</code> interface allows multiple implementations</li> <li>LLM-based, rule-based, and human-input knowledge sources use same interface</li> <li>Easy integration with causaliq-analysis and causaliq-discovery</li> </ul>"},{"location":"architecture/overview/#architecture-components","title":"Architecture Components","text":""},{"location":"architecture/overview/#core-components-v010","title":"Core Components (v0.1.0)","text":"<pre><code>causaliq_knowledge/\n\u251c\u2500\u2500 __init__.py              # Package exports\n\u251c\u2500\u2500 cli.py                   # Command-line interface\n\u251c\u2500\u2500 base.py                  # Abstract KnowledgeProvider interface\n\u251c\u2500\u2500 models.py                # Pydantic models (EdgeKnowledge, etc.)\n\u2514\u2500\u2500 llm/\n    \u251c\u2500\u2500 __init__.py          # LLM module exports\n    \u251c\u2500\u2500 groq_client.py       # Direct Groq API client\n    \u251c\u2500\u2500 gemini_client.py     # Direct Google Gemini API client\n    \u251c\u2500\u2500 prompts.py           # Prompt templates for edge queries\n    \u2514\u2500\u2500 provider.py          # LLMKnowledge implementation\n</code></pre>"},{"location":"architecture/overview/#data-flow","title":"Data Flow","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                 Consuming Package (e.g., causaliq-analysis)     \u2502\n\u2502                                                                 \u2502\n\u2502   uncertain_edges = df[df[\"entropy\"] &gt; threshold]               \u2502\n\u2502                          \u2502                                      \u2502\n\u2502                          \u25bc                                      \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502   \u2502              causaliq-knowledge                          \u2502   \u2502\n\u2502   \u2502                                                          \u2502   \u2502\n\u2502   \u2502   knowledge.query_edge(\"smoking\", \"cancer\")              \u2502   \u2502\n\u2502   \u2502       \u2502                                                  \u2502   \u2502\n\u2502   \u2502       \u25bc                                                  \u2502   \u2502\n\u2502   \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u2502   \u2502\n\u2502   \u2502   \u2502  LLM 1    \u2502    \u2502  LLM 2    \u2502    \u2502  Cache    \u2502       \u2502   \u2502\n\u2502   \u2502   \u2502 (GPT-4o)  \u2502    \u2502 (Llama3)  \u2502    \u2502 (disk)    \u2502       \u2502   \u2502\n\u2502   \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       \u2502   \u2502\n\u2502   \u2502       \u2502                 \u2502                \u2502               \u2502   \u2502\n\u2502   \u2502       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518               \u2502   \u2502\n\u2502   \u2502                         \u2502                                \u2502   \u2502\n\u2502   \u2502                         \u25bc                                \u2502   \u2502\n\u2502   \u2502               EdgeKnowledge(                             \u2502   \u2502\n\u2502   \u2502                   exists=True,                           \u2502   \u2502\n\u2502   \u2502                   direction=\"a_to_b\",                    \u2502   \u2502\n\u2502   \u2502                   confidence=0.85,                       \u2502   \u2502\n\u2502   \u2502                   reasoning=\"Established medical...\"     \u2502   \u2502\n\u2502   \u2502               )                                          \u2502   \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2502                          \u2502                                      \u2502\n\u2502                          \u25bc                                      \u2502\n\u2502   Combine with statistical probabilities                        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture/overview/#technology-choices","title":"Technology Choices","text":""},{"location":"architecture/overview/#vendor-specific-apis-over-wrapper-libraries","title":"Vendor-Specific APIs over Wrapper Libraries","text":"<p>We use direct vendor-specific API clients rather than wrapper libraries like LiteLLM or LangChain. This architectural decision provides:</p> Aspect Direct APIs Wrapper Libraries Reliability \u2705 Full control, predictable \u274c Wrapper bugs, version drift Debugging \u2705 Clear stack traces \u274c Abstraction layers Dependencies \u2705 Minimal (httpx only) \u274c Heavy transitive deps API Coverage \u2705 Full vendor features \u274c Lowest common denominator Maintenance \u2705 We control updates \u274c Wait for wrapper updates <p>Why Not LiteLLM?</p> <ul> <li>Adds 50+ transitive dependencies</li> <li>Version conflicts with other packages</li> <li>Wrapper bugs mask vendor API issues</li> <li>We only need 2-3 providers, not 100+</li> </ul> <p>Why Not LangChain?</p> <ul> <li>Massive dependency footprint (~100MB+)</li> <li>Over-engineered for simple structured queries  </li> <li>Rapid breaking changes between versions</li> <li>May reconsider for v0.4.0+ RAG features only</li> </ul>"},{"location":"architecture/overview/#current-provider-clients","title":"Current Provider Clients","text":"<ul> <li>GroqClient: Direct Groq API via httpx (free tier, fast inference)</li> <li>GeminiClient: Direct Google Gemini API via httpx (generous free tier)</li> </ul>"},{"location":"architecture/overview/#key-dependencies","title":"Key Dependencies","text":"<ul> <li>httpx: HTTP client for API calls</li> <li>pydantic: Structured response validation</li> <li>click: Command-line interface</li> <li>diskcache (v0.3.0): Persistent query caching</li> </ul>"},{"location":"architecture/overview/#integration-points","title":"Integration Points","text":""},{"location":"architecture/overview/#with-causaliq-analysis","title":"With causaliq-analysis","text":"<p>The primary integration point is the <code>average()</code> function which produces edge probability tables. Future versions will accept a <code>knowledge</code> parameter:</p> <pre><code># Future usage (v0.5.0 of causaliq-analysis)\nfrom causaliq_knowledge import LLMKnowledge\n\nknowledge = LLMKnowledge(models=[\"gpt-4o-mini\"])\ndf = average(traces, sample_size=1000, knowledge=knowledge)\n</code></pre>"},{"location":"architecture/overview/#with-causaliq-discovery","title":"With causaliq-discovery","text":"<p>Structure learning algorithms will use knowledge to guide search in uncertain areas of the graph space.</p>"},{"location":"architecture/overview/#see-also","title":"See Also","text":"<ul> <li>LLM Integration Design Note - Detailed design for LLM queries</li> <li>Roadmap - Release planning</li> </ul>"},{"location":"architecture/testing_strategy/","title":"Testing Strategy Design Note","text":""},{"location":"architecture/testing_strategy/#overview","title":"Overview","text":"<p>Testing LLM-dependent code presents unique challenges: API calls cost money, responses are non-deterministic, and external services may be unavailable. This document describes the testing strategy for causaliq-knowledge.</p>"},{"location":"architecture/testing_strategy/#testing-pyramid","title":"Testing Pyramid","text":"<pre><code>                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502   Functional    \u2502  \u2190 Cached responses (v0.3.0+)\n                    \u2502     Tests       \u2502     Real scenarios, reproducible\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n               \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n               \u2502     Integration Tests     \u2502  \u2190 Real API calls (optional)\n               \u2502   (with live LLM APIs)    \u2502     Expensive, non-deterministic\n               \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502                  Unit Tests                      \u2502  \u2190 Mocked LLM responses\n    \u2502           (mocked LLM responses)                 \u2502     Fast, free, deterministic\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture/testing_strategy/#test-categories","title":"Test Categories","text":""},{"location":"architecture/testing_strategy/#1-unit-tests-always-run-in-ci","title":"1. Unit Tests (Always Run in CI)","text":"<p>Unit tests mock all LLM calls, making them:</p> <ul> <li>Fast: No network latency</li> <li>Free: No API costs</li> <li>Deterministic: Same result every time</li> <li>Isolated: No external dependencies</li> </ul> <pre><code># tests/unit/test_llm_providers.py\nimport pytest\nfrom unittest.mock import MagicMock\n\n\ndef test_query_edge_parses_valid_response(monkeypatch):\n    \"\"\"Test that valid LLM JSON is correctly parsed.\"\"\"\n    from causaliq_knowledge.llm import LLMKnowledge\n    from causaliq_knowledge.llm.groq_client import GroqClient\n\n    # Mock the Groq client's complete_json method\n    mock_json = {\n        \"exists\": True,\n        \"direction\": \"a_to_b\",\n        \"confidence\": 0.85,\n        \"reasoning\": \"Smoking causes lung cancer via carcinogens.\"\n    }\n\n    mock_client = MagicMock(spec=GroqClient)\n    mock_client.complete_json.return_value = (mock_json, MagicMock())\n\n    knowledge = LLMKnowledge(models=[\"groq/llama-3.1-8b-instant\"])\n    knowledge._clients[\"groq/llama-3.1-8b-instant\"] = mock_client\n\n    result = knowledge.query_edge(\"smoking\", \"lung_cancer\")\n\n    assert result.exists is True\n    assert result.direction.value == \"a_to_b\"\n    assert result.confidence == 0.85\n\n\ndef test_query_edge_handles_malformed_json(monkeypatch):\n    \"\"\"Test graceful handling of invalid LLM response.\"\"\"\n    from causaliq_knowledge.llm import LLMKnowledge\n    from causaliq_knowledge.llm.groq_client import GroqClient\n\n    # Mock returning None (failed parse)\n    mock_client = MagicMock(spec=GroqClient)\n    mock_client.complete_json.return_value = (None, MagicMock())\n\n    knowledge = LLMKnowledge(models=[\"groq/llama-3.1-8b-instant\"])\n    knowledge._clients[\"groq/llama-3.1-8b-instant\"] = mock_client\n\n    result = knowledge.query_edge(\"A\", \"B\")\n\n    assert result.exists is None  # Uncertain\n    assert result.confidence == 0.0\n</code></pre>"},{"location":"architecture/testing_strategy/#2-integration-tests-optional-manual-or-ci-with-secrets","title":"2. Integration Tests (Optional, Manual or CI with Secrets)","text":"<p>Integration tests use real LLM APIs to validate actual behavior:</p> <ul> <li>Expensive: May cost money per call (though free tiers available)</li> <li>Non-deterministic: LLM responses vary</li> <li>Slow: Network latency</li> <li>Validates real integration: Catches API changes</li> </ul> <pre><code># tests/integration/test_llm_live.py\nimport pytest\nimport os\n\npytestmark = pytest.mark.skipif(\n    not os.getenv(\"GROQ_API_KEY\"),\n    reason=\"GROQ_API_KEY not set\"\n)\n\n@pytest.mark.slow\n@pytest.mark.integration\ndef test_groq_returns_valid_response():\n    \"\"\"Validate real Groq API returns parseable response.\"\"\"\n    from causaliq_knowledge.llm import LLMKnowledge\n\n    knowledge = LLMKnowledge(models=[\"groq/llama-3.1-8b-instant\"])\n    result = knowledge.query_edge(\"smoking\", \"lung_cancer\")\n\n    # Don't assert specific values - LLM may vary\n    # Just validate structure and reasonable bounds\n    assert result.exists in [True, False, None]\n    assert 0.0 &lt;= result.confidence &lt;= 1.0\n    assert len(result.reasoning) &gt; 0\n</code></pre>"},{"location":"architecture/testing_strategy/#3-functional-tests-with-cached-responses-v030","title":"3. Functional Tests with Cached Responses (v0.3.0+)","text":"<p>Once response caching is implemented, we can create reproducible functional tests using cached LLM responses. This is the best of both worlds:</p> <ul> <li>Realistic: Uses actual LLM responses (captured once)</li> <li>Deterministic: Same cached response every time</li> <li>Free: No API calls after initial capture</li> <li>Fast: Disk read instead of network call</li> </ul>"},{"location":"architecture/testing_strategy/#how-it-works","title":"How It Works","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                     Test Fixture Generation                      \u2502\n\u2502                      (run once, manually)                        \u2502\n\u2502                                                                  \u2502\n\u2502   1. Run queries against real LLMs                               \u2502\n\u2502   2. Cache stores responses in tests/data/functional/cache/     \u2502\n\u2502   3. Commit cache files to git                                   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                     Functional Tests (CI)                        \u2502\n\u2502                                                                  \u2502\n\u2502   1. Load cached responses from tests/data/functional/cache/    \u2502\n\u2502   2. LLMKnowledge configured to use cache-only mode             \u2502\n\u2502   3. Tests run with real LLM responses, no API calls            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture/testing_strategy/#example-functional-test","title":"Example Functional Test","text":"<pre><code># tests/functional/test_edge_queries.py\nimport pytest\nfrom pathlib import Path\n\nCACHE_DIR = Path(__file__).parent / \"data\" / \"cache\"\n\n@pytest.fixture\ndef cached_knowledge():\n    \"\"\"LLMKnowledge using only cached responses.\"\"\"\n    return LLMKnowledge(\n        models=[\"groq/llama-3.1-8b-instant\"],\n        cache_dir=str(CACHE_DIR),\n        cache_only=True  # Fail if cache miss, don't call API\n    )\n\ndef test_smoking_cancer_relationship(cached_knowledge):\n    \"\"\"Test with cached response for smoking-&gt;cancer query.\"\"\"\n    result = cached_knowledge.query_edge(\"smoking\", \"lung_cancer\")\n\n    # Can assert specific values since response is cached\n    assert result.exists is True\n    assert result.direction == \"a_to_b\"\n    assert result.confidence &gt; 0.8\n\ndef test_consensus_across_models(cached_knowledge):\n    \"\"\"Test multi-model consensus with cached responses.\"\"\"\n    knowledge = LLMKnowledge(\n        models=[\"groq/llama-3.1-8b-instant\", \"gemini/gemini-2.5-flash\"],\n        cache_dir=str(CACHE_DIR),\n        cache_only=True\n    )\n    result = knowledge.query_edge(\"exercise\", \"heart_health\")\n\n    assert result.exists is True\n</code></pre>"},{"location":"architecture/testing_strategy/#generating-test-fixtures","title":"Generating Test Fixtures","text":"<pre><code># scripts/generate_test_fixtures.py\n\"\"\"\nRun this script manually to generate/update cached responses for functional tests.\nRequires API keys for all models being tested.\n\"\"\"\nfrom causaliq_knowledge.llm import LLMKnowledge\nfrom pathlib import Path\n\nCACHE_DIR = Path(\"tests/data/functional/cache\")\nTEST_EDGES = [\n    (\"smoking\", \"lung_cancer\"),\n    (\"exercise\", \"heart_health\"),\n    (\"education\", \"income\"),\n    (\"rain\", \"wet_ground\"),\n]\n\ndef generate_fixtures():\n    knowledge = LLMKnowledge(\n        models=[\"groq/llama-3.1-8b-instant\", \"gemini/gemini-2.5-flash\"],\n        cache_dir=str(CACHE_DIR)\n    )\n\n    for node_a, node_b in TEST_EDGES:\n        print(f\"Caching: {node_a} -&gt; {node_b}\")\n        knowledge.query_edge(node_a, node_b)\n\n    print(f\"Fixtures saved to {CACHE_DIR}\")\n\nif __name__ == \"__main__\":\n    generate_fixtures()\n</code></pre>"},{"location":"architecture/testing_strategy/#ci-configuration","title":"CI Configuration","text":""},{"location":"architecture/testing_strategy/#pytest-markers","title":"pytest Markers","text":"<pre><code># pyproject.toml\n[tool.pytest.ini_options]\nmarkers = [\n    \"slow: marks tests as slow (deselect with '-m \\\"not slow\\\"')\",\n    \"integration: marks tests requiring live external APIs\",\n    \"functional: marks functional tests using cached responses\",\n]\naddopts = \"-ra -q --strict-markers -m 'not slow and not integration'\"\n</code></pre>"},{"location":"architecture/testing_strategy/#github-actions-strategy","title":"GitHub Actions Strategy","text":"Test Type When API Keys Cost Unit Every push/PR \u274c Not needed Free Functional Every push/PR \u274c Uses cache Free Integration Main branch only, optional \u2705 GitHub Secrets ~$0.01/run <pre><code># .github/workflows/ci.yml (conceptual addition)\njobs:\n  unit-and-functional:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - name: Run unit and functional tests\n        run: pytest tests/unit tests/functional\n\n  integration:\n    runs-on: ubuntu-latest\n    if: github.ref == 'refs/heads/main'  # Only on main\n    steps:\n      - uses: actions/checkout@v4\n      - name: Run integration tests\n        env:\n          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}\n        run: pytest tests/integration -m integration\n</code></pre>"},{"location":"architecture/testing_strategy/#test-data-management","title":"Test Data Management","text":""},{"location":"architecture/testing_strategy/#directory-structure","title":"Directory Structure","text":"<pre><code>tests/\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 unit/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 test_models.py          # EdgeKnowledge, etc.\n\u2502   \u251c\u2500\u2500 test_prompts.py         # Prompt formatting\n\u2502   \u2514\u2500\u2500 test_llm_providers.py   # Mocked LLM calls\n\u251c\u2500\u2500 integration/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2514\u2500\u2500 test_llm_live.py        # Real API calls\n\u251c\u2500\u2500 functional/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2514\u2500\u2500 test_edge_queries.py    # Using cached responses\n\u2514\u2500\u2500 data/\n    \u2514\u2500\u2500 functional/\n        \u2514\u2500\u2500 cache/              # Committed to git\n            \u251c\u2500\u2500 groq/\n            \u2502   \u251c\u2500\u2500 smoking_lung_cancer.json\n            \u2502   \u2514\u2500\u2500 exercise_heart_health.json\n            \u2514\u2500\u2500 gemini/\n                \u2514\u2500\u2500 ...\n</code></pre>"},{"location":"architecture/testing_strategy/#cache-file-format","title":"Cache File Format","text":"<pre><code>{\n  \"query\": {\n    \"node_a\": \"smoking\",\n    \"node_b\": \"lung_cancer\",\n    \"context\": {\"domain\": \"epidemiology\"}\n  },\n  \"model\": \"groq/llama-3.1-8b-instant\",\n  \"timestamp\": \"2026-01-05T10:30:00Z\",\n  \"response\": {\n    \"exists\": true,\n    \"direction\": \"a_to_b\",\n    \"confidence\": 0.92,\n    \"reasoning\": \"Smoking is an established cause of lung cancer...\"\n  }\n}\n</code></pre>"},{"location":"architecture/testing_strategy/#benefits-of-this-strategy","title":"Benefits of This Strategy","text":"Benefit How Achieved Fast CI Unit tests are mocked, functional use cache Low cost Only integration tests (optional) call APIs Reproducible Cached responses are deterministic Realistic Functional tests use real LLM responses Stable experiments Same cache = same results across runs Version controlled Cache files in git track response changes"},{"location":"architecture/testing_strategy/#future-considerations","title":"Future Considerations","text":""},{"location":"architecture/testing_strategy/#cache-invalidation-for-tests","title":"Cache Invalidation for Tests","text":"<p>When updating test fixtures:</p> <ol> <li>Delete relevant cache files</li> <li>Run fixture generation script</li> <li>Review new responses</li> <li>Commit updated cache files</li> </ol>"},{"location":"architecture/testing_strategy/#model-version-tracking","title":"Model Version Tracking","text":"<p>Cache files should include model version to detect when responses might change due to model updates.</p>"},{"location":"architecture/testing_strategy/#semantic-similarity-testing","title":"Semantic Similarity Testing","text":"<p>For v0.4.0+, consider testing that semantically similar queries hit cache (e.g., \"smoking\" vs \"tobacco use\" \u2192 \"cancer\" vs \"lung cancer\").</p>"},{"location":"userguide/introduction/","title":"CausalIQ Knowledge User Guide","text":""},{"location":"userguide/introduction/#what-is-causaliq-knowledge","title":"What is CausalIQ Knowledge?","text":"<p>CausalIQ Knowledge is a Python package that provides knowledge services for causal discovery workflows. It enables you to query Large Language Models (LLMs) about potential causal relationships between variables, helping to resolve uncertainty in learned causal graphs.</p>"},{"location":"userguide/introduction/#primary-use-case","title":"Primary Use Case","text":"<p>When averaging multiple causal graphs learned from data subsamples, some edges may be uncertain - appearing in some graphs but not others, or with inconsistent directions. CausalIQ Knowledge helps resolve this uncertainty by querying LLMs about whether:</p> <ol> <li>A causal relationship exists between two variables</li> <li>What the direction of causation is (A\u2192B or B\u2192A)</li> </ol>"},{"location":"userguide/introduction/#quick-start","title":"Quick Start","text":""},{"location":"userguide/introduction/#installation","title":"Installation","text":"<pre><code>pip install causaliq-knowledge\n</code></pre>"},{"location":"userguide/introduction/#basic-usage","title":"Basic Usage","text":"<pre><code>from causaliq_knowledge.llm import LLMKnowledge\n\n# Initialize with Groq (default, free tier)\nknowledge = LLMKnowledge(models=[\"groq/llama-3.1-8b-instant\"])\n\n# Query about a potential edge\nresult = knowledge.query_edge(\n    node_a=\"smoking\",\n    node_b=\"lung_cancer\",\n    context={\"domain\": \"epidemiology\"}\n)\n\nprint(f\"Exists: {result.exists}\")\nprint(f\"Direction: {result.direction}\")\nprint(f\"Confidence: {result.confidence}\")\nprint(f\"Reasoning: {result.reasoning}\")\n</code></pre>"},{"location":"userguide/introduction/#using-gemini-free-tier","title":"Using Gemini (Free Tier)","text":"<pre><code># Use Google Gemini for inference\nknowledge = LLMKnowledge(models=[\"gemini/gemini-2.5-flash\"])\n</code></pre>"},{"location":"userguide/introduction/#multi-model-consensus","title":"Multi-Model Consensus","text":"<pre><code># Query multiple models for more robust answers\nknowledge = LLMKnowledge(\n    models=[\"groq/llama-3.1-8b-instant\", \"gemini/gemini-2.5-flash\"],\n    consensus_strategy=\"weighted_vote\"\n)\n</code></pre>"},{"location":"userguide/introduction/#llm-provider-setup","title":"LLM Provider Setup","text":"<p>CausalIQ Knowledge uses direct vendor-specific API clients (not wrapper libraries) to communicate with LLM providers. This approach provides reliability and minimal dependencies. Currently supported:</p> <ul> <li>Groq: Free tier with fast inference</li> <li>Google Gemini: Generous free tier</li> <li>OpenAI: GPT-4o and other models</li> <li>Anthropic: Claude models</li> <li>DeepSeek: DeepSeek-V3 and R1 models</li> <li>Mistral: Mistral AI models</li> <li>Ollama: Local LLMs (free, runs locally)</li> </ul>"},{"location":"userguide/introduction/#free-options","title":"Free Options","text":""},{"location":"userguide/introduction/#groq-free-tier-very-fast","title":"Groq (Free Tier - Very Fast)","text":"<p>Groq offers a generous free tier with extremely fast inference:</p> <ol> <li>Sign up at console.groq.com</li> <li>Create an API key</li> <li>Set the environment variable (see Storing API Keys)</li> <li>Use in code:</li> </ol> <pre><code>from causaliq_knowledge.llm import LLMKnowledge\n\nknowledge = LLMKnowledge(models=[\"groq/llama-3.1-8b-instant\"])\nresult = knowledge.query_edge(\"smoking\", \"lung_cancer\")\n</code></pre> <p>Available Groq models: <code>groq/llama-3.1-8b-instant</code>, <code>groq/llama-3.1-70b-versatile</code>, <code>groq/mixtral-8x7b-32768</code></p>"},{"location":"userguide/introduction/#google-gemini-free-tier","title":"Google Gemini (Free Tier)","text":"<p>Google offers free access to Gemini models:</p> <ol> <li>Sign up at aistudio.google.com/apikey</li> <li>Create an API key</li> <li>Set <code>GEMINI_API_KEY</code> environment variable</li> <li>Use in code:</li> </ol> <pre><code>knowledge = LLMKnowledge(models=[\"gemini/gemini-2.5-flash\"])\n</code></pre>"},{"location":"userguide/introduction/#openai","title":"OpenAI","text":"<p>OpenAI provides GPT-4o and other models:</p> <ol> <li>Sign up at platform.openai.com</li> <li>Create an API key</li> <li>Set <code>OPENAI_API_KEY</code> environment variable</li> </ol> <pre><code>knowledge = LLMKnowledge(models=[\"openai/gpt-4o-mini\"])\n</code></pre>"},{"location":"userguide/introduction/#anthropic","title":"Anthropic","text":"<p>Anthropic provides Claude models:</p> <ol> <li>Sign up at console.anthropic.com</li> <li>Create an API key</li> <li>Set <code>ANTHROPIC_API_KEY</code> environment variable</li> </ol> <pre><code>knowledge = LLMKnowledge(models=[\"anthropic/claude-sonnet-4-20250514\"])\n</code></pre>"},{"location":"userguide/introduction/#deepseek","title":"DeepSeek","text":"<p>DeepSeek offers high-quality models at competitive prices:</p> <ol> <li>Sign up at platform.deepseek.com</li> <li>Create an API key</li> <li>Set <code>DEEPSEEK_API_KEY</code> environment variable</li> </ol> <pre><code>knowledge = LLMKnowledge(models=[\"deepseek/deepseek-chat\"])\n</code></pre>"},{"location":"userguide/introduction/#mistral","title":"Mistral","text":"<p>Mistral AI provides models with EU data sovereignty:</p> <ol> <li>Sign up at console.mistral.ai</li> <li>Create an API key</li> <li>Set <code>MISTRAL_API_KEY</code> environment variable</li> </ol> <pre><code>knowledge = LLMKnowledge(models=[\"mistral/mistral-small-latest\"])\n</code></pre>"},{"location":"userguide/introduction/#ollama-local","title":"Ollama (Local)","text":"<p>Run models locally with Ollama (no API key needed):</p> <ol> <li>Install Ollama from ollama.ai</li> <li>Pull a model: <code>ollama pull llama3</code></li> <li>Use in code:</li> </ol> <pre><code>knowledge = LLMKnowledge(models=[\"ollama/llama3\"])\n</code></pre>"},{"location":"userguide/introduction/#storing-api-keys","title":"Storing API Keys","text":""},{"location":"userguide/introduction/#option-1-user-environment-variables-recommended","title":"Option 1: User Environment Variables (Recommended)","text":"<p>Set permanently for your user account on Windows:</p> <pre><code>[Environment]::SetEnvironmentVariable(\"GROQ_API_KEY\", \"your-key\", \"User\")\n[Environment]::SetEnvironmentVariable(\"GEMINI_API_KEY\", \"your-key\", \"User\")\n</code></pre> <p>On Linux/macOS, add to your <code>~/.bashrc</code> or <code>~/.zshrc</code>:</p> <pre><code>export GROQ_API_KEY=\"your-key\"\nexport GEMINI_API_KEY=\"your-key\"\n</code></pre> <p>Restart your terminal for changes to take effect.</p>"},{"location":"userguide/introduction/#option-2-project-env-file","title":"Option 2: Project <code>.env</code> File","text":"<p>Create a <code>.env</code> file in your project root:</p> <pre><code>GROQ_API_KEY=your-key-here\nGEMINI_API_KEY=your-key-here\n</code></pre> <p>Important: Add <code>.env</code> to your <code>.gitignore</code> so keys aren't committed to version control!</p>"},{"location":"userguide/introduction/#option-3-password-manager","title":"Option 3: Password Manager","text":"<p>Store API keys in a password manager (LastPass, 1Password, Bitwarden, etc.) as a Secure Note. This provides:</p> <ul> <li>Encrypted backup of your keys</li> <li>Access from any machine</li> <li>Secure sharing with team members if needed</li> </ul> <p>Copy keys from your password manager when setting environment variables.</p>"},{"location":"userguide/introduction/#verifying-your-setup","title":"Verifying Your Setup","text":"<p>Test your configuration with the CLI:</p> <pre><code># Using the installed CLI\ncqknow query smoking lung_cancer --model groq/llama-3.1-8b-instant\n\n# Or with Python module\npython -m causaliq_knowledge.cli query smoking lung_cancer --model ollama/llama3\n</code></pre>"},{"location":"userguide/introduction/#whats-next","title":"What's Next?","text":"<ul> <li>Architecture Overview - Understand how the package works</li> <li>LLM Integration Design - Detailed design documentation</li> <li>API Reference - Full API documentation</li> </ul>"}]}