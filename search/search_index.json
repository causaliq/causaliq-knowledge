{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"CausalIQ Knowledge","text":""},{"location":"#welcome","title":"Welcome","text":"<p>Welcome to the documentation for CausalIQ Knowledge which combines the traditional statistical structure learning algorithms with the contextual understanding and reasoning capabilities of Large Language Models. This integration enables more interpretable, domain-aware, and human-friendly causal discovery workflows. It is part of the CausalIQ ecosystem for intelligent causal discovery</p>"},{"location":"#overview","title":"Overview","text":"<p>This site provides detailed documentation, including: development roadmap, user guide, architectural vision, design notes, and API reference for users and contributors.</p>"},{"location":"#quickstart-installation","title":"Quickstart &amp; Installation","text":"<p>For a quickstart guide and installation instructions, see the README on GitHub.</p>"},{"location":"#documentation-contents","title":"Documentation Contents","text":"<ul> <li>Development Roadmap: roadmap of upcoming features</li> <li>User Guide: comprehensive user guide</li> <li>Architecture: overall architecture and design notes</li> <li>API Reference: complete reference for Python code</li> <li>Development Guidelines: CausalIQ guidelines for developers</li> <li>Changelog</li> <li>License</li> </ul>"},{"location":"#support-community","title":"Support &amp; Community","text":"<ul> <li>GitHub Issues: Report bugs or request features.</li> <li>GitHub Discussions: Ask questions and join the community.</li> </ul> <p>Tip: Use the navigation sidebar to explore the documentation. For the latest code and releases, visit the causaliq-knowledge GitHub repository.</p> <p>Supported Python Versions: 3.9, 3.10, 3.11, 3.12 Default Python Version: 3.11</p>"},{"location":"roadmap/","title":"CausalIQ Knowledge - Development Roadmap","text":"<p>Last updated: January 06, 2026  </p> <p>This project roadmap fits into the overall ecosystem roadmap</p>"},{"location":"roadmap/#current-release","title":"\ud83c\udfaf Current Release","text":""},{"location":"roadmap/#release-v010-foundation-llm-january-2026","title":"Release v0.1.0 - Foundation LLM [January 2026]","text":"<p>Simple LLM queries to 1 or 2 LLMs about edge existence and orientation to support graph averaging.</p> <p>Scope:</p> <ul> <li>Abstract <code>KnowledgeProvider</code> interface</li> <li><code>EdgeKnowledge</code> Pydantic model for structured responses</li> <li><code>LLMKnowledge</code> implementation using LiteLLM</li> <li>Support for OpenAI, Anthropic, Google, Groq, and Ollama (local)</li> <li>Single-model and multi-model consensus queries</li> <li>Basic prompt templates for edge existence/orientation</li> <li>CLI for testing queries</li> </ul> <p>Out of scope for v0.1.0:</p> <ul> <li>Response caching (v0.3.0)</li> <li>Rich context/RAG (v0.4.0)</li> <li>Direct algorithm integration (v0.5.0)</li> </ul> <p>Implementation milestones:</p> <ul> <li>v0.1.0a: Core models and abstract interface \u2705</li> <li>v0.1.0b: LiteLLM client wrapper \u2705</li> <li>v0.1.0c: Edge existence/orientation prompts</li> <li>v0.1.0d: Multi-LLM consensus logic</li> <li>v0.1.0e: CLI and documentation</li> </ul>"},{"location":"roadmap/#previous-releases","title":"\u2705 Previous Releases","text":"<p>See Git commit history for detailed implementation progress</p> <ul> <li>none</li> </ul>"},{"location":"roadmap/#upcoming-releases","title":"\ud83d\udee3\ufe0f Upcoming Releases","text":""},{"location":"roadmap/#release-v020-additional-llms","title":"Release v0.2.0 - Additional LLMs","text":"<ul> <li>Expanded provider configurations</li> <li>Provider-specific prompt optimizations</li> <li>Cost tracking and reporting utilities</li> <li>Budget management features</li> </ul>"},{"location":"roadmap/#release-v030-llm-caching","title":"Release v0.3.0 - LLM Caching","text":"<ul> <li>Disk-based response caching (diskcache)</li> <li>Cache key: (node_a, node_b, context_hash, model)</li> <li>Cache invalidation strategies</li> <li>Semantic similarity caching (optional)</li> </ul>"},{"location":"roadmap/#release-v040-llm-context","title":"Release v0.4.0 - LLM Context","text":"<ul> <li>Variable descriptions and roles</li> <li>Domain context specification</li> <li>Literature retrieval (RAG) using LangChain components</li> <li>ChromaDB or FAISS vector store integration</li> </ul>"},{"location":"roadmap/#release-v050-algorithm-integration","title":"Release v0.5.0 - Algorithm Integration","text":"<ul> <li>Integration hooks for structure learning algorithms</li> <li>Knowledge-guided constraint generation</li> <li>Integration with causaliq-analysis <code>average()</code> function</li> <li>Entropy-based automatic query triggering</li> </ul>"},{"location":"roadmap/#release-v060-legacy-reference","title":"Release v0.6.0 - Legacy Reference","text":"<ul> <li>Support for deriving knowledge from reference networks</li> <li>Migration of functionality from legacy discovery repo</li> </ul>"},{"location":"roadmap/#dependencies-evolution","title":"\ud83d\udce6 Dependencies Evolution","text":"<pre><code># v0.1.0\ndependencies = [\n    \"click&gt;=8.0.0\",\n    \"litellm&gt;=1.0.0\",\n    \"pydantic&gt;=2.0.0\",\n]\n\n# v0.3.0 (add)\n\"diskcache&gt;=5.0.0\"\n\n# v0.4.0 (add)\n\"langchain-core&gt;=0.2.0\"\n\"langchain-community&gt;=0.2.0\"\n\"chromadb&gt;=0.4.0\"\n</code></pre>"},{"location":"api/base/","title":"Base API Reference","text":"<p>Abstract base classes defining the knowledge provider interface.</p>"},{"location":"api/base/#knowledgeprovider","title":"KnowledgeProvider","text":""},{"location":"api/base/#causaliq_knowledge.base.KnowledgeProvider","title":"KnowledgeProvider","text":"<p>Abstract interface for all knowledge sources.</p> <p>This is the base class that all knowledge providers must implement. Knowledge providers can be LLM-based, rule-based, human-input based, or any other source of causal knowledge.</p> <p>The primary method is <code>query_edge()</code> which asks about the causal relationship between two variables.</p> Example <p>class MyKnowledgeProvider(KnowledgeProvider): ...     def query_edge(self, node_a, node_b, context=None): ...         # Implementation here ...         return EdgeKnowledge(exists=True, confidence=0.8, ...) ... provider = MyKnowledgeProvider() result = provider.query_edge(\"smoking\", \"cancer\")</p> <p>Methods:</p> <ul> <li> <code>query_edge</code>             \u2013              <p>Query whether a causal edge exists between two nodes.</p> </li> <li> <code>query_edges</code>             \u2013              <p>Query multiple edges at once.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>Return the name of this knowledge provider.</p> </li> </ul>"},{"location":"api/base/#causaliq_knowledge.base.KnowledgeProvider.name","title":"name  <code>property</code>","text":"<pre><code>name: str\n</code></pre> <p>Return the name of this knowledge provider.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>Class name by default. Subclasses may override.</p> </li> </ul>"},{"location":"api/base/#causaliq_knowledge.base.KnowledgeProvider.query_edge","title":"query_edge  <code>abstractmethod</code>","text":"<pre><code>query_edge(node_a: str, node_b: str, context: Optional[dict] = None) -&gt; EdgeKnowledge\n</code></pre> <p>Query whether a causal edge exists between two nodes.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>EdgeKnowledge</code>           \u2013            <p>EdgeKnowledge with: - exists: True, False, or None (uncertain) - direction: \"a_to_b\", \"b_to_a\", \"undirected\", or None - confidence: 0.0 to 1.0 - reasoning: Human-readable explanation - model: Source identifier (optional)</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>NotImplementedError</code>             \u2013            <p>If not implemented by subclass.</p> </li> </ul>"},{"location":"api/base/#causaliq_knowledge.base.KnowledgeProvider.query_edge(node_a)","title":"<code>node_a</code>","text":"(<code>str</code>)           \u2013            <p>Name of the first variable.</p>"},{"location":"api/base/#causaliq_knowledge.base.KnowledgeProvider.query_edge(node_b)","title":"<code>node_b</code>","text":"(<code>str</code>)           \u2013            <p>Name of the second variable.</p>"},{"location":"api/base/#causaliq_knowledge.base.KnowledgeProvider.query_edge(context)","title":"<code>context</code>","text":"(<code>Optional[dict]</code>, default:                   <code>None</code> )           \u2013            <p>Optional context dictionary that may include: - domain: The domain (e.g., \"medicine\", \"economics\") - descriptions: Dict mapping variable names to descriptions - additional_info: Any other relevant context</p>"},{"location":"api/base/#causaliq_knowledge.base.KnowledgeProvider.query_edges","title":"query_edges","text":"<pre><code>query_edges(\n    edges: list[tuple[str, str]], context: Optional[dict] = None\n) -&gt; list[EdgeKnowledge]\n</code></pre> <p>Query multiple edges at once.</p> <p>Default implementation calls query_edge for each pair. Subclasses may override for batch optimization.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>list[EdgeKnowledge]</code>           \u2013            <p>List of EdgeKnowledge results, one per edge pair.</p> </li> </ul>"},{"location":"api/base/#causaliq_knowledge.base.KnowledgeProvider.query_edges(edges)","title":"<code>edges</code>","text":"(<code>list[tuple[str, str]]</code>)           \u2013            <p>List of (node_a, node_b) tuples to query.</p>"},{"location":"api/base/#causaliq_knowledge.base.KnowledgeProvider.query_edges(context)","title":"<code>context</code>","text":"(<code>Optional[dict]</code>, default:                   <code>None</code> )           \u2013            <p>Optional context dictionary (shared across all queries).</p>"},{"location":"api/cli/","title":"CausalIQ Knowledge CLI","text":""},{"location":"api/cli/#cli-entry-point","title":"CLI entry point","text":"<p>This is the entry point for the CLI logic.</p>"},{"location":"api/cli/#causaliq_knowledge.cli","title":"cli","text":"<p>Command-line interface for causaliq-knowledge.</p> <p>Functions:</p> <ul> <li> <code>cli</code>             \u2013              <p>Simple CLI example.</p> </li> <li> <code>main</code>             \u2013              <p>Entry point for the CLI.</p> </li> </ul>"},{"location":"api/cli/#causaliq_knowledge.cli.cli","title":"cli","text":"<pre><code>cli(name: str, greet: str) -&gt; None\n</code></pre> <p>Simple CLI example.</p> <p>NAME is the person to greet</p>"},{"location":"api/cli/#causaliq_knowledge.cli.main","title":"main","text":"<pre><code>main() -&gt; None\n</code></pre> <p>Entry point for the CLI.</p>"},{"location":"api/llm/","title":"LLM Client API Reference","text":"<p>LiteLLM wrapper for making LLM calls with cost tracking and configuration.</p>"},{"location":"api/llm/#llmconfig","title":"LLMConfig","text":""},{"location":"api/llm/#causaliq_knowledge.llm.client.LLMConfig","title":"LLMConfig  <code>dataclass</code>","text":"<pre><code>LLMConfig(\n    model: str = \"gpt-4o-mini\",\n    temperature: float = 0.0,\n    max_tokens: int = 1024,\n    timeout: float = 30.0,\n    max_retries: int = 3,\n    api_base: Optional[str] = None,\n)\n</code></pre> <p>Configuration for an LLM client.</p> <p>Attributes:</p> <ul> <li> <code>model</code>               (<code>str</code>)           \u2013            <p>LiteLLM model identifier (e.g., \"gpt-4o-mini\", \"ollama/llama3\")</p> </li> <li> <code>temperature</code>               (<code>float</code>)           \u2013            <p>Sampling temperature (0.0 = deterministic)</p> </li> <li> <code>max_tokens</code>               (<code>int</code>)           \u2013            <p>Maximum tokens in response</p> </li> <li> <code>timeout</code>               (<code>float</code>)           \u2013            <p>Request timeout in seconds</p> </li> <li> <code>max_retries</code>               (<code>int</code>)           \u2013            <p>Number of retries on failure</p> </li> <li> <code>api_base</code>               (<code>Optional[str]</code>)           \u2013            <p>Optional custom API base URL (for local models)</p> </li> </ul>"},{"location":"api/llm/#causaliq_knowledge.llm.client.LLMConfig.api_base","title":"api_base  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>api_base: Optional[str] = None\n</code></pre>"},{"location":"api/llm/#causaliq_knowledge.llm.client.LLMConfig.max_retries","title":"max_retries  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_retries: int = 3\n</code></pre>"},{"location":"api/llm/#causaliq_knowledge.llm.client.LLMConfig.max_tokens","title":"max_tokens  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_tokens: int = 1024\n</code></pre>"},{"location":"api/llm/#causaliq_knowledge.llm.client.LLMConfig.model","title":"model  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model: str = 'gpt-4o-mini'\n</code></pre>"},{"location":"api/llm/#causaliq_knowledge.llm.client.LLMConfig.temperature","title":"temperature  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>temperature: float = 0.0\n</code></pre>"},{"location":"api/llm/#causaliq_knowledge.llm.client.LLMConfig.timeout","title":"timeout  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>timeout: float = 30.0\n</code></pre>"},{"location":"api/llm/#llmresponse","title":"LLMResponse","text":""},{"location":"api/llm/#causaliq_knowledge.llm.client.LLMResponse","title":"LLMResponse  <code>dataclass</code>","text":"<pre><code>LLMResponse(\n    content: str,\n    model: str,\n    input_tokens: int = 0,\n    output_tokens: int = 0,\n    cost: float = 0.0,\n    raw_response: Optional[Any] = None,\n)\n</code></pre> <p>Response from an LLM call.</p> <p>Attributes:</p> <ul> <li> <code>content</code>               (<code>str</code>)           \u2013            <p>The text content of the response</p> </li> <li> <code>model</code>               (<code>str</code>)           \u2013            <p>The model that generated the response</p> </li> <li> <code>input_tokens</code>               (<code>int</code>)           \u2013            <p>Number of input tokens used</p> </li> <li> <code>output_tokens</code>               (<code>int</code>)           \u2013            <p>Number of output tokens generated</p> </li> <li> <code>cost</code>               (<code>float</code>)           \u2013            <p>Estimated cost in USD (if available)</p> </li> <li> <code>raw_response</code>               (<code>Optional[Any]</code>)           \u2013            <p>The raw response object from LiteLLM</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>parse_json</code>             \u2013              <p>Parse content as JSON.</p> </li> </ul>"},{"location":"api/llm/#causaliq_knowledge.llm.client.LLMResponse.content","title":"content  <code>instance-attribute</code>","text":"<pre><code>content: str\n</code></pre>"},{"location":"api/llm/#causaliq_knowledge.llm.client.LLMResponse.cost","title":"cost  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>cost: float = 0.0\n</code></pre>"},{"location":"api/llm/#causaliq_knowledge.llm.client.LLMResponse.input_tokens","title":"input_tokens  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>input_tokens: int = 0\n</code></pre>"},{"location":"api/llm/#causaliq_knowledge.llm.client.LLMResponse.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model: str\n</code></pre>"},{"location":"api/llm/#causaliq_knowledge.llm.client.LLMResponse.output_tokens","title":"output_tokens  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>output_tokens: int = 0\n</code></pre>"},{"location":"api/llm/#causaliq_knowledge.llm.client.LLMResponse.raw_response","title":"raw_response  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>raw_response: Optional[Any] = field(default=None, repr=False)\n</code></pre>"},{"location":"api/llm/#causaliq_knowledge.llm.client.LLMResponse.parse_json","title":"parse_json","text":"<pre><code>parse_json() -&gt; Optional[dict]\n</code></pre> <p>Parse content as JSON.</p> <p>Returns:</p> <ul> <li> <code>Optional[dict]</code>           \u2013            <p>Parsed JSON dict, or None if parsing fails.</p> </li> </ul>"},{"location":"api/llm/#llmclient","title":"LLMClient","text":""},{"location":"api/llm/#causaliq_knowledge.llm.client.LLMClient","title":"LLMClient","text":"<pre><code>LLMClient(config: Optional[LLMConfig] = None)\n</code></pre> <p>Client for making LLM calls via LiteLLM.</p> <p>This class wraps LiteLLM to provide: - Consistent configuration across calls - Cost tracking and token counting - JSON response parsing - Error handling with retries</p> Example <p>client = LLMClient(LLMConfig(model=\"gpt-4o-mini\")) response = client.complete( ...     system=\"You are a helpful assistant.\", ...     user=\"What is 2+2?\" ... ) print(response.content)</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>complete</code>             \u2013              <p>Make a completion request to the LLM.</p> </li> <li> <code>complete_json</code>             \u2013              <p>Make a completion request and parse response as JSON.</p> </li> <li> <code>get_stats</code>             \u2013              <p>Get current usage statistics.</p> </li> <li> <code>reset_stats</code>             \u2013              <p>Reset cost and token tracking statistics.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>_call_count</code>               (<code>int</code>)           \u2013            </li> <li> <code>_total_cost</code>               (<code>float</code>)           \u2013            </li> <li> <code>_total_input_tokens</code>               (<code>int</code>)           \u2013            </li> <li> <code>_total_output_tokens</code>               (<code>int</code>)           \u2013            </li> <li> <code>call_count</code>               (<code>int</code>)           \u2013            <p>Number of successful calls made.</p> </li> <li> <code>config</code>           \u2013            </li> <li> <code>total_cost</code>               (<code>float</code>)           \u2013            <p>Total cost in USD across all calls.</p> </li> <li> <code>total_input_tokens</code>               (<code>int</code>)           \u2013            <p>Total input tokens across all calls.</p> </li> <li> <code>total_output_tokens</code>               (<code>int</code>)           \u2013            <p>Total output tokens across all calls.</p> </li> </ul>"},{"location":"api/llm/#causaliq_knowledge.llm.client.LLMClient(config)","title":"<code>config</code>","text":"(<code>Optional[LLMConfig]</code>, default:                   <code>None</code> )           \u2013            <p>LLM configuration. Defaults to gpt-4o-mini.</p>"},{"location":"api/llm/#causaliq_knowledge.llm.client.LLMClient._call_count","title":"_call_count  <code>instance-attribute</code>","text":"<pre><code>_call_count: int = 0\n</code></pre>"},{"location":"api/llm/#causaliq_knowledge.llm.client.LLMClient._total_cost","title":"_total_cost  <code>instance-attribute</code>","text":"<pre><code>_total_cost: float = 0.0\n</code></pre>"},{"location":"api/llm/#causaliq_knowledge.llm.client.LLMClient._total_input_tokens","title":"_total_input_tokens  <code>instance-attribute</code>","text":"<pre><code>_total_input_tokens: int = 0\n</code></pre>"},{"location":"api/llm/#causaliq_knowledge.llm.client.LLMClient._total_output_tokens","title":"_total_output_tokens  <code>instance-attribute</code>","text":"<pre><code>_total_output_tokens: int = 0\n</code></pre>"},{"location":"api/llm/#causaliq_knowledge.llm.client.LLMClient.call_count","title":"call_count  <code>property</code>","text":"<pre><code>call_count: int\n</code></pre> <p>Number of successful calls made.</p>"},{"location":"api/llm/#causaliq_knowledge.llm.client.LLMClient.config","title":"config  <code>instance-attribute</code>","text":"<pre><code>config = config or LLMConfig()\n</code></pre>"},{"location":"api/llm/#causaliq_knowledge.llm.client.LLMClient.total_cost","title":"total_cost  <code>property</code>","text":"<pre><code>total_cost: float\n</code></pre> <p>Total cost in USD across all calls.</p>"},{"location":"api/llm/#causaliq_knowledge.llm.client.LLMClient.total_input_tokens","title":"total_input_tokens  <code>property</code>","text":"<pre><code>total_input_tokens: int\n</code></pre> <p>Total input tokens across all calls.</p>"},{"location":"api/llm/#causaliq_knowledge.llm.client.LLMClient.total_output_tokens","title":"total_output_tokens  <code>property</code>","text":"<pre><code>total_output_tokens: int\n</code></pre> <p>Total output tokens across all calls.</p>"},{"location":"api/llm/#causaliq_knowledge.llm.client.LLMClient.complete","title":"complete","text":"<pre><code>complete(\n    user: str,\n    system: Optional[str] = None,\n    response_format: Optional[type[BaseModel]] = None,\n) -&gt; LLMResponse\n</code></pre> <p>Make a completion request to the LLM.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>LLMResponse</code>           \u2013            <p>LLMResponse with content and usage statistics.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>APIError</code>             \u2013            <p>On API failures after retries.</p> </li> </ul>"},{"location":"api/llm/#causaliq_knowledge.llm.client.LLMClient.complete(user)","title":"<code>user</code>","text":"(<code>str</code>)           \u2013            <p>The user message/prompt.</p>"},{"location":"api/llm/#causaliq_knowledge.llm.client.LLMClient.complete(system)","title":"<code>system</code>","text":"(<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional system message.</p>"},{"location":"api/llm/#causaliq_knowledge.llm.client.LLMClient.complete(response_format)","title":"<code>response_format</code>","text":"(<code>Optional[type[BaseModel]]</code>, default:                   <code>None</code> )           \u2013            <p>Optional Pydantic model for structured output.</p>"},{"location":"api/llm/#causaliq_knowledge.llm.client.LLMClient.complete_json","title":"complete_json","text":"<pre><code>complete_json(\n    user: str, system: Optional[str] = None\n) -&gt; tuple[Optional[dict], LLMResponse]\n</code></pre> <p>Make a completion request and parse response as JSON.</p> <p>Convenience method that calls complete() and parses the result.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>Optional[dict]</code>           \u2013            <p>Tuple of (parsed_json, response). parsed_json may be None</p> </li> <li> <code>LLMResponse</code>           \u2013            <p>if parsing fails.</p> </li> </ul>"},{"location":"api/llm/#causaliq_knowledge.llm.client.LLMClient.complete_json(user)","title":"<code>user</code>","text":"(<code>str</code>)           \u2013            <p>The user message/prompt.</p>"},{"location":"api/llm/#causaliq_knowledge.llm.client.LLMClient.complete_json(system)","title":"<code>system</code>","text":"(<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional system message.</p>"},{"location":"api/llm/#causaliq_knowledge.llm.client.LLMClient.get_stats","title":"get_stats","text":"<pre><code>get_stats() -&gt; dict\n</code></pre> <p>Get current usage statistics.</p> <p>Returns:</p> <ul> <li> <code>dict</code>           \u2013            <p>Dictionary with cost, token, and call count statistics.</p> </li> </ul>"},{"location":"api/llm/#causaliq_knowledge.llm.client.LLMClient.reset_stats","title":"reset_stats","text":"<pre><code>reset_stats() -&gt; None\n</code></pre> <p>Reset cost and token tracking statistics.</p>"},{"location":"api/models/","title":"Models API Reference","text":"<p>Core Pydantic models for representing causal knowledge.</p>"},{"location":"api/models/#edgedirection","title":"EdgeDirection","text":""},{"location":"api/models/#causaliq_knowledge.models.EdgeDirection","title":"EdgeDirection","text":"<p>Direction of a causal edge between two nodes.</p> <p>Attributes:</p> <ul> <li> <code>A_TO_B</code>           \u2013            </li> <li> <code>B_TO_A</code>           \u2013            </li> <li> <code>UNDIRECTED</code>           \u2013            </li> </ul>"},{"location":"api/models/#causaliq_knowledge.models.EdgeDirection.A_TO_B","title":"A_TO_B  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>A_TO_B = 'a_to_b'\n</code></pre>"},{"location":"api/models/#causaliq_knowledge.models.EdgeDirection.B_TO_A","title":"B_TO_A  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>B_TO_A = 'b_to_a'\n</code></pre>"},{"location":"api/models/#causaliq_knowledge.models.EdgeDirection.UNDIRECTED","title":"UNDIRECTED  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>UNDIRECTED = 'undirected'\n</code></pre>"},{"location":"api/models/#edgeknowledge","title":"EdgeKnowledge","text":""},{"location":"api/models/#causaliq_knowledge.models.EdgeKnowledge","title":"EdgeKnowledge","text":"<p>Structured knowledge about a potential causal edge.</p> <p>This model represents the result of querying a knowledge source about whether a causal relationship exists between two variables.</p> <p>Attributes:</p> <ul> <li> <code>exists</code>               (<code>Optional[bool]</code>)           \u2013            <p>Whether a causal edge exists. True, False, or None (uncertain).</p> </li> <li> <code>direction</code>               (<code>Optional[EdgeDirection]</code>)           \u2013            <p>The direction of the causal relationship if it exists. \"a_to_b\" means node_a causes node_b, \"b_to_a\" means the reverse, \"undirected\" means bidirectional or direction unknown.</p> </li> <li> <code>confidence</code>               (<code>float</code>)           \u2013            <p>Confidence score from 0.0 (no confidence) to 1.0 (certain).</p> </li> <li> <code>reasoning</code>               (<code>str</code>)           \u2013            <p>Human-readable explanation for the knowledge assessment.</p> </li> <li> <code>model</code>               (<code>Optional[str]</code>)           \u2013            <p>The LLM or knowledge source that provided this response.</p> </li> </ul> Example <p>knowledge = EdgeKnowledge( ...     exists=True, ...     direction=\"a_to_b\", ...     confidence=0.85, ...     reasoning=\"Smoking causes lung cancer.\", ...     model=\"gpt-4o-mini\" ... )</p> <p>Methods:</p> <ul> <li> <code>is_uncertain</code>             \u2013              <p>Check if this knowledge is uncertain.</p> </li> <li> <code>to_dict</code>             \u2013              <p>Convert to dictionary with string direction.</p> </li> <li> <code>uncertain</code>             \u2013              <p>Create an uncertain EdgeKnowledge instance.</p> </li> <li> <code>validate_direction</code>             \u2013              <p>Convert string direction to EdgeDirection enum.</p> </li> </ul>"},{"location":"api/models/#causaliq_knowledge.models.EdgeKnowledge.confidence","title":"confidence  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>confidence: float = Field(\n    default=0.0, ge=0.0, le=1.0, description=\"Confidence score from 0.0 to 1.0.\"\n)\n</code></pre>"},{"location":"api/models/#causaliq_knowledge.models.EdgeKnowledge.direction","title":"direction  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>direction: Optional[EdgeDirection] = Field(\n    default=None, description=\"Direction of the causal relationship if it exists.\"\n)\n</code></pre>"},{"location":"api/models/#causaliq_knowledge.models.EdgeKnowledge.exists","title":"exists  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>exists: Optional[bool] = Field(\n    default=None, description=\"Whether a causal edge exists. None = uncertain.\"\n)\n</code></pre>"},{"location":"api/models/#causaliq_knowledge.models.EdgeKnowledge.model","title":"model  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model: Optional[str] = Field(\n    default=None, description=\"The model/source that provided this knowledge.\"\n)\n</code></pre>"},{"location":"api/models/#causaliq_knowledge.models.EdgeKnowledge.reasoning","title":"reasoning  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>reasoning: str = Field(\n    default=\"\", description=\"Human-readable explanation for the assessment.\"\n)\n</code></pre>"},{"location":"api/models/#causaliq_knowledge.models.EdgeKnowledge.is_uncertain","title":"is_uncertain","text":"<pre><code>is_uncertain() -&gt; bool\n</code></pre> <p>Check if this knowledge is uncertain.</p> <p>Returns:</p> <ul> <li> <code>bool</code>           \u2013            <p>True if exists is None or confidence is below 0.5.</p> </li> </ul>"},{"location":"api/models/#causaliq_knowledge.models.EdgeKnowledge.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; dict\n</code></pre> <p>Convert to dictionary with string direction.</p> <p>Returns:</p> <ul> <li> <code>dict</code>           \u2013            <p>Dictionary representation suitable for JSON serialization.</p> </li> </ul>"},{"location":"api/models/#causaliq_knowledge.models.EdgeKnowledge.uncertain","title":"uncertain  <code>classmethod</code>","text":"<pre><code>uncertain(\n    reasoning: str = \"Unable to determine\", model: Optional[str] = None\n) -&gt; EdgeKnowledge\n</code></pre> <p>Create an uncertain EdgeKnowledge instance.</p> <p>Useful for error cases or when knowledge source cannot provide an answer.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>EdgeKnowledge</code>           \u2013            <p>EdgeKnowledge with exists=None and confidence=0.0.</p> </li> </ul>"},{"location":"api/models/#causaliq_knowledge.models.EdgeKnowledge.uncertain(reasoning)","title":"<code>reasoning</code>","text":"(<code>str</code>, default:                   <code>'Unable to determine'</code> )           \u2013            <p>Explanation for why the result is uncertain.</p>"},{"location":"api/models/#causaliq_knowledge.models.EdgeKnowledge.uncertain(model)","title":"<code>model</code>","text":"(<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>The model/source that was queried.</p>"},{"location":"api/models/#causaliq_knowledge.models.EdgeKnowledge.validate_direction","title":"validate_direction  <code>classmethod</code>","text":"<pre><code>validate_direction(v: Optional[str]) -&gt; Optional[EdgeDirection]\n</code></pre> <p>Convert string direction to EdgeDirection enum.</p>"},{"location":"api/overview/","title":"CausalIQ Knowledge API Reference","text":"<p>API documentation for causaliq-knowledge, organized by module.</p>"},{"location":"api/overview/#modules","title":"Modules","text":""},{"location":"api/overview/#models","title":"Models","text":"<p>Core Pydantic models for representing causal knowledge:</p> <ul> <li>EdgeDirection - Enum for causal edge direction (a_to_b, b_to_a, undirected)</li> <li>EdgeKnowledge - Structured knowledge about a potential causal edge</li> </ul>"},{"location":"api/overview/#base","title":"Base","text":"<p>Abstract interfaces for knowledge providers:</p> <ul> <li>KnowledgeProvider - Abstract base class that all knowledge sources implement</li> </ul>"},{"location":"api/overview/#llm-client","title":"LLM Client","text":"<p>LiteLLM wrapper for making LLM calls:</p> <ul> <li>LLMConfig - Configuration for LLM client (model, temperature, etc.)</li> <li>LLMResponse - Response from an LLM call with content and usage stats</li> <li>LLMClient - Client for making LLM calls with cost tracking</li> </ul>"},{"location":"api/overview/#cli","title":"CLI","text":"<p>Command-line interface for testing and querying.</p>"},{"location":"architecture/llm_integration/","title":"LLM Integration Design Note","text":""},{"location":"architecture/llm_integration/#overview","title":"Overview","text":"<p>This document describes how causaliq-knowledge integrates with Large Language Models (LLMs) to provide knowledge about causal relationships. The primary use case for v0.1.0 is answering queries about edge existence and edge orientation to support graph averaging in causaliq-analysis.</p>"},{"location":"architecture/llm_integration/#how-it-works","title":"How it works","text":""},{"location":"architecture/llm_integration/#query-flow","title":"Query Flow","text":"<ol> <li>Consumer requests knowledge about a potential edge (e.g., \"Does smoking cause cancer?\")</li> <li>KnowledgeProvider receives the query with optional context</li> <li>LLM client formats the query using structured prompts</li> <li>One or more LLMs are queried (configurable)</li> <li>Responses are parsed into structured <code>EdgeKnowledge</code> objects</li> <li>Multi-LLM consensus combines responses (if multiple models used)</li> <li>Result returned with confidence score and reasoning</li> </ol>"},{"location":"architecture/llm_integration/#core-interface","title":"Core Interface","text":"<pre><code>from abc import ABC, abstractmethod\nfrom pydantic import BaseModel\n\nclass EdgeKnowledge(BaseModel):\n    \"\"\"Structured knowledge about a potential causal edge.\"\"\"\n    exists: bool | None           # True, False, or None (uncertain)\n    direction: str | None         # \"a_to_b\", \"b_to_a\", \"undirected\", None\n    confidence: float             # 0.0 to 1.0\n    reasoning: str                # Human-readable explanation\n    model: str | None = None      # Which LLM provided this (for logging)\n\nclass KnowledgeProvider(ABC):\n    \"\"\"Abstract interface for all knowledge sources.\"\"\"\n\n    @abstractmethod\n    def query_edge(\n        self,\n        node_a: str,\n        node_b: str,\n        context: dict | None = None\n    ) -&gt; EdgeKnowledge:\n        \"\"\"\n        Query whether a causal edge exists between two nodes.\n\n        Args:\n            node_a: Name of first variable\n            node_b: Name of second variable  \n            context: Optional context (domain, variable descriptions, etc.)\n\n        Returns:\n            EdgeKnowledge with existence, direction, confidence, reasoning\n        \"\"\"\n        pass\n</code></pre>"},{"location":"architecture/llm_integration/#llm-implementation","title":"LLM Implementation","text":"<pre><code>class LLMKnowledge(KnowledgeProvider):\n    \"\"\"LLM-based knowledge provider using LiteLLM.\"\"\"\n\n    def __init__(\n        self,\n        models: list[str] = [\"gpt-4o-mini\"],\n        consensus_strategy: str = \"weighted_vote\",\n        cache_dir: str | None = None,\n    ):\n        \"\"\"\n        Initialize LLM knowledge provider.\n\n        Args:\n            models: List of LiteLLM model identifiers\n                   e.g., [\"gpt-4o-mini\", \"ollama/llama3\", \"claude-3-haiku\"]\n            consensus_strategy: How to combine multi-model responses\n                               \"weighted_vote\" or \"highest_confidence\"\n            cache_dir: Directory for caching responses (None = no cache)\n        \"\"\"\n        ...\n</code></pre>"},{"location":"architecture/llm_integration/#llm-provider-configuration","title":"LLM Provider Configuration","text":""},{"location":"architecture/llm_integration/#supported-providers-via-litellm","title":"Supported Providers (via LiteLLM)","text":"Provider Model Examples API Key Environment Variable OpenAI <code>gpt-4o</code>, <code>gpt-4o-mini</code> <code>OPENAI_API_KEY</code> Anthropic <code>claude-3-sonnet</code>, <code>claude-3-haiku</code> <code>ANTHROPIC_API_KEY</code> Google <code>gemini-pro</code>, <code>gemini-flash</code> <code>GEMINI_API_KEY</code> Groq <code>groq/llama3-70b</code> <code>GROQ_API_KEY</code> Ollama (local) <code>ollama/llama3</code>, <code>ollama/mistral</code> None (local) Together AI <code>together_ai/mistral-7b</code> <code>TOGETHER_API_KEY</code>"},{"location":"architecture/llm_integration/#cost-considerations","title":"Cost Considerations","text":"<p>For edge queries (~500 tokens each):</p> Model Cost per 1000 queries Quality Speed GPT-4o-mini ~$0.08 Good Fast Claude 3 Haiku ~$0.13 Good Fast Gemini Flash Free tier Good Fast Groq Llama3 Free tier Good Very fast Ollama (local) $0 Varies Depends on hardware <p>Recommendation: Start with Groq free tier or local Ollama for development, use GPT-4o-mini for production quality.</p>"},{"location":"architecture/llm_integration/#prompt-design","title":"Prompt Design","text":""},{"location":"architecture/llm_integration/#edge-existence-query","title":"Edge Existence Query","text":"<pre><code>System: You are an expert in causal reasoning and domain knowledge. \nYour task is to assess whether a causal relationship exists between two variables.\nRespond in JSON format with: exists (true/false/null), direction (a_to_b/b_to_a/undirected/null), confidence (0-1), reasoning (string).\n\nUser: In the domain of {domain}, does a causal relationship exist between \"{node_a}\" and \"{node_b}\"?\nConsider:\n- Direct causation (A causes B)\n- Reverse causation (B causes A)  \n- Bidirectional/feedback relationships\n- No causal relationship (correlation only or independence)\n\nVariable context:\n{variable_descriptions}\n</code></pre>"},{"location":"architecture/llm_integration/#response-format","title":"Response Format","text":"<pre><code>{\n  \"exists\": true,\n  \"direction\": \"a_to_b\",\n  \"confidence\": 0.85,\n  \"reasoning\": \"Smoking is an established cause of lung cancer through well-documented biological mechanisms including DNA damage from carcinogens in tobacco smoke.\"\n}\n</code></pre>"},{"location":"architecture/llm_integration/#multi-llm-consensus","title":"Multi-LLM Consensus","text":"<p>When multiple models are configured, responses are combined:</p>"},{"location":"architecture/llm_integration/#weighted-vote-strategy-default","title":"Weighted Vote Strategy (default)","text":"<pre><code>def weighted_vote(responses: list[EdgeKnowledge]) -&gt; EdgeKnowledge:\n    \"\"\"Combine responses weighted by confidence.\"\"\"\n    # For existence: weighted majority vote\n    # For direction: weighted majority among those agreeing on existence\n    # Final confidence: average confidence of agreeing models\n    # Reasoning: concatenate key points from each model\n</code></pre>"},{"location":"architecture/llm_integration/#highest-confidence-strategy","title":"Highest Confidence Strategy","text":"<pre><code>def highest_confidence(responses: list[EdgeKnowledge]) -&gt; EdgeKnowledge:\n    \"\"\"Return response with highest confidence.\"\"\"\n    return max(responses, key=lambda r: r.confidence)\n</code></pre>"},{"location":"architecture/llm_integration/#integration-with-graph-averaging","title":"Integration with Graph Averaging","text":"<p>The primary consumer is <code>causaliq_analysis.graph.average()</code>:</p> <pre><code># Current output from average()\ndf = average(traces, sample_size=1000)\n# Returns: node_a, node_b, p_a_to_b, p_b_to_a, p_undirected, p_no_edge\n\n# Entropy calculation identifies uncertain edges\ndef edge_entropy(row):\n    probs = [row.p_a_to_b, row.p_b_to_a, row.p_undirected, row.p_no_edge]\n    probs = [p for p in probs if p &gt; 0]\n    return -sum(p * math.log2(p) for p in probs)\n\ndf[\"entropy\"] = df.apply(edge_entropy, axis=1)\nuncertain_edges = df[df[\"entropy\"] &gt; 1.5]  # High uncertainty\n\n# Query LLM for uncertain edges\nknowledge = LLMKnowledge(models=[\"gpt-4o-mini\"])\nfor _, row in uncertain_edges.iterrows():\n    result = knowledge.query_edge(row.node_a, row.node_b)\n    # Combine statistical and LLM probabilities...\n</code></pre>"},{"location":"architecture/llm_integration/#design-rationale","title":"Design Rationale","text":""},{"location":"architecture/llm_integration/#why-litellm","title":"Why LiteLLM?","text":"<ol> <li>Unified API: Single interface for 100+ providers</li> <li>Built-in cost tracking: Essential for budget-conscious research</li> <li>Lightweight: ~5MB vs ~100MB+ for LangChain</li> <li>Simple: No complex abstractions for straightforward queries</li> </ol>"},{"location":"architecture/llm_integration/#why-structured-json-responses","title":"Why structured JSON responses?","text":"<ol> <li>Reliable parsing: Avoids regex/heuristic extraction</li> <li>Validation: Pydantic ensures response integrity</li> <li>Consistency: Same structure regardless of model</li> </ol>"},{"location":"architecture/llm_integration/#why-multi-model-consensus","title":"Why multi-model consensus?","text":"<ol> <li>Reduced hallucination: Multiple models catch individual errors</li> <li>Confidence calibration: Agreement increases confidence</li> <li>Robustness: Not dependent on single provider availability</li> </ol>"},{"location":"architecture/llm_integration/#error-handling-and-resilience","title":"Error Handling and Resilience","text":""},{"location":"architecture/llm_integration/#api-failures","title":"API Failures","text":"<ul> <li>Automatic retry with exponential backoff (LiteLLM built-in)</li> <li>Fallback to next model in list if primary fails</li> <li>Return <code>EdgeKnowledge(exists=None, confidence=0.0)</code> if all fail</li> </ul>"},{"location":"architecture/llm_integration/#invalid-responses","title":"Invalid Responses","text":"<ul> <li>Pydantic validation catches malformed JSON</li> <li>Default to <code>exists=None</code> if parsing fails</li> <li>Log warnings for debugging</li> </ul>"},{"location":"architecture/llm_integration/#rate-limiting","title":"Rate Limiting","text":"<ul> <li>LiteLLM handles rate limit errors with automatic retry</li> <li>Configure <code>max_retries</code> and <code>timeout</code> per model</li> </ul>"},{"location":"architecture/llm_integration/#performance","title":"Performance","text":""},{"location":"architecture/llm_integration/#latency","title":"Latency","text":"<ul> <li>Single query: 0.5-2s depending on model/provider</li> <li>Batch queries: Can parallelize across edges (async)</li> <li>Cached queries: &lt;10ms</li> </ul>"},{"location":"architecture/llm_integration/#throughput-v030-with-caching","title":"Throughput (v0.3.0 with caching)","text":"<ul> <li>First query to new edge: 1-2s</li> <li>Cached query: &lt;10ms</li> <li>1000 unique edges: ~20-30 minutes (sequential), ~5 min (parallel)</li> </ul>"},{"location":"architecture/llm_integration/#future-extensions","title":"Future Extensions","text":""},{"location":"architecture/llm_integration/#v030-caching","title":"v0.3.0: Caching","text":"<ul> <li>Disk-based cache keyed by (node_a, node_b, context_hash)</li> <li>Semantic similarity cache for similar variable names</li> </ul>"},{"location":"architecture/llm_integration/#v040-rich-context","title":"v0.4.0: Rich Context","text":"<ul> <li>Variable descriptions and roles</li> <li>Domain-specific literature retrieval (RAG)</li> <li>Conversation history for follow-up queries</li> </ul>"},{"location":"architecture/llm_integration/#v050-algorithm-integration","title":"v0.5.0: Algorithm Integration","text":"<ul> <li>Direct integration with structure learning search</li> <li>Knowledge-guided constraint generation</li> </ul>"},{"location":"architecture/overview/","title":"Architecture Vision for causaliq-knowledge","text":""},{"location":"architecture/overview/#causaliq-ecosystem","title":"CausalIQ Ecosystem","text":"<p>causaliq-knowledge is a component of the overall CausalIQ ecosystem architecture.</p> <p>This package provides knowledge services to other CausalIQ packages, enabling them to incorporate LLM-derived and human-specified knowledge into causal discovery and inference workflows.</p>"},{"location":"architecture/overview/#architectural-principles","title":"Architectural Principles","text":""},{"location":"architecture/overview/#simplicity-first","title":"Simplicity First","text":"<ul> <li>Use lightweight libraries over heavy frameworks</li> <li>Start with minimal viable features, extend incrementally</li> <li>Prefer explicit code over framework \"magic\"</li> </ul>"},{"location":"architecture/overview/#cost-efficiency","title":"Cost Efficiency","text":"<ul> <li>Built-in cost tracking and budget management (critical for independent research)</li> <li>Caching of LLM queries and responses to avoid redundant API calls</li> <li>Support for cheap/free local models alongside cloud providers</li> </ul>"},{"location":"architecture/overview/#transparency-and-reproducibility","title":"Transparency and Reproducibility","text":"<ul> <li>Cache all LLM interactions for experiment reproducibility</li> <li>Provide reasoning/explanations with all knowledge outputs</li> <li>Log confidence levels to enable uncertainty-aware decisions</li> </ul>"},{"location":"architecture/overview/#clean-interfaces","title":"Clean Interfaces","text":"<ul> <li>Abstract <code>KnowledgeProvider</code> interface allows multiple implementations</li> <li>LLM-based, rule-based, and human-input knowledge sources use same interface</li> <li>Easy integration with causaliq-analysis and causaliq-discovery</li> </ul>"},{"location":"architecture/overview/#architecture-components","title":"Architecture Components","text":""},{"location":"architecture/overview/#core-components-v010","title":"Core Components (v0.1.0)","text":"<pre><code>causaliq_knowledge/\n\u251c\u2500\u2500 __init__.py              # Package exports\n\u251c\u2500\u2500 cli.py                   # Command-line interface\n\u251c\u2500\u2500 base.py                  # Abstract KnowledgeProvider interface\n\u251c\u2500\u2500 models.py                # Pydantic models (EdgeKnowledge, etc.)\n\u251c\u2500\u2500 llm/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 client.py            # LiteLLM wrapper with configuration\n\u2502   \u251c\u2500\u2500 prompts.py           # Prompt templates for edge queries\n\u2502   \u2514\u2500\u2500 providers.py         # LLMKnowledge implementation\n\u2514\u2500\u2500 utils/\n    \u2514\u2500\u2500 __init__.py\n</code></pre>"},{"location":"architecture/overview/#data-flow","title":"Data Flow","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                 Consuming Package (e.g., causaliq-analysis)     \u2502\n\u2502                                                                 \u2502\n\u2502   uncertain_edges = df[df[\"entropy\"] &gt; threshold]               \u2502\n\u2502                          \u2502                                      \u2502\n\u2502                          \u25bc                                      \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502   \u2502              causaliq-knowledge                          \u2502   \u2502\n\u2502   \u2502                                                          \u2502   \u2502\n\u2502   \u2502   knowledge.query_edge(\"smoking\", \"cancer\")              \u2502   \u2502\n\u2502   \u2502       \u2502                                                  \u2502   \u2502\n\u2502   \u2502       \u25bc                                                  \u2502   \u2502\n\u2502   \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u2502   \u2502\n\u2502   \u2502   \u2502  LLM 1    \u2502    \u2502  LLM 2    \u2502    \u2502  Cache    \u2502       \u2502   \u2502\n\u2502   \u2502   \u2502 (GPT-4o)  \u2502    \u2502 (Llama3)  \u2502    \u2502 (disk)    \u2502       \u2502   \u2502\n\u2502   \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       \u2502   \u2502\n\u2502   \u2502       \u2502                 \u2502                \u2502               \u2502   \u2502\n\u2502   \u2502       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518               \u2502   \u2502\n\u2502   \u2502                         \u2502                                \u2502   \u2502\n\u2502   \u2502                         \u25bc                                \u2502   \u2502\n\u2502   \u2502               EdgeKnowledge(                             \u2502   \u2502\n\u2502   \u2502                   exists=True,                           \u2502   \u2502\n\u2502   \u2502                   direction=\"a_to_b\",                    \u2502   \u2502\n\u2502   \u2502                   confidence=0.85,                       \u2502   \u2502\n\u2502   \u2502                   reasoning=\"Established medical...\"     \u2502   \u2502\n\u2502   \u2502               )                                          \u2502   \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2502                          \u2502                                      \u2502\n\u2502                          \u25bc                                      \u2502\n\u2502   Combine with statistical probabilities                        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture/overview/#technology-choices","title":"Technology Choices","text":""},{"location":"architecture/overview/#litellm-over-langchain-for-v010-v030","title":"LiteLLM over LangChain (for v0.1.0 - v0.3.0)","text":"Requirement LiteLLM LangChain Multi-provider unified API \u2705 100+ providers \u2705 Many providers Built-in cost tracking \u2705 Yes \u274c OpenAI only Built-in caching \u2705 disk/redis/semantic \u2705 Various Complexity Low High Package size ~5MB ~100MB+ <p>Rationale: For simple, structured queries about edge existence/orientation, LiteLLM provides everything needed with less complexity. LangChain may be added in v0.4.0+ when RAG capabilities are needed for literature context.</p>"},{"location":"architecture/overview/#key-dependencies","title":"Key Dependencies","text":"<ul> <li>litellm: Unified LLM API with cost tracking</li> <li>pydantic: Structured response validation</li> <li>diskcache (v0.3.0): Persistent query caching</li> </ul>"},{"location":"architecture/overview/#integration-points","title":"Integration Points","text":""},{"location":"architecture/overview/#with-causaliq-analysis","title":"With causaliq-analysis","text":"<p>The primary integration point is the <code>average()</code> function which produces edge probability tables. Future versions will accept a <code>knowledge</code> parameter:</p> <pre><code># Future usage (v0.5.0 of causaliq-analysis)\nfrom causaliq_knowledge import LLMKnowledge\n\nknowledge = LLMKnowledge(models=[\"gpt-4o-mini\"])\ndf = average(traces, sample_size=1000, knowledge=knowledge)\n</code></pre>"},{"location":"architecture/overview/#with-causaliq-discovery","title":"With causaliq-discovery","text":"<p>Structure learning algorithms will use knowledge to guide search in uncertain areas of the graph space.</p>"},{"location":"architecture/overview/#see-also","title":"See Also","text":"<ul> <li>LLM Integration Design Note - Detailed design for LLM queries</li> <li>Roadmap - Release planning</li> </ul>"},{"location":"architecture/testing_strategy/","title":"Testing Strategy Design Note","text":""},{"location":"architecture/testing_strategy/#overview","title":"Overview","text":"<p>Testing LLM-dependent code presents unique challenges: API calls cost money, responses are non-deterministic, and external services may be unavailable. This document describes the testing strategy for causaliq-knowledge.</p>"},{"location":"architecture/testing_strategy/#testing-pyramid","title":"Testing Pyramid","text":"<pre><code>                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502   Functional    \u2502  \u2190 Cached responses (v0.3.0+)\n                    \u2502     Tests       \u2502     Real scenarios, reproducible\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n               \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n               \u2502     Integration Tests     \u2502  \u2190 Real API calls (optional)\n               \u2502   (with live LLM APIs)    \u2502     Expensive, non-deterministic\n               \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502                  Unit Tests                      \u2502  \u2190 Mocked LLM responses\n    \u2502           (mocked LLM responses)                 \u2502     Fast, free, deterministic\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture/testing_strategy/#test-categories","title":"Test Categories","text":""},{"location":"architecture/testing_strategy/#1-unit-tests-always-run-in-ci","title":"1. Unit Tests (Always Run in CI)","text":"<p>Unit tests mock all LLM calls, making them:</p> <ul> <li>Fast: No network latency</li> <li>Free: No API costs</li> <li>Deterministic: Same result every time</li> <li>Isolated: No external dependencies</li> </ul> <pre><code># tests/unit/test_llm_providers.py\nimport pytest\nfrom types import SimpleNamespace\n\n\ndef _make_mock_response(content: str):\n    \"\"\"Helper to create a mock LLM response object.\"\"\"\n    return SimpleNamespace(\n        choices=[SimpleNamespace(message=SimpleNamespace(content=content))]\n    )\n\n\ndef test_query_edge_parses_valid_response(monkeypatch):\n    \"\"\"Test that valid LLM JSON is correctly parsed.\"\"\"\n    mock_response = _make_mock_response('''\n    {\"exists\": true, \"direction\": \"a_to_b\", \"confidence\": 0.85, \n     \"reasoning\": \"Smoking causes lung cancer via carcinogens.\"}\n    ''')\n\n    import litellm\n    monkeypatch.setattr(litellm, \"completion\", lambda *args, **kwargs: mock_response)\n\n    knowledge = LLMKnowledge(models=[\"gpt-4o-mini\"])\n    result = knowledge.query_edge(\"smoking\", \"lung_cancer\")\n\n    assert result.exists is True\n    assert result.direction == \"a_to_b\"\n    assert result.confidence == 0.85\n\n\ndef test_query_edge_handles_malformed_json(monkeypatch):\n    \"\"\"Test graceful handling of invalid LLM response.\"\"\"\n    mock_response = _make_mock_response(\"This is not JSON\")\n\n    import litellm\n    monkeypatch.setattr(litellm, \"completion\", lambda *args, **kwargs: mock_response)\n\n    knowledge = LLMKnowledge(models=[\"gpt-4o-mini\"])\n    result = knowledge.query_edge(\"A\", \"B\")\n\n    assert result.exists is None  # Uncertain\n    assert result.confidence == 0.0\n</code></pre>"},{"location":"architecture/testing_strategy/#2-integration-tests-optional-manual-or-ci-with-secrets","title":"2. Integration Tests (Optional, Manual or CI with Secrets)","text":"<p>Integration tests use real LLM APIs to validate actual behavior:</p> <ul> <li>Expensive: Costs money per call</li> <li>Non-deterministic: LLM responses vary</li> <li>Slow: Network latency</li> <li>Validates real integration: Catches API changes</li> </ul> <pre><code># tests/integration/test_llm_live.py\nimport pytest\nimport os\n\npytestmark = pytest.mark.skipif(\n    not os.getenv(\"OPENAI_API_KEY\"),\n    reason=\"OPENAI_API_KEY not set\"\n)\n\n@pytest.mark.slow\n@pytest.mark.integration\ndef test_openai_returns_valid_response():\n    \"\"\"Validate real OpenAI API returns parseable response.\"\"\"\n    knowledge = LLMKnowledge(models=[\"gpt-4o-mini\"])\n    result = knowledge.query_edge(\"smoking\", \"lung_cancer\")\n\n    # Don't assert specific values - LLM may vary\n    # Just validate structure and reasonable bounds\n    assert result.exists in [True, False, None]\n    assert 0.0 &lt;= result.confidence &lt;= 1.0\n    assert len(result.reasoning) &gt; 0\n</code></pre>"},{"location":"architecture/testing_strategy/#3-functional-tests-with-cached-responses-v030","title":"3. Functional Tests with Cached Responses (v0.3.0+)","text":"<p>Once response caching is implemented, we can create reproducible functional tests using cached LLM responses. This is the best of both worlds:</p> <ul> <li>Realistic: Uses actual LLM responses (captured once)</li> <li>Deterministic: Same cached response every time</li> <li>Free: No API calls after initial capture</li> <li>Fast: Disk read instead of network call</li> </ul>"},{"location":"architecture/testing_strategy/#how-it-works","title":"How It Works","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                     Test Fixture Generation                      \u2502\n\u2502                      (run once, manually)                        \u2502\n\u2502                                                                  \u2502\n\u2502   1. Run queries against real LLMs                               \u2502\n\u2502   2. Cache stores responses in tests/data/functional/cache/     \u2502\n\u2502   3. Commit cache files to git                                   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                     Functional Tests (CI)                        \u2502\n\u2502                                                                  \u2502\n\u2502   1. Load cached responses from tests/data/functional/cache/    \u2502\n\u2502   2. LLMKnowledge configured to use cache-only mode             \u2502\n\u2502   3. Tests run with real LLM responses, no API calls            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture/testing_strategy/#example-functional-test","title":"Example Functional Test","text":"<pre><code># tests/functional/test_edge_queries.py\nimport pytest\nfrom pathlib import Path\n\nCACHE_DIR = Path(__file__).parent / \"data\" / \"cache\"\n\n@pytest.fixture\ndef cached_knowledge():\n    \"\"\"LLMKnowledge using only cached responses.\"\"\"\n    return LLMKnowledge(\n        models=[\"gpt-4o-mini\"],\n        cache_dir=str(CACHE_DIR),\n        cache_only=True  # Fail if cache miss, don't call API\n    )\n\ndef test_smoking_cancer_relationship(cached_knowledge):\n    \"\"\"Test with cached response for smoking-&gt;cancer query.\"\"\"\n    result = cached_knowledge.query_edge(\"smoking\", \"lung_cancer\")\n\n    # Can assert specific values since response is cached\n    assert result.exists is True\n    assert result.direction == \"a_to_b\"\n    assert result.confidence &gt; 0.8\n\ndef test_consensus_across_models(cached_knowledge):\n    \"\"\"Test multi-model consensus with cached responses.\"\"\"\n    knowledge = LLMKnowledge(\n        models=[\"gpt-4o-mini\", \"claude-3-haiku\"],\n        cache_dir=str(CACHE_DIR),\n        cache_only=True\n    )\n    result = knowledge.query_edge(\"exercise\", \"heart_health\")\n\n    assert result.exists is True\n</code></pre>"},{"location":"architecture/testing_strategy/#generating-test-fixtures","title":"Generating Test Fixtures","text":"<pre><code># scripts/generate_test_fixtures.py\n\"\"\"\nRun this script manually to generate/update cached responses for functional tests.\nRequires API keys for all models being tested.\n\"\"\"\nfrom causaliq_knowledge import LLMKnowledge\nfrom pathlib import Path\n\nCACHE_DIR = Path(\"tests/data/functional/cache\")\nTEST_EDGES = [\n    (\"smoking\", \"lung_cancer\"),\n    (\"exercise\", \"heart_health\"),\n    (\"education\", \"income\"),\n    (\"rain\", \"wet_ground\"),\n]\n\ndef generate_fixtures():\n    knowledge = LLMKnowledge(\n        models=[\"gpt-4o-mini\", \"claude-3-haiku\"],\n        cache_dir=str(CACHE_DIR)\n    )\n\n    for node_a, node_b in TEST_EDGES:\n        print(f\"Caching: {node_a} -&gt; {node_b}\")\n        knowledge.query_edge(node_a, node_b)\n\n    print(f\"Fixtures saved to {CACHE_DIR}\")\n\nif __name__ == \"__main__\":\n    generate_fixtures()\n</code></pre>"},{"location":"architecture/testing_strategy/#ci-configuration","title":"CI Configuration","text":""},{"location":"architecture/testing_strategy/#pytest-markers","title":"pytest Markers","text":"<pre><code># pyproject.toml\n[tool.pytest.ini_options]\nmarkers = [\n    \"slow: marks tests as slow (deselect with '-m \\\"not slow\\\"')\",\n    \"integration: marks tests requiring live external APIs\",\n    \"functional: marks functional tests using cached responses\",\n]\naddopts = \"-ra -q --strict-markers -m 'not slow and not integration'\"\n</code></pre>"},{"location":"architecture/testing_strategy/#github-actions-strategy","title":"GitHub Actions Strategy","text":"Test Type When API Keys Cost Unit Every push/PR \u274c Not needed Free Functional Every push/PR \u274c Uses cache Free Integration Main branch only, optional \u2705 GitHub Secrets ~$0.01/run <pre><code># .github/workflows/ci.yml (conceptual addition)\njobs:\n  unit-and-functional:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - name: Run unit and functional tests\n        run: pytest tests/unit tests/functional\n\n  integration:\n    runs-on: ubuntu-latest\n    if: github.ref == 'refs/heads/main'  # Only on main\n    steps:\n      - uses: actions/checkout@v4\n      - name: Run integration tests\n        env:\n          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}\n        run: pytest tests/integration -m integration\n</code></pre>"},{"location":"architecture/testing_strategy/#test-data-management","title":"Test Data Management","text":""},{"location":"architecture/testing_strategy/#directory-structure","title":"Directory Structure","text":"<pre><code>tests/\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 unit/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 test_models.py          # EdgeKnowledge, etc.\n\u2502   \u251c\u2500\u2500 test_prompts.py         # Prompt formatting\n\u2502   \u2514\u2500\u2500 test_llm_providers.py   # Mocked LLM calls\n\u251c\u2500\u2500 integration/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2514\u2500\u2500 test_llm_live.py        # Real API calls\n\u251c\u2500\u2500 functional/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2514\u2500\u2500 test_edge_queries.py    # Using cached responses\n\u2514\u2500\u2500 data/\n    \u2514\u2500\u2500 functional/\n        \u2514\u2500\u2500 cache/              # Committed to git\n            \u251c\u2500\u2500 gpt-4o-mini/\n            \u2502   \u251c\u2500\u2500 smoking_lung_cancer.json\n            \u2502   \u2514\u2500\u2500 exercise_heart_health.json\n            \u2514\u2500\u2500 claude-3-haiku/\n                \u2514\u2500\u2500 ...\n</code></pre>"},{"location":"architecture/testing_strategy/#cache-file-format","title":"Cache File Format","text":"<pre><code>{\n  \"query\": {\n    \"node_a\": \"smoking\",\n    \"node_b\": \"lung_cancer\",\n    \"context\": {\"domain\": \"epidemiology\"}\n  },\n  \"model\": \"gpt-4o-mini\",\n  \"timestamp\": \"2026-01-05T10:30:00Z\",\n  \"response\": {\n    \"exists\": true,\n    \"direction\": \"a_to_b\",\n    \"confidence\": 0.92,\n    \"reasoning\": \"Smoking is an established cause of lung cancer...\"\n  }\n}\n</code></pre>"},{"location":"architecture/testing_strategy/#benefits-of-this-strategy","title":"Benefits of This Strategy","text":"Benefit How Achieved Fast CI Unit tests are mocked, functional use cache Low cost Only integration tests (optional) call APIs Reproducible Cached responses are deterministic Realistic Functional tests use real LLM responses Stable experiments Same cache = same results across runs Version controlled Cache files in git track response changes"},{"location":"architecture/testing_strategy/#future-considerations","title":"Future Considerations","text":""},{"location":"architecture/testing_strategy/#cache-invalidation-for-tests","title":"Cache Invalidation for Tests","text":"<p>When updating test fixtures:</p> <ol> <li>Delete relevant cache files</li> <li>Run fixture generation script</li> <li>Review new responses</li> <li>Commit updated cache files</li> </ol>"},{"location":"architecture/testing_strategy/#model-version-tracking","title":"Model Version Tracking","text":"<p>Cache files should include model version to detect when responses might change due to model updates.</p>"},{"location":"architecture/testing_strategy/#semantic-similarity-testing","title":"Semantic Similarity Testing","text":"<p>For v0.4.0+, consider testing that semantically similar queries hit cache (e.g., \"smoking\" vs \"tobacco use\" \u2192 \"cancer\" vs \"lung cancer\").</p>"},{"location":"userguide/introduction/","title":"CausalIQ Knowledge User Guide","text":""},{"location":"userguide/introduction/#what-is-causaliq-knowledge","title":"What is CausalIQ Knowledge?","text":"<p>CausalIQ Knowledge is a Python package that provides knowledge services for causal discovery workflows. It enables you to query Large Language Models (LLMs) about potential causal relationships between variables, helping to resolve uncertainty in learned causal graphs.</p>"},{"location":"userguide/introduction/#primary-use-case","title":"Primary Use Case","text":"<p>When averaging multiple causal graphs learned from data subsamples, some edges may be uncertain - appearing in some graphs but not others, or with inconsistent directions. CausalIQ Knowledge helps resolve this uncertainty by querying LLMs about whether:</p> <ol> <li>A causal relationship exists between two variables</li> <li>What the direction of causation is (A\u2192B or B\u2192A)</li> </ol>"},{"location":"userguide/introduction/#quick-start","title":"Quick Start","text":""},{"location":"userguide/introduction/#installation","title":"Installation","text":"<pre><code>pip install causaliq-knowledge\n</code></pre>"},{"location":"userguide/introduction/#basic-usage","title":"Basic Usage","text":"<pre><code>from causaliq_knowledge import LLMKnowledge, EdgeKnowledge\n\n# Initialize with your preferred model\nknowledge = LLMKnowledge(models=[\"gpt-4o-mini\"])\n\n# Query about a potential edge\nresult: EdgeKnowledge = knowledge.query_edge(\n    node_a=\"smoking\",\n    node_b=\"lung_cancer\",\n    context={\"domain\": \"epidemiology\"}\n)\n\nprint(f\"Exists: {result.exists}\")\nprint(f\"Direction: {result.direction}\")\nprint(f\"Confidence: {result.confidence}\")\nprint(f\"Reasoning: {result.reasoning}\")\n</code></pre>"},{"location":"userguide/introduction/#using-local-models-free","title":"Using Local Models (Free)","text":"<pre><code># Use Ollama for free local inference\n# First: install Ollama and run `ollama pull llama3`\nknowledge = LLMKnowledge(models=[\"ollama/llama3\"])\n</code></pre>"},{"location":"userguide/introduction/#multi-model-consensus","title":"Multi-Model Consensus","text":"<pre><code># Query multiple models for more robust answers\nknowledge = LLMKnowledge(\n    models=[\"gpt-4o-mini\", \"ollama/llama3\"],\n    consensus_strategy=\"weighted_vote\"\n)\n</code></pre>"},{"location":"userguide/introduction/#llm-provider-setup","title":"LLM Provider Setup","text":"<p>CausalIQ Knowledge uses LiteLLM to support 100+ LLM providers through a unified interface. You need API access to at least one provider:</p>"},{"location":"userguide/introduction/#free-options","title":"Free Options","text":"Provider Setup Ollama (local) Install from ollama.ai, run <code>ollama pull llama3</code> Groq Sign up at console.groq.com, set <code>GROQ_API_KEY</code> Google Gemini Sign up at makersuite.google.com, set <code>GEMINI_API_KEY</code>"},{"location":"userguide/introduction/#paid-options-pay-per-use","title":"Paid Options (Pay-per-use)","text":"Provider Setup Cost OpenAI Sign up at platform.openai.com, set <code>OPENAI_API_KEY</code> ~$0.15/1M tokens (mini) Anthropic Sign up at console.anthropic.com, set <code>ANTHROPIC_API_KEY</code> ~$0.25/1M tokens (haiku)"},{"location":"userguide/introduction/#whats-next","title":"What's Next?","text":"<ul> <li>Architecture Overview - Understand how the package works</li> <li>LLM Integration Design - Detailed design documentation</li> <li>API Reference - Full API documentation</li> </ul>"}]}