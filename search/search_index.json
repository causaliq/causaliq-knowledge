{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"CausalIQ Knowledge","text":""},{"location":"#welcome","title":"Welcome","text":"<p>Welcome to the documentation for CausalIQ Knowledge, which combines traditional statistical structure learning algorithms with the contextual understanding and reasoning capabilities of Large Language Models. This integration enables more interpretable, domain-aware, and human-friendly causal discovery workflows. It is part of the CausalIQ ecosystem for intelligent causal discovery.</p>"},{"location":"#overview","title":"Overview","text":"<p>This site provides detailed documentation, including:</p> <ul> <li>Development roadmap</li> <li>User guide</li> <li>Architectural vision</li> <li>Design notes</li> <li>API reference for users and contributors</li> </ul>"},{"location":"#quickstart-installation","title":"Quickstart &amp; Installation","text":"<p>For a quickstart guide and installation instructions, see the README on GitHub.</p>"},{"location":"#documentation-contents","title":"Documentation Contents","text":"<ul> <li>Development Roadmap: Roadmap of upcoming features</li> <li>User Guide: Comprehensive user guide</li> <li>Network Context Format: JSON format for network context</li> <li>Architecture: Overall architecture and design notes</li> <li>API Reference: Complete reference for Python code</li> <li>Graph Module: Network context, filtering, generation</li> <li>Development Guidelines: CausalIQ guidelines for developers</li> <li>Changelog</li> <li>License</li> </ul>"},{"location":"#support-community","title":"Support &amp; Community","text":"<ul> <li>GitHub Issues: Report bugs or request features.</li> <li>GitHub Discussions: Ask questions and join the community.</li> </ul> <p>Tip: Use the navigation sidebar to explore the documentation. For the latest code and releases, visit the causaliq-knowledge GitHub repository.</p> <p>Supported Python Versions: 3.9, 3.10, 3.11, 3.12, 3.13 Default Python Version: 3.11</p>"},{"location":"roadmap/","title":"CausalIQ Knowledge - Development Roadmap","text":"<p>Last updated: February 20, 2026  </p> <p>This project roadmap fits into the overall ecosystem roadmap</p>"},{"location":"roadmap/#under-development","title":"\ud83d\udea7  Under development","text":"<p>No releases currently under active development.</p>"},{"location":"roadmap/#previous-releases","title":"\u2705 Previous Releases","text":"<ul> <li> <p>v0.1.0 - Foundation LLM [January 2026]: Foundation release establishing LLM client infrastructure for causal graph generation.</p> </li> <li> <p>v0.2.0 - Additional LLMs [January 2026]: Expanded LLM provider support from 2 to 7 providers.</p> </li> <li> <p>v0.3.0 - LLM Caching [January 2026]: SQLite-based response caching with CLI tools for cache management.</p> </li> <li> <p>v0.4.0 - Graph Generation [February 2026]: CLI tools for LLM-generated causal graphs.</p> </li> <li> <p>v0.5.0 - Workflow Integration [February 2026]: Integration into CausalIQ Workflows including writing results to cache.</p> </li> </ul>"},{"location":"roadmap/#upcoming-releases-speculative","title":"\ud83d\udee3\ufe0f Upcoming Releases (speculative)","text":""},{"location":"roadmap/#release-v060-statistical-fusion","title":"Release v0.6.0 - Statistical Fusion","text":"<p>Support knowledge requirements for fusing LLM knowledge and statistical graph averaging.</p> <p>Scope:</p> <ul> <li>graph generation returns separate existence and orientation probabilities</li> <li>integrate with latest infrastructure to record graph ede probabilities</li> </ul>"},{"location":"roadmap/#release-v070-llm-provider-cost-tracking","title":"Release v0.7.0 - LLM Provider Cost Tracking","text":"<p>Query LLM provider APIs for usage and cost statistics.</p> <p>Scope:</p> <ul> <li>Provider usage API clients</li> <li>CLI <code>llm costs</code> command</li> <li>Cache savings analysis</li> </ul>"},{"location":"roadmap/#release-v080-enhanced-llm-context","title":"Release v0.8.0 - Enhanced LLM Context","text":"<p>Background literature supplied to LLMs</p>"},{"location":"roadmap/#release-v090-structure-learning","title":"Release v0.9.0 - Structure Learning","text":"<p>Integration with structure learning algorithms</p>"},{"location":"roadmap/#release-v0100-legacy-reference","title":"Release v0.10.0 - Legacy Reference","text":"<p>Support for deriving knowledge from reference networks and migration of functionality from legacy discovery repo</p>"},{"location":"api/base_client/","title":"LLM Client Base Interface","text":"<p>Abstract base class and common types for all LLM vendor clients. This module defines the interface that all vendor-specific clients must implement, ensuring consistent behavior across different LLM providers.</p>"},{"location":"api/base_client/#overview","title":"Overview","text":"<p>The base client module provides:</p> <ul> <li>BaseLLMClient - Abstract base class defining the client interface</li> <li>LLMConfig - Base configuration dataclass for all clients</li> <li>LLMResponse - Unified response format from any LLM provider</li> </ul>"},{"location":"api/base_client/#caching-support","title":"Caching Support","text":"<p>BaseLLMClient includes built-in caching integration:</p> <ul> <li>set_cache() - Configure a TokenCache for response caching</li> <li>cached_completion() - Make completion requests with automatic caching</li> <li>_build_cache_key() - Generate deterministic cache keys (SHA-256)</li> </ul>"},{"location":"api/base_client/#design-philosophy","title":"Design Philosophy","text":"<p>We use vendor-specific API clients rather than wrapper libraries like LiteLLM or LangChain. This provides:</p> <ul> <li>Minimal dependencies (httpx only for HTTP)</li> <li>Reliable and predictable behavior</li> <li>Easy debugging without abstraction layers</li> <li>Full control over API interactions</li> </ul> <p>The abstract interface ensures that all vendor clients behave consistently, making it easy to swap providers or add new ones.</p>"},{"location":"api/base_client/#usage","title":"Usage","text":"<p>Vendor-specific clients inherit from <code>BaseLLMClient</code>:</p> <pre><code>from causaliq_knowledge.llm import (\n    BaseLLMClient,\n    LLMConfig,\n    LLMResponse,\n    GroqClient,\n    GeminiClient,\n)\n\n# All clients share the same interface\ndef query_llm(client: BaseLLMClient, prompt: str) -&gt; str:\n    messages = [{\"role\": \"user\", \"content\": prompt}]\n    response = client.completion(messages)\n    return response.content\n\n# Works with any client\ngroq = GroqClient()\ngemini = GeminiClient()\n\nresult1 = query_llm(groq, \"What is 2+2?\")\nresult2 = query_llm(gemini, \"What is 2+2?\")\n</code></pre>"},{"location":"api/base_client/#caching-llm-responses","title":"Caching LLM Responses","text":"<p>Enable caching to avoid redundant API calls:</p> <pre><code>from causaliq_core.cache import TokenCache\nfrom causaliq_knowledge.llm import GroqClient, LLMConfig\n\n# Create a persistent cache\nwith TokenCache(\"llm_cache.db\") as cache:\n    client = GroqClient(LLMConfig(model=\"llama-3.1-8b-instant\"))\n    client.set_cache(cache)\n\n    messages = [{\"role\": \"user\", \"content\": \"What is Python?\"}]\n\n    # First call - hits API, stores in cache\n    response1 = client.cached_completion(messages)\n\n    # Second call - returns from cache, no API call\n    response2 = client.cached_completion(messages)\n\n    assert response1.content == response2.content\n    assert client.call_count == 1  # Only one API call made\n</code></pre> <p>The cache uses the LLMEntryEncoder automatically, storing:</p> <ul> <li>Request details (model, messages, temperature, max_tokens)</li> <li>Response content</li> <li>Metadata (provider, token counts, cost, latency)</li> </ul> <p>Each cached entry captures latency timing automatically using <code>time.perf_counter()</code>, enabling performance analysis across providers and models.</p> <p>See LLM Cache for details on the cache entry structure. <pre><code>## Creating a Custom Client\n\nTo add support for a new LLM provider, implement the `BaseLLMClient` interface:\n\n```python\nfrom causaliq_knowledge.llm import BaseLLMClient, LLMConfig, LLMResponse\n\nclass MyCustomClient(BaseLLMClient):\n    def __init__(self, config: LLMConfig) -&gt; None:\n        self.config = config\n        self._total_calls = 0\n\n    @property\n    def provider_name(self) -&gt; str:\n        return \"my_provider\"\n\n    def completion(self, messages, **kwargs) -&gt; LLMResponse:\n        # Implement API call here\n        ...\n        return LLMResponse(\n            content=\"response text\",\n            model=self.config.model,\n            input_tokens=10,\n            output_tokens=20,\n        )\n\n    @property\n    def call_count(self) -&gt; int:\n        return self._total_calls\n</code></pre></p>"},{"location":"api/base_client/#llmconfig","title":"LLMConfig","text":""},{"location":"api/base_client/#causaliq_knowledge.llm.base_client.LLMConfig","title":"LLMConfig  <code>dataclass</code>","text":"<pre><code>LLMConfig(\n    model: str,\n    temperature: float = 0.1,\n    max_tokens: int = 500,\n    timeout: float = 30.0,\n    api_key: Optional[str] = None,\n)\n</code></pre> <p>Base configuration for all LLM clients.</p> <p>This dataclass defines common configuration options shared by all LLM provider clients. Vendor-specific clients may extend this with additional options.</p> <p>Attributes:</p> <ul> <li> <code>model</code>               (<code>str</code>)           \u2013            <p>Model identifier (provider-specific format).</p> </li> <li> <code>temperature</code>               (<code>float</code>)           \u2013            <p>Sampling temperature (0.0=deterministic, 1.0=creative).</p> </li> <li> <code>max_tokens</code>               (<code>int</code>)           \u2013            <p>Maximum tokens in the response.</p> </li> <li> <code>timeout</code>               (<code>float</code>)           \u2013            <p>Request timeout in seconds.</p> </li> <li> <code>api_key</code>               (<code>Optional[str]</code>)           \u2013            <p>API key for authentication (optional, can use env var).</p> </li> </ul>"},{"location":"api/base_client/#llmresponse","title":"LLMResponse","text":""},{"location":"api/base_client/#causaliq_knowledge.llm.base_client.LLMResponse","title":"LLMResponse  <code>dataclass</code>","text":"<pre><code>LLMResponse(\n    content: str,\n    model: str,\n    input_tokens: int = 0,\n    output_tokens: int = 0,\n    cost: float = 0.0,\n    finish_reason: str = \"stop\",\n    raw_response: Optional[Dict[str, Any]] = None,\n    llm_timestamp: Optional[datetime] = None,\n    llm_latency_ms: Optional[int] = None,\n)\n</code></pre> <p>Standard response from any LLM client.</p> <p>This dataclass provides a unified response format across all LLM providers, abstracting away provider-specific response structures.</p> <p>Attributes:</p> <ul> <li> <code>content</code>               (<code>str</code>)           \u2013            <p>The text content of the response.</p> </li> <li> <code>model</code>               (<code>str</code>)           \u2013            <p>The model that generated the response.</p> </li> <li> <code>input_tokens</code>               (<code>int</code>)           \u2013            <p>Number of input/prompt tokens used.</p> </li> <li> <code>output_tokens</code>               (<code>int</code>)           \u2013            <p>Number of output/completion tokens generated.</p> </li> <li> <code>cost</code>               (<code>float</code>)           \u2013            <p>Estimated cost of the request (if available).</p> </li> <li> <code>finish_reason</code>               (<code>str</code>)           \u2013            <p>Why generation stopped (stop, length, etc.).</p> </li> <li> <code>raw_response</code>               (<code>Optional[Dict[str, Any]]</code>)           \u2013            <p>The original provider-specific response (for debugging).</p> </li> <li> <code>llm_timestamp</code>               (<code>Optional[datetime]</code>)           \u2013            <p>Original LLM response timestamp (from cache or current).</p> </li> <li> <code>llm_latency_ms</code>               (<code>Optional[int]</code>)           \u2013            <p>Original LLM response latency (from cache or current).</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>parse_json</code>             \u2013              <p>Parse content as JSON, handling common formatting issues.</p> </li> </ul>"},{"location":"api/base_client/#causaliq_knowledge.llm.base_client.LLMResponse.parse_json","title":"parse_json","text":"<pre><code>parse_json() -&gt; Optional[Dict[str, Any]]\n</code></pre> <p>Parse content as JSON, handling common formatting issues.</p> <p>LLMs sometimes wrap JSON in markdown code blocks. This method handles those cases and attempts to extract valid JSON.</p> <p>Returns:</p> <ul> <li> <code>Optional[Dict[str, Any]]</code>           \u2013            <p>Parsed JSON as dict, or None if parsing fails.</p> </li> </ul>"},{"location":"api/base_client/#basellmclient","title":"BaseLLMClient","text":""},{"location":"api/base_client/#causaliq_knowledge.llm.base_client.BaseLLMClient","title":"BaseLLMClient","text":"<pre><code>BaseLLMClient(config: LLMConfig)\n</code></pre> <p>Abstract base class for LLM clients.</p> <p>All LLM vendor clients (OpenAI, Anthropic, Groq, Gemini, Llama, etc.) must implement this interface to ensure consistent behavior across the codebase.</p> <p>This abstraction allows: - Easy addition of new LLM providers - Consistent API for all providers - Provider-agnostic code in higher-level modules - Simplified testing with mock implementations</p> Example <p>class MyClient(BaseLLMClient): ...     def completion(self, messages, **kwargs): ...         # Implementation here ...         pass ... client = MyClient(config) msgs = [{\"role\": \"user\", \"content\": \"Hello\"}] response = client.completion(msgs) print(response.content)</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>cached_completion</code>             \u2013              <p>Make a completion request with caching.</p> </li> <li> <code>complete_json</code>             \u2013              <p>Make a completion request and parse response as JSON.</p> </li> <li> <code>completion</code>             \u2013              <p>Make a chat completion request.</p> </li> <li> <code>is_available</code>             \u2013              <p>Check if the LLM provider is available and configured.</p> </li> <li> <code>list_models</code>             \u2013              <p>List available models from the provider.</p> </li> <li> <code>set_cache</code>             \u2013              <p>Configure caching for this client.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>cache</code>               (<code>Optional['TokenCache']</code>)           \u2013            <p>Return the configured cache, if any.</p> </li> <li> <code>call_count</code>               (<code>int</code>)           \u2013            <p>Return the number of API calls made by this client.</p> </li> <li> <code>model_name</code>               (<code>str</code>)           \u2013            <p>Return the model name being used.</p> </li> <li> <code>provider_name</code>               (<code>str</code>)           \u2013            <p>Return the name of the LLM provider.</p> </li> <li> <code>use_cache</code>               (<code>bool</code>)           \u2013            <p>Return whether caching is enabled.</p> </li> </ul>"},{"location":"api/base_client/#causaliq_knowledge.llm.base_client.BaseLLMClient(config)","title":"<code>config</code>","text":"(<code>LLMConfig</code>)           \u2013            <p>Configuration for the LLM client.</p>"},{"location":"api/base_client/#causaliq_knowledge.llm.base_client.BaseLLMClient.cache","title":"cache  <code>property</code>","text":"<pre><code>cache: Optional['TokenCache']\n</code></pre> <p>Return the configured cache, if any.</p>"},{"location":"api/base_client/#causaliq_knowledge.llm.base_client.BaseLLMClient.call_count","title":"call_count  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>call_count: int\n</code></pre> <p>Return the number of API calls made by this client.</p> <p>Returns:</p> <ul> <li> <code>int</code>           \u2013            <p>Total number of completion calls made.</p> </li> </ul>"},{"location":"api/base_client/#causaliq_knowledge.llm.base_client.BaseLLMClient.model_name","title":"model_name  <code>property</code>","text":"<pre><code>model_name: str\n</code></pre> <p>Return the model name being used.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>Model identifier string.</p> </li> </ul>"},{"location":"api/base_client/#causaliq_knowledge.llm.base_client.BaseLLMClient.provider_name","title":"provider_name  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>provider_name: str\n</code></pre> <p>Return the name of the LLM provider.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>Provider name (e.g., \"openai\", \"anthropic\", \"groq\").</p> </li> </ul>"},{"location":"api/base_client/#causaliq_knowledge.llm.base_client.BaseLLMClient.use_cache","title":"use_cache  <code>property</code>","text":"<pre><code>use_cache: bool\n</code></pre> <p>Return whether caching is enabled.</p>"},{"location":"api/base_client/#causaliq_knowledge.llm.base_client.BaseLLMClient.cached_completion","title":"cached_completion","text":"<pre><code>cached_completion(messages: List[Dict[str, str]], **kwargs: Any) -&gt; LLMResponse\n</code></pre> <p>Make a completion request with caching.</p> <p>If caching is enabled and a cached response exists, returns the cached response without making an API call. Otherwise, makes the API call and caches the result.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>LLMResponse</code>           \u2013            <p>LLMResponse with the generated content and metadata.</p> </li> </ul>"},{"location":"api/base_client/#causaliq_knowledge.llm.base_client.BaseLLMClient.cached_completion(messages)","title":"<code>messages</code>","text":"(<code>List[Dict[str, str]]</code>)           \u2013            <p>List of message dicts with \"role\" and \"content\" keys.</p>"},{"location":"api/base_client/#causaliq_knowledge.llm.base_client.BaseLLMClient.cached_completion(**kwargs)","title":"<code>**kwargs</code>","text":"(<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Provider-specific options (temperature, max_tokens, etc.) Also accepts request_id (str) for identifying requests in exports. Note: request_id is NOT part of the cache key.</p>"},{"location":"api/base_client/#causaliq_knowledge.llm.base_client.BaseLLMClient.complete_json","title":"complete_json","text":"<pre><code>complete_json(\n    messages: List[Dict[str, str]], **kwargs: Any\n) -&gt; tuple[Optional[Dict[str, Any]], LLMResponse]\n</code></pre> <p>Make a completion request and parse response as JSON.</p> <p>Convenience method that calls completion() and attempts to parse the response content as JSON.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>tuple[Optional[Dict[str, Any]], LLMResponse]</code>           \u2013            <p>Tuple of (parsed JSON dict or None, raw LLMResponse).</p> </li> </ul>"},{"location":"api/base_client/#causaliq_knowledge.llm.base_client.BaseLLMClient.complete_json(messages)","title":"<code>messages</code>","text":"(<code>List[Dict[str, str]]</code>)           \u2013            <p>List of message dicts with \"role\" and \"content\" keys.</p>"},{"location":"api/base_client/#causaliq_knowledge.llm.base_client.BaseLLMClient.complete_json(**kwargs)","title":"<code>**kwargs</code>","text":"(<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Provider-specific options passed to completion().</p>"},{"location":"api/base_client/#causaliq_knowledge.llm.base_client.BaseLLMClient.completion","title":"completion  <code>abstractmethod</code>","text":"<pre><code>completion(messages: List[Dict[str, str]], **kwargs: Any) -&gt; LLMResponse\n</code></pre> <p>Make a chat completion request.</p> <p>This is the core method that sends a request to the LLM provider and returns a standardized response.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>LLMResponse</code>           \u2013            <p>LLMResponse with the generated content and metadata.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the API request fails or returns an error.</p> </li> </ul>"},{"location":"api/base_client/#causaliq_knowledge.llm.base_client.BaseLLMClient.completion(messages)","title":"<code>messages</code>","text":"(<code>List[Dict[str, str]]</code>)           \u2013            <p>List of message dicts with \"role\" and \"content\" keys. Roles can be: \"system\", \"user\", \"assistant\".</p>"},{"location":"api/base_client/#causaliq_knowledge.llm.base_client.BaseLLMClient.completion(**kwargs)","title":"<code>**kwargs</code>","text":"(<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Provider-specific options (temperature, max_tokens, etc.) that override the config defaults.</p>"},{"location":"api/base_client/#causaliq_knowledge.llm.base_client.BaseLLMClient.is_available","title":"is_available  <code>abstractmethod</code>","text":"<pre><code>is_available() -&gt; bool\n</code></pre> <p>Check if the LLM provider is available and configured.</p> <p>This method checks whether the client can make API calls: - For cloud providers: checks if API key is set - For local providers: checks if server is running</p> <p>Returns:</p> <ul> <li> <code>bool</code>           \u2013            <p>True if the provider is available and ready for requests.</p> </li> </ul>"},{"location":"api/base_client/#causaliq_knowledge.llm.base_client.BaseLLMClient.list_models","title":"list_models  <code>abstractmethod</code>","text":"<pre><code>list_models() -&gt; List[str]\n</code></pre> <p>List available models from the provider.</p> <p>Queries the provider's API to get the list of models accessible with the current API key or configuration. Results are filtered by the user's subscription/access level.</p> <p>Returns:</p> <ul> <li> <code>List[str]</code>           \u2013            <p>List of model identifiers available for use.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the API request fails.</p> </li> </ul>"},{"location":"api/base_client/#causaliq_knowledge.llm.base_client.BaseLLMClient.set_cache","title":"set_cache","text":"<pre><code>set_cache(cache: Optional['TokenCache'], use_cache: bool = True) -&gt; None\n</code></pre> <p>Configure caching for this client.</p> <p>Parameters:</p>"},{"location":"api/base_client/#causaliq_knowledge.llm.base_client.BaseLLMClient.set_cache(cache)","title":"<code>cache</code>","text":"(<code>Optional['TokenCache']</code>)           \u2013            <p>TokenCache instance for caching, or None to disable.</p>"},{"location":"api/base_client/#causaliq_knowledge.llm.base_client.BaseLLMClient.set_cache(use_cache)","title":"<code>use_cache</code>","text":"(<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use the cache (default True).</p>"},{"location":"api/cli/","title":"CausalIQ Knowledge CLI","text":"<p>The command-line interface provides a quick way to test LLM queries about causal relationships.</p>"},{"location":"api/cli/#installation","title":"Installation","text":"<p>The CLI is automatically installed when you install the package:</p> <pre><code>pip install causaliq-knowledge\n</code></pre>"},{"location":"api/cli/#usage","title":"Usage","text":""},{"location":"api/cli/#basic-query","title":"Basic Query","text":"<pre><code># Query using default model (Groq)\ncqknow query smoking lung_cancer\n\n# With domain context\ncqknow query smoking lung_cancer --domain medicine\n</code></pre>"},{"location":"api/cli/#multiple-models","title":"Multiple Models","text":"<pre><code># Query multiple models for consensus\ncqknow query X Y --model groq/llama-3.1-8b-instant --model gemini/gemini-2.5-flash\n</code></pre>"},{"location":"api/cli/#json-output","title":"JSON Output","text":"<pre><code># Get structured JSON output\ncqknow query smoking lung_cancer --json\n</code></pre>"},{"location":"api/cli/#options","title":"Options","text":"Option Short Description <code>--model</code> <code>-m</code> LLM model to query (can be repeated) <code>--domain</code> <code>-d</code> Domain context (e.g., \"medicine\") <code>--strategy</code> <code>-s</code> Consensus strategy: weighted_vote or highest_confidence <code>--json</code> Output as JSON <code>--temperature</code> <code>-t</code> LLM temperature (0.0-1.0)"},{"location":"api/cli/#cache-management","title":"Cache Management","text":"<p>The <code>cache</code> command group provides tools for inspecting and managing the LLM response cache.</p>"},{"location":"api/cli/#cache-stats","title":"Cache Stats","text":"<p>View statistics about a cache database:</p> <pre><code># Show cache statistics\ncqknow cache stats ./llm_cache.db\n\n# Output:\n# Cache: ./llm_cache.db\n# ========================================\n# Entries:  42\n# Tokens:   15,230\n</code></pre>"},{"location":"api/cli/#cache-export","title":"Cache Export","text":"<p>Export cache entries to human-readable JSON files:</p> <pre><code># Export all entries to a directory\ncqknow cache export ./llm_cache.db ./export_dir\n\n# Export to a zip archive (auto-detected from .zip extension)\ncqknow cache export ./llm_cache.db ./export.zip\n</code></pre> <p>Files are named using the format <code>{id}_{timestamp}_{provider}.json</code>: <pre><code>cli_2026-01-29-143052_groq.json\nexpt01_2026-01-28-091523_gemini.json\n</code></pre></p>"},{"location":"api/cli/#json-output_1","title":"JSON Output","text":"<pre><code># Get stats as JSON for scripting\ncqknow cache stats ./llm_cache.db --json\n\n# Get export result as JSON\ncqknow cache export ./llm_cache.db ./export_dir --json\n</code></pre>"},{"location":"api/cli/#cache-command-options","title":"Cache Command Options","text":"Command Description <code>cache stats &lt;path&gt;</code> Show entry and token counts <code>cache stats &lt;path&gt; --json</code> Output stats as JSON <code>cache export &lt;path&gt; &lt;dir&gt;</code> Export entries to directory <code>cache export &lt;path&gt; &lt;file.zip&gt;</code> Export entries to zip archive <code>cache export &lt;path&gt; &lt;output&gt; --json</code> Output export result as JSON <code>cache import &lt;cache&gt; &lt;input&gt;</code> Import entries from directory or zip <code>cache import &lt;cache&gt; &lt;input&gt; --json</code> Output import result as JSON"},{"location":"api/cli/#cache-import","title":"Cache Import","text":"<p>Import cache entries from JSON files:</p> <pre><code># Import from a directory\ncqknow cache import ./llm_cache.db ./import_dir\n\n# Import from a zip archive (auto-detected from .zip extension)\ncqknow cache import ./llm_cache.db ./export.zip\n\n# Get import result as JSON\ncqknow cache import ./llm_cache.db ./import_dir --json\n</code></pre> <p>Entry types are auto-detected from JSON structure: - LLM entries: JSON containing <code>cache_key.model</code>, <code>cache_key.messages</code>, and <code>response</code> - Generic JSON: Any other valid JSON file</p> <p>This enables round-trip operations: export from one cache, import into another.</p>"},{"location":"api/cli/#graph-generation","title":"Graph Generation","text":"<p>The <code>generate</code> command group provides tools for generating causal graphs from network context files using LLMs.</p>"},{"location":"api/cli/#generate-graph","title":"Generate Graph","text":"<p>Generate a complete causal graph from a network context file:</p> <pre><code># Basic usage with default settings\ncqknow generate graph -n context.json\n\n# Use a specific LLM and request ID\ncqknow generate graph -n context.json -m gemini/gemini-2.5-flash --id expt01\n\n# Use rich context level\ncqknow generate graph -n context.json --prompt-detail rich\n\n# Save output to file\ncqknow generate graph -n context.json -o output.json\n</code></pre>"},{"location":"api/cli/#generate-command-options","title":"Generate Command Options","text":"Option Short Description <code>--network-context</code> <code>-n</code> Path to network context JSON file (required) <code>--prompt-detail</code> <code>-p</code> Context level: <code>minimal</code>, <code>standard</code>, or <code>rich</code> <code>--llm</code> <code>-m</code> LLM model to use <code>--output</code> <code>-o</code> Output file path (JSON) <code>--format</code> <code>-f</code> Output format: <code>edge_list</code> or <code>adjacency_matrix</code> <code>--json</code> Output result as JSON to stdout <code>--id</code> Request identifier for export filenames (default: cli) <code>--use-benchmark-names</code> Use benchmark names instead of LLM names <code>--cache/--no-cache</code> Enable/disable response caching <code>--cache-path</code> <code>-c</code> Path to cache database <code>--temperature</code> <code>-t</code> LLM temperature (0.0-1.0) <p>The <code>--id</code> option sets a request identifier that is stored in the cache metadata (not affecting cache key matching). This identifier is used in export filenames with the format <code>{id}_{timestamp}_{provider}.json</code>.</p>"},{"location":"api/cli/#cli-entry-point","title":"CLI Entry Point","text":""},{"location":"api/cli/#causaliq_knowledge.cli","title":"cli","text":"<p>Command-line interface for causaliq-knowledge.</p> <p>This package provides the CLI implementation split into logical modules:</p> <ul> <li>main: Core CLI entry point</li> <li>cache: Cache management commands (stats, export, import)</li> <li>generate: Graph generation commands</li> <li>models: Model listing command</li> </ul> <p>Modules:</p> <ul> <li> <code>cache</code>           \u2013            <p>Cache management CLI commands.</p> </li> <li> <code>generate</code>           \u2013            <p>Graph generation CLI commands.</p> </li> <li> <code>main</code>           \u2013            <p>Main CLI entry point and core commands.</p> </li> <li> <code>models</code>           \u2013            <p>Model listing CLI command.</p> </li> </ul> <p>Functions:</p> <ul> <li> <code>cli</code>             \u2013              <p>CausalIQ Knowledge - LLM knowledge for causal discovery.</p> </li> </ul>"},{"location":"api/cli/#causaliq_knowledge.cli.cli","title":"cli","text":"<pre><code>cli() -&gt; None\n</code></pre> <p>CausalIQ Knowledge - LLM knowledge for causal discovery.</p> <p>Use 'cqknow generate_graph' to generate causal graphs. Use 'cqknow list_models' to list available LLM models.</p> <p>LLM Cache Management (for caching LLM API responses):</p> <p>Use 'cqknow cache_stats' to view LLM cache statistics. Use 'cqknow export_cache' to export LLM cache entries. Use 'cqknow import_cache' to import LLM cache entries.</p>"},{"location":"api/overview/","title":"CausalIQ Knowledge API Reference","text":"<p>API documentation for causaliq-knowledge, organised by module.</p>"},{"location":"api/overview/#import-patterns","title":"Import Patterns","text":"<p>Graph generation classes are available from the <code>graph</code> submodule:</p> <pre><code>from causaliq_knowledge.graph import (\n    # Network context (main model)\n    NetworkContext,\n    NetworkLoadError,\n    # Variable specification\n    VariableSpec,\n    VariableType,\n    VariableRole,\n    # Supporting models\n    ViewDefinition,\n    Provenance,\n    LLMGuidance,\n    Constraints,\n    CausalPrinciple,\n    GroundTruth,\n    PromptDetails,\n    # Filtering\n    ViewFilter,\n    PromptDetail,\n    # Generation\n    GraphGenerator,\n    GraphGeneratorConfig,\n    GeneratedGraph,\n    ProposedEdge,\n    GenerationMetadata,\n    # Parameters\n    GenerateGraphParams,\n    # Prompts\n    GraphQueryPrompt,\n    OutputFormat,\n    # Cache\n    GraphCompressor,\n)\n</code></pre> <p>Cache infrastructure is available from causaliq-core:</p> <pre><code>from causaliq_core.cache import TokenCache\n</code></pre> <p>LLM clients should be imported from the <code>llm</code> submodule:</p> <pre><code>from causaliq_knowledge.llm import (\n    # Abstract base interface\n    BaseLLMClient,\n    LLMConfig,\n    LLMResponse,\n    # Vendor clients\n    GroqClient,\n    GroqConfig,\n    GeminiClient,\n    GeminiConfig,\n    OpenAIClient,\n    OpenAIConfig,\n    AnthropicClient,\n    AnthropicConfig,\n    DeepSeekClient,\n    DeepSeekConfig,\n    MistralClient,\n    MistralConfig,\n    OllamaClient,\n    OllamaConfig,\n)\n</code></pre>"},{"location":"api/overview/#modules","title":"Modules","text":""},{"location":"api/overview/#graph-module","title":"Graph Module","text":"<p>LLM-based causal graph generation from network context specifications:</p> <ul> <li>Graph Generator - Generate complete causal graphs</li> <li>GraphGenerator, GraphGeneratorConfig</li> <li>GeneratedGraph, ProposedEdge, GenerationMetadata</li> <li>Network Context - Pydantic models for network context</li> <li>NetworkContext, NetworkLoadError</li> <li>VariableSpec, VariableType, VariableRole</li> <li>PromptDetails, ViewDefinition, Provenance, Constraints</li> <li>View Filter - Extract context levels</li> <li>ViewFilter, PromptDetail (MINIMAL, STANDARD, RICH)</li> <li>Graph Prompts - Prompt builders</li> <li>GraphQueryPrompt, OutputFormat</li> <li>Response Models - Response parsing</li> <li>ProposedEdge, GeneratedGraph, GenerationMetadata</li> </ul>"},{"location":"api/overview/#llm-client-interface","title":"LLM Client Interface","text":"<p>Abstract base class and common types for LLM vendor clients:</p> <ul> <li>BaseLLMClient - Abstract interface all vendor clients implement</li> <li>LLMConfig - Base configuration dataclass</li> <li>LLMResponse - Unified response format</li> </ul>"},{"location":"api/overview/#vendor-api-clients","title":"Vendor API Clients","text":"<p>Direct API clients for specific LLM providers. All implement the <code>BaseLLMClient</code> interface.</p> <ul> <li>Groq Client - Fast inference via Groq API</li> <li>Gemini Client - Google Gemini API</li> <li>OpenAI Client - OpenAI GPT models</li> <li>Anthropic Client - Anthropic Claude models</li> <li>DeepSeek Client - DeepSeek models</li> <li>Mistral Client - Mistral AI models</li> <li>Ollama Client - Local LLMs via Ollama</li> </ul>"},{"location":"api/overview/#cli","title":"CLI","text":"<p>Command-line interface for graph generation and cache management.</p>"},{"location":"api/clients/anthropic/","title":"Anthropic Client API Reference","text":"<p>Direct Anthropic API client for Claude models. This client implements the BaseLLMClient interface using httpx to communicate directly with the Anthropic API.</p>"},{"location":"api/clients/anthropic/#overview","title":"Overview","text":"<p>The Anthropic client provides:</p> <ul> <li>Direct HTTP communication with Anthropic's API</li> <li>Implements the <code>BaseLLMClient</code> abstract interface</li> <li>JSON response parsing with error handling</li> <li>Call counting for usage tracking</li> <li>Configurable timeout and retry settings</li> <li>Proper handling of Anthropic's system prompt format</li> </ul>"},{"location":"api/clients/anthropic/#usage","title":"Usage","text":"<pre><code>from causaliq_knowledge.llm import AnthropicClient, AnthropicConfig\n\n# Create client with custom config\nconfig = AnthropicConfig(\n    model=\"claude-sonnet-4-20250514\",\n    temperature=0.1,\n    max_tokens=500,\n)\nclient = AnthropicClient(config=config)\n\n# Make a completion request\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"What is 2+2?\"},\n]\nresponse = client.completion(messages)\nprint(response.content)\n\n# Parse JSON response\njson_data = response.parse_json()\n</code></pre>"},{"location":"api/clients/anthropic/#environment-variables","title":"Environment Variables","text":"<p>The Anthropic client requires the <code>ANTHROPIC_API_KEY</code> environment variable to be set:</p> <pre><code>export ANTHROPIC_API_KEY=your_api_key_here\n</code></pre>"},{"location":"api/clients/anthropic/#anthropicconfig","title":"AnthropicConfig","text":""},{"location":"api/clients/anthropic/#causaliq_knowledge.llm.anthropic_client.AnthropicConfig","title":"AnthropicConfig  <code>dataclass</code>","text":"<pre><code>AnthropicConfig(\n    model: str = \"claude-sonnet-4-20250514\",\n    temperature: float = 0.1,\n    max_tokens: int = 500,\n    timeout: float = 30.0,\n    api_key: Optional[str] = None,\n)\n</code></pre> <p>Configuration for Anthropic API client.</p> <p>Extends LLMConfig with Anthropic-specific defaults.</p> <p>Attributes:</p> <ul> <li> <code>model</code>               (<code>str</code>)           \u2013            <p>Anthropic model identifier (default: claude-sonnet-4-20250514).</p> </li> <li> <code>temperature</code>               (<code>float</code>)           \u2013            <p>Sampling temperature (default: 0.1).</p> </li> <li> <code>max_tokens</code>               (<code>int</code>)           \u2013            <p>Maximum response tokens (default: 500).</p> </li> <li> <code>timeout</code>               (<code>float</code>)           \u2013            <p>Request timeout in seconds (default: 30.0).</p> </li> <li> <code>api_key</code>               (<code>Optional[str]</code>)           \u2013            <p>Anthropic API key (falls back to ANTHROPIC_API_KEY env var).</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>__post_init__</code>             \u2013              <p>Set API key from environment if not provided.</p> </li> </ul>"},{"location":"api/clients/anthropic/#causaliq_knowledge.llm.anthropic_client.AnthropicConfig.api_key","title":"api_key  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>api_key: Optional[str] = None\n</code></pre>"},{"location":"api/clients/anthropic/#causaliq_knowledge.llm.anthropic_client.AnthropicConfig.max_tokens","title":"max_tokens  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_tokens: int = 500\n</code></pre>"},{"location":"api/clients/anthropic/#causaliq_knowledge.llm.anthropic_client.AnthropicConfig.model","title":"model  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model: str = 'claude-sonnet-4-20250514'\n</code></pre>"},{"location":"api/clients/anthropic/#causaliq_knowledge.llm.anthropic_client.AnthropicConfig.temperature","title":"temperature  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>temperature: float = 0.1\n</code></pre>"},{"location":"api/clients/anthropic/#causaliq_knowledge.llm.anthropic_client.AnthropicConfig.timeout","title":"timeout  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>timeout: float = 30.0\n</code></pre>"},{"location":"api/clients/anthropic/#causaliq_knowledge.llm.anthropic_client.AnthropicConfig.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__() -&gt; None\n</code></pre> <p>Set API key from environment if not provided.</p>"},{"location":"api/clients/anthropic/#anthropicclient","title":"AnthropicClient","text":""},{"location":"api/clients/anthropic/#causaliq_knowledge.llm.anthropic_client.AnthropicClient","title":"AnthropicClient","text":"<pre><code>AnthropicClient(config: Optional[AnthropicConfig] = None)\n</code></pre> <p>Direct Anthropic API client.</p> <p>Implements the BaseLLMClient interface for Anthropic's Claude API. Uses httpx for HTTP requests.</p> Example <p>config = AnthropicConfig(model=\"claude-sonnet-4-20250514\") client = AnthropicClient(config) msgs = [{\"role\": \"user\", \"content\": \"Hello\"}] response = client.completion(msgs) print(response.content)</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>_build_cache_key</code>             \u2013              <p>Build a deterministic cache key for the request.</p> </li> <li> <code>cached_completion</code>             \u2013              <p>Make a completion request with caching.</p> </li> <li> <code>complete_json</code>             \u2013              <p>Make a completion request and parse response as JSON.</p> </li> <li> <code>completion</code>             \u2013              <p>Make a chat completion request to Anthropic.</p> </li> <li> <code>is_available</code>             \u2013              <p>Check if Anthropic API is available.</p> </li> <li> <code>list_models</code>             \u2013              <p>List available Claude models from Anthropic API.</p> </li> <li> <code>set_cache</code>             \u2013              <p>Configure caching for this client.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>API_VERSION</code>           \u2013            </li> <li> <code>BASE_URL</code>           \u2013            </li> <li> <code>_total_calls</code>           \u2013            </li> <li> <code>cache</code>               (<code>Optional['TokenCache']</code>)           \u2013            <p>Return the configured cache, if any.</p> </li> <li> <code>call_count</code>               (<code>int</code>)           \u2013            <p>Return the number of API calls made.</p> </li> <li> <code>config</code>           \u2013            </li> <li> <code>model_name</code>               (<code>str</code>)           \u2013            <p>Return the model name being used.</p> </li> <li> <code>provider_name</code>               (<code>str</code>)           \u2013            <p>Return the provider name.</p> </li> <li> <code>use_cache</code>               (<code>bool</code>)           \u2013            <p>Return whether caching is enabled.</p> </li> </ul>"},{"location":"api/clients/anthropic/#causaliq_knowledge.llm.anthropic_client.AnthropicClient(config)","title":"<code>config</code>","text":"(<code>Optional[AnthropicConfig]</code>, default:                   <code>None</code> )           \u2013            <p>Anthropic configuration. If None, uses defaults with    API key from ANTHROPIC_API_KEY environment variable.</p>"},{"location":"api/clients/anthropic/#causaliq_knowledge.llm.anthropic_client.AnthropicClient.API_VERSION","title":"API_VERSION  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>API_VERSION = '2023-06-01'\n</code></pre>"},{"location":"api/clients/anthropic/#causaliq_knowledge.llm.anthropic_client.AnthropicClient.BASE_URL","title":"BASE_URL  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>BASE_URL = 'https://api.anthropic.com/v1'\n</code></pre>"},{"location":"api/clients/anthropic/#causaliq_knowledge.llm.anthropic_client.AnthropicClient._total_calls","title":"_total_calls  <code>instance-attribute</code>","text":"<pre><code>_total_calls = 0\n</code></pre>"},{"location":"api/clients/anthropic/#causaliq_knowledge.llm.anthropic_client.AnthropicClient.cache","title":"cache  <code>property</code>","text":"<pre><code>cache: Optional['TokenCache']\n</code></pre> <p>Return the configured cache, if any.</p>"},{"location":"api/clients/anthropic/#causaliq_knowledge.llm.anthropic_client.AnthropicClient.call_count","title":"call_count  <code>property</code>","text":"<pre><code>call_count: int\n</code></pre> <p>Return the number of API calls made.</p>"},{"location":"api/clients/anthropic/#causaliq_knowledge.llm.anthropic_client.AnthropicClient.config","title":"config  <code>instance-attribute</code>","text":"<pre><code>config = config or AnthropicConfig()\n</code></pre>"},{"location":"api/clients/anthropic/#causaliq_knowledge.llm.anthropic_client.AnthropicClient.model_name","title":"model_name  <code>property</code>","text":"<pre><code>model_name: str\n</code></pre> <p>Return the model name being used.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>Model identifier string.</p> </li> </ul>"},{"location":"api/clients/anthropic/#causaliq_knowledge.llm.anthropic_client.AnthropicClient.provider_name","title":"provider_name  <code>property</code>","text":"<pre><code>provider_name: str\n</code></pre> <p>Return the provider name.</p>"},{"location":"api/clients/anthropic/#causaliq_knowledge.llm.anthropic_client.AnthropicClient.use_cache","title":"use_cache  <code>property</code>","text":"<pre><code>use_cache: bool\n</code></pre> <p>Return whether caching is enabled.</p>"},{"location":"api/clients/anthropic/#causaliq_knowledge.llm.anthropic_client.AnthropicClient._build_cache_key","title":"_build_cache_key","text":"<pre><code>_build_cache_key(\n    messages: List[Dict[str, str]],\n    temperature: Optional[float] = None,\n    max_tokens: Optional[int] = None,\n) -&gt; str\n</code></pre> <p>Build a deterministic cache key for the request.</p> <p>Creates a SHA-256 hash from the model, messages, temperature, and max_tokens. The hash is truncated to 16 hex characters (64 bits).</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>16-character hex string cache key.</p> </li> </ul>"},{"location":"api/clients/anthropic/#causaliq_knowledge.llm.anthropic_client.AnthropicClient._build_cache_key(messages)","title":"<code>messages</code>","text":"(<code>List[Dict[str, str]]</code>)           \u2013            <p>List of message dicts with \"role\" and \"content\" keys.</p>"},{"location":"api/clients/anthropic/#causaliq_knowledge.llm.anthropic_client.AnthropicClient._build_cache_key(temperature)","title":"<code>temperature</code>","text":"(<code>Optional[float]</code>, default:                   <code>None</code> )           \u2013            <p>Sampling temperature (defaults to config value).</p>"},{"location":"api/clients/anthropic/#causaliq_knowledge.llm.anthropic_client.AnthropicClient._build_cache_key(max_tokens)","title":"<code>max_tokens</code>","text":"(<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>Maximum tokens (defaults to config value).</p>"},{"location":"api/clients/anthropic/#causaliq_knowledge.llm.anthropic_client.AnthropicClient.cached_completion","title":"cached_completion","text":"<pre><code>cached_completion(messages: List[Dict[str, str]], **kwargs: Any) -&gt; LLMResponse\n</code></pre> <p>Make a completion request with caching.</p> <p>If caching is enabled and a cached response exists, returns the cached response without making an API call. Otherwise, makes the API call and caches the result.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>LLMResponse</code>           \u2013            <p>LLMResponse with the generated content and metadata.</p> </li> </ul>"},{"location":"api/clients/anthropic/#causaliq_knowledge.llm.anthropic_client.AnthropicClient.cached_completion(messages)","title":"<code>messages</code>","text":"(<code>List[Dict[str, str]]</code>)           \u2013            <p>List of message dicts with \"role\" and \"content\" keys.</p>"},{"location":"api/clients/anthropic/#causaliq_knowledge.llm.anthropic_client.AnthropicClient.cached_completion(**kwargs)","title":"<code>**kwargs</code>","text":"(<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Provider-specific options (temperature, max_tokens, etc.) Also accepts request_id (str) for identifying requests in exports. Note: request_id is NOT part of the cache key.</p>"},{"location":"api/clients/anthropic/#causaliq_knowledge.llm.anthropic_client.AnthropicClient.complete_json","title":"complete_json","text":"<pre><code>complete_json(\n    messages: List[Dict[str, str]], **kwargs: Any\n) -&gt; tuple[Optional[Dict[str, Any]], LLMResponse]\n</code></pre> <p>Make a completion request and parse response as JSON.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>tuple[Optional[Dict[str, Any]], LLMResponse]</code>           \u2013            <p>Tuple of (parsed JSON dict or None, raw LLMResponse).</p> </li> </ul>"},{"location":"api/clients/anthropic/#causaliq_knowledge.llm.anthropic_client.AnthropicClient.complete_json(messages)","title":"<code>messages</code>","text":"(<code>List[Dict[str, str]]</code>)           \u2013            <p>List of message dicts with \"role\" and \"content\" keys.</p>"},{"location":"api/clients/anthropic/#causaliq_knowledge.llm.anthropic_client.AnthropicClient.complete_json(**kwargs)","title":"<code>**kwargs</code>","text":"(<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Override config options passed to completion().</p>"},{"location":"api/clients/anthropic/#causaliq_knowledge.llm.anthropic_client.AnthropicClient.completion","title":"completion","text":"<pre><code>completion(messages: List[Dict[str, str]], **kwargs: Any) -&gt; LLMResponse\n</code></pre> <p>Make a chat completion request to Anthropic.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>LLMResponse</code>           \u2013            <p>LLMResponse with the generated content and metadata.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the API request fails.</p> </li> </ul>"},{"location":"api/clients/anthropic/#causaliq_knowledge.llm.anthropic_client.AnthropicClient.completion(messages)","title":"<code>messages</code>","text":"(<code>List[Dict[str, str]]</code>)           \u2013            <p>List of message dicts with \"role\" and \"content\" keys.</p>"},{"location":"api/clients/anthropic/#causaliq_knowledge.llm.anthropic_client.AnthropicClient.completion(**kwargs)","title":"<code>**kwargs</code>","text":"(<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Override config options (temperature, max_tokens).</p>"},{"location":"api/clients/anthropic/#causaliq_knowledge.llm.anthropic_client.AnthropicClient.is_available","title":"is_available","text":"<pre><code>is_available() -&gt; bool\n</code></pre> <p>Check if Anthropic API is available.</p> <p>Returns:</p> <ul> <li> <code>bool</code>           \u2013            <p>True if ANTHROPIC_API_KEY is configured.</p> </li> </ul>"},{"location":"api/clients/anthropic/#causaliq_knowledge.llm.anthropic_client.AnthropicClient.list_models","title":"list_models","text":"<pre><code>list_models() -&gt; List[str]\n</code></pre> <p>List available Claude models from Anthropic API.</p> <p>Queries the Anthropic /v1/models endpoint to get available models.</p> <p>Returns:</p> <ul> <li> <code>List[str]</code>           \u2013            <p>List of model identifiers</p> </li> <li> <code>List[str]</code>           \u2013            <p>(e.g., ['claude-sonnet-4-20250514', ...]).</p> </li> </ul>"},{"location":"api/clients/anthropic/#causaliq_knowledge.llm.anthropic_client.AnthropicClient.set_cache","title":"set_cache","text":"<pre><code>set_cache(cache: Optional['TokenCache'], use_cache: bool = True) -&gt; None\n</code></pre> <p>Configure caching for this client.</p> <p>Parameters:</p>"},{"location":"api/clients/anthropic/#causaliq_knowledge.llm.anthropic_client.AnthropicClient.set_cache(cache)","title":"<code>cache</code>","text":"(<code>Optional['TokenCache']</code>)           \u2013            <p>TokenCache instance for caching, or None to disable.</p>"},{"location":"api/clients/anthropic/#causaliq_knowledge.llm.anthropic_client.AnthropicClient.set_cache(use_cache)","title":"<code>use_cache</code>","text":"(<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use the cache (default True).</p>"},{"location":"api/clients/anthropic/#supported-models","title":"Supported Models","text":"<p>Anthropic provides the Claude family of models:</p> Model Description Free Tier <code>claude-sonnet-4-20250514</code> Claude Sonnet 4 - balanced performance \u274c No <code>claude-opus-4-20250514</code> Claude Opus 4 - highest capability \u274c No <code>claude-3-5-haiku-latest</code> Claude 3.5 Haiku - fast and efficient \u274c No <p>See Anthropic documentation for the full list of available models.</p>"},{"location":"api/clients/deepseek/","title":"DeepSeek Client","text":"<p>Direct DeepSeek API client for DeepSeek-V3 and DeepSeek-R1 models.</p>"},{"location":"api/clients/deepseek/#overview","title":"Overview","text":"<p>DeepSeek is a Chinese AI company known for highly capable models at competitive prices. Their API is OpenAI-compatible, making integration straightforward.</p> <p>Key features:</p> <ul> <li>DeepSeek-V3: General purpose chat model, excellent performance</li> <li>DeepSeek-R1: Advanced reasoning model, rivals OpenAI o1 at much lower cost</li> <li>Very competitive pricing (~$0.14/1M input for chat)</li> <li>OpenAI-compatible API</li> </ul>"},{"location":"api/clients/deepseek/#configuration","title":"Configuration","text":"<p>The client requires a <code>DEEPSEEK_API_KEY</code> environment variable:</p> <pre><code># Linux/macOS\nexport DEEPSEEK_API_KEY=\"your-api-key\"\n\n# Windows PowerShell\n$env:DEEPSEEK_API_KEY=\"your-api-key\"\n\n# Windows cmd\nset DEEPSEEK_API_KEY=your-api-key\n</code></pre> <p>Get your API key from: https://platform.deepseek.com</p>"},{"location":"api/clients/deepseek/#usage","title":"Usage","text":""},{"location":"api/clients/deepseek/#basic-usage","title":"Basic Usage","text":"<pre><code>from causaliq_knowledge.llm import DeepSeekClient, DeepSeekConfig\n\n# Default config (uses DEEPSEEK_API_KEY env var)\nclient = DeepSeekClient()\n\n# Or with custom config\nconfig = DeepSeekConfig(\n    model=\"deepseek-chat\",\n    temperature=0.1,\n    max_tokens=500,\n    timeout=30.0,\n)\nclient = DeepSeekClient(config)\n\n# Make a completion request\nmessages = [{\"role\": \"user\", \"content\": \"What is 2 + 2?\"}]\nresponse = client.completion(messages)\nprint(response.content)\n</code></pre>"},{"location":"api/clients/deepseek/#using-with-cli","title":"Using with CLI","text":"<pre><code># Query with DeepSeek\ncqknow query smoking lung_cancer --model deepseek/deepseek-chat\n\n# Use reasoning model for complex queries\ncqknow query income education --model deepseek/deepseek-reasoner --domain economics\n\n# List available DeepSeek models\ncqknow models deepseek\n</code></pre>"},{"location":"api/clients/deepseek/#using-with-llmknowledge-provider","title":"Using with LLMKnowledge Provider","text":"<pre><code>from causaliq_knowledge.llm import LLMKnowledge\n\n# Single model\nprovider = LLMKnowledge(models=[\"deepseek/deepseek-chat\"])\nresult = provider.query_edge(\"smoking\", \"lung_cancer\")\n\n# Multi-model consensus\nprovider = LLMKnowledge(\n    models=[\n        \"deepseek/deepseek-chat\",\n        \"groq/llama-3.1-8b-instant\",\n    ],\n    consensus_strategy=\"weighted_vote\",\n)\n</code></pre>"},{"location":"api/clients/deepseek/#available-models","title":"Available Models","text":"Model Description Best For <code>deepseek-chat</code> DeepSeek-V3 general purpose Fast, general queries <code>deepseek-reasoner</code> DeepSeek-R1 reasoning model Complex reasoning tasks"},{"location":"api/clients/deepseek/#pricing","title":"Pricing","text":"<p>DeepSeek offers very competitive pricing (as of Jan 2025):</p> Model Input (per 1M tokens) Output (per 1M tokens) deepseek-chat $0.14 $0.28 deepseek-reasoner $0.55 $2.19 <p>Note: Cache hits are even cheaper. See DeepSeek pricing for details.</p>"},{"location":"api/clients/deepseek/#api-reference","title":"API Reference","text":""},{"location":"api/clients/deepseek/#causaliq_knowledge.llm.deepseek_client.DeepSeekConfig","title":"DeepSeekConfig  <code>dataclass</code>","text":"<pre><code>DeepSeekConfig(\n    model: str = \"deepseek-chat\",\n    temperature: float = 0.1,\n    max_tokens: int = 500,\n    timeout: float = 30.0,\n    api_key: Optional[str] = None,\n)\n</code></pre> <p>Configuration for DeepSeek API client.</p> <p>Extends OpenAICompatConfig with DeepSeek-specific defaults.</p> <p>Attributes:</p> <ul> <li> <code>model</code>               (<code>str</code>)           \u2013            <p>DeepSeek model identifier (default: deepseek-chat).</p> </li> <li> <code>temperature</code>               (<code>float</code>)           \u2013            <p>Sampling temperature (default: 0.1).</p> </li> <li> <code>max_tokens</code>               (<code>int</code>)           \u2013            <p>Maximum response tokens (default: 500).</p> </li> <li> <code>timeout</code>               (<code>float</code>)           \u2013            <p>Request timeout in seconds (default: 30.0).</p> </li> <li> <code>api_key</code>               (<code>Optional[str]</code>)           \u2013            <p>DeepSeek API key (falls back to DEEPSEEK_API_KEY env var).</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>__post_init__</code>             \u2013              <p>Set API key from environment if not provided.</p> </li> </ul>"},{"location":"api/clients/deepseek/#causaliq_knowledge.llm.deepseek_client.DeepSeekConfig.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__() -&gt; None\n</code></pre> <p>Set API key from environment if not provided.</p>"},{"location":"api/clients/deepseek/#causaliq_knowledge.llm.deepseek_client.DeepSeekClient","title":"DeepSeekClient","text":"<pre><code>DeepSeekClient(config: Optional[DeepSeekConfig] = None)\n</code></pre> <p>Direct DeepSeek API client.</p> <p>DeepSeek uses an OpenAI-compatible API, making integration straightforward. Known for excellent reasoning capabilities (R1) at low cost.</p> Available models <ul> <li>deepseek-chat: General purpose (DeepSeek-V3)</li> <li>deepseek-reasoner: Advanced reasoning (DeepSeek-R1)</li> </ul> Example <p>config = DeepSeekConfig(model=\"deepseek-chat\") client = DeepSeekClient(config) msgs = [{\"role\": \"user\", \"content\": \"Hello\"}] response = client.completion(msgs) print(response.content)</p> <p>Parameters:</p> <ul> <li> </li> </ul> <p>Methods:</p> <ul> <li> <code>cached_completion</code>             \u2013              <p>Make a completion request with caching.</p> </li> <li> <code>complete_json</code>             \u2013              <p>Make a completion request and parse response as JSON.</p> </li> <li> <code>completion</code>             \u2013              <p>Make a chat completion request.</p> </li> <li> <code>is_available</code>             \u2013              <p>Check if the API is available.</p> </li> <li> <code>list_models</code>             \u2013              <p>List available models from the API.</p> </li> <li> <code>set_cache</code>             \u2013              <p>Configure caching for this client.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>cache</code>               (<code>Optional['TokenCache']</code>)           \u2013            <p>Return the configured cache, if any.</p> </li> <li> <code>call_count</code>               (<code>int</code>)           \u2013            <p>Return the number of API calls made.</p> </li> <li> <code>model_name</code>               (<code>str</code>)           \u2013            <p>Return the model name being used.</p> </li> <li> <code>provider_name</code>               (<code>str</code>)           \u2013            <p>Return the provider name.</p> </li> <li> <code>use_cache</code>               (<code>bool</code>)           \u2013            <p>Return whether caching is enabled.</p> </li> </ul>"},{"location":"api/clients/deepseek/#causaliq_knowledge.llm.deepseek_client.DeepSeekClient(config)","title":"<code>config</code>","text":"(<code>Optional[DeepSeekConfig]</code>, default:                   <code>None</code> )           \u2013            <p>DeepSeek configuration. If None, uses defaults with    API key from DEEPSEEK_API_KEY environment variable.</p>"},{"location":"api/clients/deepseek/#causaliq_knowledge.llm.deepseek_client.DeepSeekClient.cache","title":"cache  <code>property</code>","text":"<pre><code>cache: Optional['TokenCache']\n</code></pre> <p>Return the configured cache, if any.</p>"},{"location":"api/clients/deepseek/#causaliq_knowledge.llm.deepseek_client.DeepSeekClient.call_count","title":"call_count  <code>property</code>","text":"<pre><code>call_count: int\n</code></pre> <p>Return the number of API calls made.</p>"},{"location":"api/clients/deepseek/#causaliq_knowledge.llm.deepseek_client.DeepSeekClient.model_name","title":"model_name  <code>property</code>","text":"<pre><code>model_name: str\n</code></pre> <p>Return the model name being used.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>Model identifier string.</p> </li> </ul>"},{"location":"api/clients/deepseek/#causaliq_knowledge.llm.deepseek_client.DeepSeekClient.provider_name","title":"provider_name  <code>property</code>","text":"<pre><code>provider_name: str\n</code></pre> <p>Return the provider name.</p>"},{"location":"api/clients/deepseek/#causaliq_knowledge.llm.deepseek_client.DeepSeekClient.use_cache","title":"use_cache  <code>property</code>","text":"<pre><code>use_cache: bool\n</code></pre> <p>Return whether caching is enabled.</p>"},{"location":"api/clients/deepseek/#causaliq_knowledge.llm.deepseek_client.DeepSeekClient.cached_completion","title":"cached_completion","text":"<pre><code>cached_completion(messages: List[Dict[str, str]], **kwargs: Any) -&gt; LLMResponse\n</code></pre> <p>Make a completion request with caching.</p> <p>If caching is enabled and a cached response exists, returns the cached response without making an API call. Otherwise, makes the API call and caches the result.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>LLMResponse</code>           \u2013            <p>LLMResponse with the generated content and metadata.</p> </li> </ul>"},{"location":"api/clients/deepseek/#causaliq_knowledge.llm.deepseek_client.DeepSeekClient.cached_completion(messages)","title":"<code>messages</code>","text":"(<code>List[Dict[str, str]]</code>)           \u2013            <p>List of message dicts with \"role\" and \"content\" keys.</p>"},{"location":"api/clients/deepseek/#causaliq_knowledge.llm.deepseek_client.DeepSeekClient.cached_completion(**kwargs)","title":"<code>**kwargs</code>","text":"(<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Provider-specific options (temperature, max_tokens, etc.) Also accepts request_id (str) for identifying requests in exports. Note: request_id is NOT part of the cache key.</p>"},{"location":"api/clients/deepseek/#causaliq_knowledge.llm.deepseek_client.DeepSeekClient.complete_json","title":"complete_json","text":"<pre><code>complete_json(\n    messages: List[Dict[str, str]], **kwargs: Any\n) -&gt; tuple[Optional[Dict[str, Any]], LLMResponse]\n</code></pre> <p>Make a completion request and parse response as JSON.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>tuple[Optional[Dict[str, Any]], LLMResponse]</code>           \u2013            <p>Tuple of (parsed JSON dict or None, raw LLMResponse).</p> </li> </ul>"},{"location":"api/clients/deepseek/#causaliq_knowledge.llm.deepseek_client.DeepSeekClient.complete_json(messages)","title":"<code>messages</code>","text":"(<code>List[Dict[str, str]]</code>)           \u2013            <p>List of message dicts with \"role\" and \"content\" keys.</p>"},{"location":"api/clients/deepseek/#causaliq_knowledge.llm.deepseek_client.DeepSeekClient.complete_json(**kwargs)","title":"<code>**kwargs</code>","text":"(<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Override config options passed to completion().</p>"},{"location":"api/clients/deepseek/#causaliq_knowledge.llm.deepseek_client.DeepSeekClient.completion","title":"completion","text":"<pre><code>completion(messages: List[Dict[str, str]], **kwargs: Any) -&gt; LLMResponse\n</code></pre> <p>Make a chat completion request.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>LLMResponse</code>           \u2013            <p>LLMResponse with the generated content and metadata.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the API request fails.</p> </li> </ul>"},{"location":"api/clients/deepseek/#causaliq_knowledge.llm.deepseek_client.DeepSeekClient.completion(messages)","title":"<code>messages</code>","text":"(<code>List[Dict[str, str]]</code>)           \u2013            <p>List of message dicts with \"role\" and \"content\" keys.</p>"},{"location":"api/clients/deepseek/#causaliq_knowledge.llm.deepseek_client.DeepSeekClient.completion(**kwargs)","title":"<code>**kwargs</code>","text":"(<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Override config options (temperature, max_tokens).</p>"},{"location":"api/clients/deepseek/#causaliq_knowledge.llm.deepseek_client.DeepSeekClient.is_available","title":"is_available","text":"<pre><code>is_available() -&gt; bool\n</code></pre> <p>Check if the API is available.</p> <p>Returns:</p> <ul> <li> <code>bool</code>           \u2013            <p>True if API key is configured.</p> </li> </ul>"},{"location":"api/clients/deepseek/#causaliq_knowledge.llm.deepseek_client.DeepSeekClient.list_models","title":"list_models","text":"<pre><code>list_models() -&gt; List[str]\n</code></pre> <p>List available models from the API.</p> <p>Queries the API to get models accessible with the current API key, then filters using _filter_models().</p> <p>Returns:</p> <ul> <li> <code>List[str]</code>           \u2013            <p>List of model identifiers.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the API request fails.</p> </li> </ul>"},{"location":"api/clients/deepseek/#causaliq_knowledge.llm.deepseek_client.DeepSeekClient.set_cache","title":"set_cache","text":"<pre><code>set_cache(cache: Optional['TokenCache'], use_cache: bool = True) -&gt; None\n</code></pre> <p>Configure caching for this client.</p> <p>Parameters:</p>"},{"location":"api/clients/deepseek/#causaliq_knowledge.llm.deepseek_client.DeepSeekClient.set_cache(cache)","title":"<code>cache</code>","text":"(<code>Optional['TokenCache']</code>)           \u2013            <p>TokenCache instance for caching, or None to disable.</p>"},{"location":"api/clients/deepseek/#causaliq_knowledge.llm.deepseek_client.DeepSeekClient.set_cache(use_cache)","title":"<code>use_cache</code>","text":"(<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use the cache (default True).</p>"},{"location":"api/clients/gemini/","title":"Gemini Client API Reference","text":"<p>Direct Google Gemini API client. This client implements the BaseLLMClient interface using httpx to communicate directly with Google's Generative Language API.</p>"},{"location":"api/clients/gemini/#overview","title":"Overview","text":"<p>The Gemini client provides:</p> <ul> <li>Direct HTTP communication with Google's Generative Language API</li> <li>Implements the <code>BaseLLMClient</code> abstract interface</li> <li>Automatic conversion from OpenAI-style messages to Gemini format</li> <li>JSON response parsing with error handling</li> <li>Call counting for usage tracking</li> <li>Configurable timeout settings</li> </ul>"},{"location":"api/clients/gemini/#usage","title":"Usage","text":"<pre><code>from causaliq_knowledge.llm import GeminiClient, GeminiConfig\n\n# Create client with custom config\nconfig = GeminiConfig(\n    model=\"gemini-2.5-flash\",\n    temperature=0.1,\n    max_tokens=500,\n)\nclient = GeminiClient(config=config)\n\n# Make a completion request (OpenAI-style messages)\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"What is 2+2?\"},\n]\nresponse = client.completion(messages)\nprint(response.content)\n\n# Parse JSON response\njson_data = response.parse_json()\n</code></pre>"},{"location":"api/clients/gemini/#environment-variables","title":"Environment Variables","text":"<p>The Gemini client requires the <code>GEMINI_API_KEY</code> environment variable to be set:</p> <pre><code>export GEMINI_API_KEY=your_api_key_here\n</code></pre>"},{"location":"api/clients/gemini/#geminiconfig","title":"GeminiConfig","text":""},{"location":"api/clients/gemini/#causaliq_knowledge.llm.gemini_client.GeminiConfig","title":"GeminiConfig  <code>dataclass</code>","text":"<pre><code>GeminiConfig(\n    model: str = \"gemini-2.5-flash\",\n    temperature: float = 0.1,\n    max_tokens: int = 500,\n    timeout: float = 30.0,\n    api_key: Optional[str] = None,\n)\n</code></pre> <p>Configuration for Gemini API client.</p> <p>Extends LLMConfig with Gemini-specific defaults.</p> <p>Attributes:</p> <ul> <li> <code>model</code>               (<code>str</code>)           \u2013            <p>Gemini model identifier (default: gemini-2.5-flash).</p> </li> <li> <code>temperature</code>               (<code>float</code>)           \u2013            <p>Sampling temperature (default: 0.1).</p> </li> <li> <code>max_tokens</code>               (<code>int</code>)           \u2013            <p>Maximum response tokens (default: 500).</p> </li> <li> <code>timeout</code>               (<code>float</code>)           \u2013            <p>Request timeout in seconds (default: 30.0).</p> </li> <li> <code>api_key</code>               (<code>Optional[str]</code>)           \u2013            <p>Gemini API key (falls back to GEMINI_API_KEY env var).</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>__post_init__</code>             \u2013              <p>Set API key from environment if not provided.</p> </li> </ul>"},{"location":"api/clients/gemini/#causaliq_knowledge.llm.gemini_client.GeminiConfig.api_key","title":"api_key  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>api_key: Optional[str] = None\n</code></pre>"},{"location":"api/clients/gemini/#causaliq_knowledge.llm.gemini_client.GeminiConfig.max_tokens","title":"max_tokens  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_tokens: int = 500\n</code></pre>"},{"location":"api/clients/gemini/#causaliq_knowledge.llm.gemini_client.GeminiConfig.model","title":"model  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model: str = 'gemini-2.5-flash'\n</code></pre>"},{"location":"api/clients/gemini/#causaliq_knowledge.llm.gemini_client.GeminiConfig.temperature","title":"temperature  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>temperature: float = 0.1\n</code></pre>"},{"location":"api/clients/gemini/#causaliq_knowledge.llm.gemini_client.GeminiConfig.timeout","title":"timeout  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>timeout: float = 30.0\n</code></pre>"},{"location":"api/clients/gemini/#causaliq_knowledge.llm.gemini_client.GeminiConfig.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__() -&gt; None\n</code></pre> <p>Set API key from environment if not provided.</p>"},{"location":"api/clients/gemini/#geminiclient","title":"GeminiClient","text":""},{"location":"api/clients/gemini/#causaliq_knowledge.llm.gemini_client.GeminiClient","title":"GeminiClient","text":"<pre><code>GeminiClient(config: Optional[GeminiConfig] = None)\n</code></pre> <p>Direct Gemini API client.</p> <p>Implements the BaseLLMClient interface for Google's Gemini API. Uses httpx for HTTP requests.</p> Example <p>config = GeminiConfig(model=\"gemini-2.5-flash\") client = GeminiClient(config) msgs = [{\"role\": \"user\", \"content\": \"Hello\"}] response = client.completion(msgs) print(response.content)</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>_build_cache_key</code>             \u2013              <p>Build a deterministic cache key for the request.</p> </li> <li> <code>cached_completion</code>             \u2013              <p>Make a completion request with caching.</p> </li> <li> <code>complete_json</code>             \u2013              <p>Make a completion request and parse response as JSON.</p> </li> <li> <code>completion</code>             \u2013              <p>Make a chat completion request to Gemini.</p> </li> <li> <code>is_available</code>             \u2013              <p>Check if Gemini API is available.</p> </li> <li> <code>list_models</code>             \u2013              <p>List available models from Gemini API.</p> </li> <li> <code>set_cache</code>             \u2013              <p>Configure caching for this client.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>BASE_URL</code>           \u2013            </li> <li> <code>_total_calls</code>           \u2013            </li> <li> <code>cache</code>               (<code>Optional['TokenCache']</code>)           \u2013            <p>Return the configured cache, if any.</p> </li> <li> <code>call_count</code>               (<code>int</code>)           \u2013            <p>Return the number of API calls made.</p> </li> <li> <code>config</code>           \u2013            </li> <li> <code>model_name</code>               (<code>str</code>)           \u2013            <p>Return the model name being used.</p> </li> <li> <code>provider_name</code>               (<code>str</code>)           \u2013            <p>Return the provider name.</p> </li> <li> <code>use_cache</code>               (<code>bool</code>)           \u2013            <p>Return whether caching is enabled.</p> </li> </ul>"},{"location":"api/clients/gemini/#causaliq_knowledge.llm.gemini_client.GeminiClient(config)","title":"<code>config</code>","text":"(<code>Optional[GeminiConfig]</code>, default:                   <code>None</code> )           \u2013            <p>Gemini configuration. If None, uses defaults with    API key from GEMINI_API_KEY environment variable.</p>"},{"location":"api/clients/gemini/#causaliq_knowledge.llm.gemini_client.GeminiClient.BASE_URL","title":"BASE_URL  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>BASE_URL = 'https://generativelanguage.googleapis.com/v1beta/models'\n</code></pre>"},{"location":"api/clients/gemini/#causaliq_knowledge.llm.gemini_client.GeminiClient._total_calls","title":"_total_calls  <code>instance-attribute</code>","text":"<pre><code>_total_calls = 0\n</code></pre>"},{"location":"api/clients/gemini/#causaliq_knowledge.llm.gemini_client.GeminiClient.cache","title":"cache  <code>property</code>","text":"<pre><code>cache: Optional['TokenCache']\n</code></pre> <p>Return the configured cache, if any.</p>"},{"location":"api/clients/gemini/#causaliq_knowledge.llm.gemini_client.GeminiClient.call_count","title":"call_count  <code>property</code>","text":"<pre><code>call_count: int\n</code></pre> <p>Return the number of API calls made.</p>"},{"location":"api/clients/gemini/#causaliq_knowledge.llm.gemini_client.GeminiClient.config","title":"config  <code>instance-attribute</code>","text":"<pre><code>config = config or GeminiConfig()\n</code></pre>"},{"location":"api/clients/gemini/#causaliq_knowledge.llm.gemini_client.GeminiClient.model_name","title":"model_name  <code>property</code>","text":"<pre><code>model_name: str\n</code></pre> <p>Return the model name being used.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>Model identifier string.</p> </li> </ul>"},{"location":"api/clients/gemini/#causaliq_knowledge.llm.gemini_client.GeminiClient.provider_name","title":"provider_name  <code>property</code>","text":"<pre><code>provider_name: str\n</code></pre> <p>Return the provider name.</p>"},{"location":"api/clients/gemini/#causaliq_knowledge.llm.gemini_client.GeminiClient.use_cache","title":"use_cache  <code>property</code>","text":"<pre><code>use_cache: bool\n</code></pre> <p>Return whether caching is enabled.</p>"},{"location":"api/clients/gemini/#causaliq_knowledge.llm.gemini_client.GeminiClient._build_cache_key","title":"_build_cache_key","text":"<pre><code>_build_cache_key(\n    messages: List[Dict[str, str]],\n    temperature: Optional[float] = None,\n    max_tokens: Optional[int] = None,\n) -&gt; str\n</code></pre> <p>Build a deterministic cache key for the request.</p> <p>Creates a SHA-256 hash from the model, messages, temperature, and max_tokens. The hash is truncated to 16 hex characters (64 bits).</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>16-character hex string cache key.</p> </li> </ul>"},{"location":"api/clients/gemini/#causaliq_knowledge.llm.gemini_client.GeminiClient._build_cache_key(messages)","title":"<code>messages</code>","text":"(<code>List[Dict[str, str]]</code>)           \u2013            <p>List of message dicts with \"role\" and \"content\" keys.</p>"},{"location":"api/clients/gemini/#causaliq_knowledge.llm.gemini_client.GeminiClient._build_cache_key(temperature)","title":"<code>temperature</code>","text":"(<code>Optional[float]</code>, default:                   <code>None</code> )           \u2013            <p>Sampling temperature (defaults to config value).</p>"},{"location":"api/clients/gemini/#causaliq_knowledge.llm.gemini_client.GeminiClient._build_cache_key(max_tokens)","title":"<code>max_tokens</code>","text":"(<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>Maximum tokens (defaults to config value).</p>"},{"location":"api/clients/gemini/#causaliq_knowledge.llm.gemini_client.GeminiClient.cached_completion","title":"cached_completion","text":"<pre><code>cached_completion(messages: List[Dict[str, str]], **kwargs: Any) -&gt; LLMResponse\n</code></pre> <p>Make a completion request with caching.</p> <p>If caching is enabled and a cached response exists, returns the cached response without making an API call. Otherwise, makes the API call and caches the result.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>LLMResponse</code>           \u2013            <p>LLMResponse with the generated content and metadata.</p> </li> </ul>"},{"location":"api/clients/gemini/#causaliq_knowledge.llm.gemini_client.GeminiClient.cached_completion(messages)","title":"<code>messages</code>","text":"(<code>List[Dict[str, str]]</code>)           \u2013            <p>List of message dicts with \"role\" and \"content\" keys.</p>"},{"location":"api/clients/gemini/#causaliq_knowledge.llm.gemini_client.GeminiClient.cached_completion(**kwargs)","title":"<code>**kwargs</code>","text":"(<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Provider-specific options (temperature, max_tokens, etc.) Also accepts request_id (str) for identifying requests in exports. Note: request_id is NOT part of the cache key.</p>"},{"location":"api/clients/gemini/#causaliq_knowledge.llm.gemini_client.GeminiClient.complete_json","title":"complete_json","text":"<pre><code>complete_json(\n    messages: List[Dict[str, str]], **kwargs: Any\n) -&gt; tuple[Optional[Dict[str, Any]], LLMResponse]\n</code></pre> <p>Make a completion request and parse response as JSON.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>tuple[Optional[Dict[str, Any]], LLMResponse]</code>           \u2013            <p>Tuple of (parsed JSON dict or None, raw LLMResponse).</p> </li> </ul>"},{"location":"api/clients/gemini/#causaliq_knowledge.llm.gemini_client.GeminiClient.complete_json(messages)","title":"<code>messages</code>","text":"(<code>List[Dict[str, str]]</code>)           \u2013            <p>List of message dicts with \"role\" and \"content\" keys.</p>"},{"location":"api/clients/gemini/#causaliq_knowledge.llm.gemini_client.GeminiClient.complete_json(**kwargs)","title":"<code>**kwargs</code>","text":"(<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Override config options passed to completion().</p>"},{"location":"api/clients/gemini/#causaliq_knowledge.llm.gemini_client.GeminiClient.completion","title":"completion","text":"<pre><code>completion(messages: List[Dict[str, str]], **kwargs: Any) -&gt; LLMResponse\n</code></pre> <p>Make a chat completion request to Gemini.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>LLMResponse</code>           \u2013            <p>LLMResponse with the generated content and metadata.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the API request fails.</p> </li> </ul>"},{"location":"api/clients/gemini/#causaliq_knowledge.llm.gemini_client.GeminiClient.completion(messages)","title":"<code>messages</code>","text":"(<code>List[Dict[str, str]]</code>)           \u2013            <p>List of message dicts with \"role\" and \"content\" keys.</p>"},{"location":"api/clients/gemini/#causaliq_knowledge.llm.gemini_client.GeminiClient.completion(**kwargs)","title":"<code>**kwargs</code>","text":"(<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Override config options (temperature, max_tokens).</p>"},{"location":"api/clients/gemini/#causaliq_knowledge.llm.gemini_client.GeminiClient.is_available","title":"is_available","text":"<pre><code>is_available() -&gt; bool\n</code></pre> <p>Check if Gemini API is available.</p> <p>Returns:</p> <ul> <li> <code>bool</code>           \u2013            <p>True if GEMINI_API_KEY is configured.</p> </li> </ul>"},{"location":"api/clients/gemini/#causaliq_knowledge.llm.gemini_client.GeminiClient.list_models","title":"list_models","text":"<pre><code>list_models() -&gt; List[str]\n</code></pre> <p>List available models from Gemini API.</p> <p>Queries the Gemini API to get models accessible with the current API key. Filters to only include models that support generateContent.</p> <p>Returns:</p> <ul> <li> <code>List[str]</code>           \u2013            <p>List of model identifiers (e.g., ['gemini-2.5-flash', ...]).</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the API request fails.</p> </li> </ul>"},{"location":"api/clients/gemini/#causaliq_knowledge.llm.gemini_client.GeminiClient.set_cache","title":"set_cache","text":"<pre><code>set_cache(cache: Optional['TokenCache'], use_cache: bool = True) -&gt; None\n</code></pre> <p>Configure caching for this client.</p> <p>Parameters:</p>"},{"location":"api/clients/gemini/#causaliq_knowledge.llm.gemini_client.GeminiClient.set_cache(cache)","title":"<code>cache</code>","text":"(<code>Optional['TokenCache']</code>)           \u2013            <p>TokenCache instance for caching, or None to disable.</p>"},{"location":"api/clients/gemini/#causaliq_knowledge.llm.gemini_client.GeminiClient.set_cache(use_cache)","title":"<code>use_cache</code>","text":"(<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use the cache (default True).</p>"},{"location":"api/clients/gemini/#message-format-conversion","title":"Message Format Conversion","text":"<p>The client automatically converts OpenAI-style messages to Gemini's format:</p> OpenAI Role Gemini Role <code>system</code> System instruction (separate field) <code>user</code> <code>user</code> <code>assistant</code> <code>model</code>"},{"location":"api/clients/gemini/#supported-models","title":"Supported Models","text":"<p>Google Gemini provides a generous free tier:</p> Model Description Free Tier <code>gemini-2.5-flash</code> Fast and efficient \u2705 Yes <code>gemini-2.5-pro</code> Most capable \u2705 Limited <code>gemini-1.5-flash</code> Previous generation \u2705 Yes <p>See Google AI documentation for the full list of available models.</p>"},{"location":"api/clients/groq/","title":"Groq Client API Reference","text":"<p>Direct Groq API client for fast LLM inference. This client implements the BaseLLMClient interface using httpx to communicate directly with the Groq API.</p>"},{"location":"api/clients/groq/#overview","title":"Overview","text":"<p>The Groq client provides:</p> <ul> <li>Direct HTTP communication with Groq's API</li> <li>Implements the <code>BaseLLMClient</code> abstract interface</li> <li>JSON response parsing with error handling</li> <li>Call counting for usage tracking</li> <li>Configurable timeout and retry settings</li> </ul>"},{"location":"api/clients/groq/#usage","title":"Usage","text":"<pre><code>from causaliq_knowledge.llm import GroqClient, GroqConfig\n\n# Create client with custom config\nconfig = GroqConfig(\n    model=\"llama-3.1-8b-instant\",\n    temperature=0.1,\n    max_tokens=500,\n)\nclient = GroqClient(config=config)\n\n# Make a completion request\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"What is 2+2?\"},\n]\nresponse = client.completion(messages)\nprint(response.content)\n\n# Parse JSON response\njson_data = response.parse_json()\n</code></pre>"},{"location":"api/clients/groq/#environment-variables","title":"Environment Variables","text":"<p>The Groq client requires the <code>GROQ_API_KEY</code> environment variable to be set:</p> <pre><code>export GROQ_API_KEY=your_api_key_here\n</code></pre>"},{"location":"api/clients/groq/#groqconfig","title":"GroqConfig","text":""},{"location":"api/clients/groq/#causaliq_knowledge.llm.groq_client.GroqConfig","title":"GroqConfig  <code>dataclass</code>","text":"<pre><code>GroqConfig(\n    model: str = \"llama-3.1-8b-instant\",\n    temperature: float = 0.1,\n    max_tokens: int = 500,\n    timeout: float = 30.0,\n    api_key: Optional[str] = None,\n)\n</code></pre> <p>Configuration for Groq API client.</p> <p>Extends LLMConfig with Groq-specific defaults.</p> <p>Attributes:</p> <ul> <li> <code>model</code>               (<code>str</code>)           \u2013            <p>Groq model identifier (default: llama-3.1-8b-instant).</p> </li> <li> <code>temperature</code>               (<code>float</code>)           \u2013            <p>Sampling temperature (default: 0.1).</p> </li> <li> <code>max_tokens</code>               (<code>int</code>)           \u2013            <p>Maximum response tokens (default: 500).</p> </li> <li> <code>timeout</code>               (<code>float</code>)           \u2013            <p>Request timeout in seconds (default: 30.0).</p> </li> <li> <code>api_key</code>               (<code>Optional[str]</code>)           \u2013            <p>Groq API key (falls back to GROQ_API_KEY env var).</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>__post_init__</code>             \u2013              <p>Set API key from environment if not provided.</p> </li> </ul>"},{"location":"api/clients/groq/#causaliq_knowledge.llm.groq_client.GroqConfig.api_key","title":"api_key  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>api_key: Optional[str] = None\n</code></pre>"},{"location":"api/clients/groq/#causaliq_knowledge.llm.groq_client.GroqConfig.max_tokens","title":"max_tokens  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_tokens: int = 500\n</code></pre>"},{"location":"api/clients/groq/#causaliq_knowledge.llm.groq_client.GroqConfig.model","title":"model  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model: str = 'llama-3.1-8b-instant'\n</code></pre>"},{"location":"api/clients/groq/#causaliq_knowledge.llm.groq_client.GroqConfig.temperature","title":"temperature  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>temperature: float = 0.1\n</code></pre>"},{"location":"api/clients/groq/#causaliq_knowledge.llm.groq_client.GroqConfig.timeout","title":"timeout  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>timeout: float = 30.0\n</code></pre>"},{"location":"api/clients/groq/#causaliq_knowledge.llm.groq_client.GroqConfig.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__() -&gt; None\n</code></pre> <p>Set API key from environment if not provided.</p>"},{"location":"api/clients/groq/#groqclient","title":"GroqClient","text":""},{"location":"api/clients/groq/#causaliq_knowledge.llm.groq_client.GroqClient","title":"GroqClient","text":"<pre><code>GroqClient(config: Optional[GroqConfig] = None)\n</code></pre> <p>Direct Groq API client.</p> <p>Implements the BaseLLMClient interface for Groq's API. Uses httpx for HTTP requests.</p> Example <p>config = GroqConfig(model=\"llama-3.1-8b-instant\") client = GroqClient(config) msgs = [{\"role\": \"user\", \"content\": \"Hello\"}] response = client.completion(msgs) print(response.content)</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>_build_cache_key</code>             \u2013              <p>Build a deterministic cache key for the request.</p> </li> <li> <code>cached_completion</code>             \u2013              <p>Make a completion request with caching.</p> </li> <li> <code>complete_json</code>             \u2013              <p>Make a completion request and parse response as JSON.</p> </li> <li> <code>completion</code>             \u2013              <p>Make a chat completion request to Groq.</p> </li> <li> <code>is_available</code>             \u2013              <p>Check if Groq API is available.</p> </li> <li> <code>list_models</code>             \u2013              <p>List available models from Groq API.</p> </li> <li> <code>set_cache</code>             \u2013              <p>Configure caching for this client.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>BASE_URL</code>           \u2013            </li> <li> <code>_total_calls</code>           \u2013            </li> <li> <code>cache</code>               (<code>Optional['TokenCache']</code>)           \u2013            <p>Return the configured cache, if any.</p> </li> <li> <code>call_count</code>               (<code>int</code>)           \u2013            <p>Return the number of API calls made.</p> </li> <li> <code>config</code>           \u2013            </li> <li> <code>model_name</code>               (<code>str</code>)           \u2013            <p>Return the model name being used.</p> </li> <li> <code>provider_name</code>               (<code>str</code>)           \u2013            <p>Return the provider name.</p> </li> <li> <code>use_cache</code>               (<code>bool</code>)           \u2013            <p>Return whether caching is enabled.</p> </li> </ul>"},{"location":"api/clients/groq/#causaliq_knowledge.llm.groq_client.GroqClient(config)","title":"<code>config</code>","text":"(<code>Optional[GroqConfig]</code>, default:                   <code>None</code> )           \u2013            <p>Groq configuration. If None, uses defaults with    API key from GROQ_API_KEY environment variable.</p>"},{"location":"api/clients/groq/#causaliq_knowledge.llm.groq_client.GroqClient.BASE_URL","title":"BASE_URL  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>BASE_URL = 'https://api.groq.com/openai/v1'\n</code></pre>"},{"location":"api/clients/groq/#causaliq_knowledge.llm.groq_client.GroqClient._total_calls","title":"_total_calls  <code>instance-attribute</code>","text":"<pre><code>_total_calls = 0\n</code></pre>"},{"location":"api/clients/groq/#causaliq_knowledge.llm.groq_client.GroqClient.cache","title":"cache  <code>property</code>","text":"<pre><code>cache: Optional['TokenCache']\n</code></pre> <p>Return the configured cache, if any.</p>"},{"location":"api/clients/groq/#causaliq_knowledge.llm.groq_client.GroqClient.call_count","title":"call_count  <code>property</code>","text":"<pre><code>call_count: int\n</code></pre> <p>Return the number of API calls made.</p>"},{"location":"api/clients/groq/#causaliq_knowledge.llm.groq_client.GroqClient.config","title":"config  <code>instance-attribute</code>","text":"<pre><code>config = config or GroqConfig()\n</code></pre>"},{"location":"api/clients/groq/#causaliq_knowledge.llm.groq_client.GroqClient.model_name","title":"model_name  <code>property</code>","text":"<pre><code>model_name: str\n</code></pre> <p>Return the model name being used.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>Model identifier string.</p> </li> </ul>"},{"location":"api/clients/groq/#causaliq_knowledge.llm.groq_client.GroqClient.provider_name","title":"provider_name  <code>property</code>","text":"<pre><code>provider_name: str\n</code></pre> <p>Return the provider name.</p>"},{"location":"api/clients/groq/#causaliq_knowledge.llm.groq_client.GroqClient.use_cache","title":"use_cache  <code>property</code>","text":"<pre><code>use_cache: bool\n</code></pre> <p>Return whether caching is enabled.</p>"},{"location":"api/clients/groq/#causaliq_knowledge.llm.groq_client.GroqClient._build_cache_key","title":"_build_cache_key","text":"<pre><code>_build_cache_key(\n    messages: List[Dict[str, str]],\n    temperature: Optional[float] = None,\n    max_tokens: Optional[int] = None,\n) -&gt; str\n</code></pre> <p>Build a deterministic cache key for the request.</p> <p>Creates a SHA-256 hash from the model, messages, temperature, and max_tokens. The hash is truncated to 16 hex characters (64 bits).</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>16-character hex string cache key.</p> </li> </ul>"},{"location":"api/clients/groq/#causaliq_knowledge.llm.groq_client.GroqClient._build_cache_key(messages)","title":"<code>messages</code>","text":"(<code>List[Dict[str, str]]</code>)           \u2013            <p>List of message dicts with \"role\" and \"content\" keys.</p>"},{"location":"api/clients/groq/#causaliq_knowledge.llm.groq_client.GroqClient._build_cache_key(temperature)","title":"<code>temperature</code>","text":"(<code>Optional[float]</code>, default:                   <code>None</code> )           \u2013            <p>Sampling temperature (defaults to config value).</p>"},{"location":"api/clients/groq/#causaliq_knowledge.llm.groq_client.GroqClient._build_cache_key(max_tokens)","title":"<code>max_tokens</code>","text":"(<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>Maximum tokens (defaults to config value).</p>"},{"location":"api/clients/groq/#causaliq_knowledge.llm.groq_client.GroqClient.cached_completion","title":"cached_completion","text":"<pre><code>cached_completion(messages: List[Dict[str, str]], **kwargs: Any) -&gt; LLMResponse\n</code></pre> <p>Make a completion request with caching.</p> <p>If caching is enabled and a cached response exists, returns the cached response without making an API call. Otherwise, makes the API call and caches the result.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>LLMResponse</code>           \u2013            <p>LLMResponse with the generated content and metadata.</p> </li> </ul>"},{"location":"api/clients/groq/#causaliq_knowledge.llm.groq_client.GroqClient.cached_completion(messages)","title":"<code>messages</code>","text":"(<code>List[Dict[str, str]]</code>)           \u2013            <p>List of message dicts with \"role\" and \"content\" keys.</p>"},{"location":"api/clients/groq/#causaliq_knowledge.llm.groq_client.GroqClient.cached_completion(**kwargs)","title":"<code>**kwargs</code>","text":"(<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Provider-specific options (temperature, max_tokens, etc.) Also accepts request_id (str) for identifying requests in exports. Note: request_id is NOT part of the cache key.</p>"},{"location":"api/clients/groq/#causaliq_knowledge.llm.groq_client.GroqClient.complete_json","title":"complete_json","text":"<pre><code>complete_json(\n    messages: List[Dict[str, str]], **kwargs: Any\n) -&gt; tuple[Optional[Dict[str, Any]], LLMResponse]\n</code></pre> <p>Make a completion request and parse response as JSON.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>tuple[Optional[Dict[str, Any]], LLMResponse]</code>           \u2013            <p>Tuple of (parsed JSON dict or None, raw LLMResponse).</p> </li> </ul>"},{"location":"api/clients/groq/#causaliq_knowledge.llm.groq_client.GroqClient.complete_json(messages)","title":"<code>messages</code>","text":"(<code>List[Dict[str, str]]</code>)           \u2013            <p>List of message dicts with \"role\" and \"content\" keys.</p>"},{"location":"api/clients/groq/#causaliq_knowledge.llm.groq_client.GroqClient.complete_json(**kwargs)","title":"<code>**kwargs</code>","text":"(<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Override config options passed to completion().</p>"},{"location":"api/clients/groq/#causaliq_knowledge.llm.groq_client.GroqClient.completion","title":"completion","text":"<pre><code>completion(messages: List[Dict[str, str]], **kwargs: Any) -&gt; LLMResponse\n</code></pre> <p>Make a chat completion request to Groq.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>LLMResponse</code>           \u2013            <p>LLMResponse with the generated content and metadata.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the API request fails.</p> </li> </ul>"},{"location":"api/clients/groq/#causaliq_knowledge.llm.groq_client.GroqClient.completion(messages)","title":"<code>messages</code>","text":"(<code>List[Dict[str, str]]</code>)           \u2013            <p>List of message dicts with \"role\" and \"content\" keys.</p>"},{"location":"api/clients/groq/#causaliq_knowledge.llm.groq_client.GroqClient.completion(**kwargs)","title":"<code>**kwargs</code>","text":"(<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Override config options (temperature, max_tokens).</p>"},{"location":"api/clients/groq/#causaliq_knowledge.llm.groq_client.GroqClient.is_available","title":"is_available","text":"<pre><code>is_available() -&gt; bool\n</code></pre> <p>Check if Groq API is available.</p> <p>Returns:</p> <ul> <li> <code>bool</code>           \u2013            <p>True if GROQ_API_KEY is configured.</p> </li> </ul>"},{"location":"api/clients/groq/#causaliq_knowledge.llm.groq_client.GroqClient.list_models","title":"list_models","text":"<pre><code>list_models() -&gt; List[str]\n</code></pre> <p>List available models from Groq API.</p> <p>Queries the Groq API to get models accessible with the current API key. Filters to only include text generation models.</p> <p>Returns:</p> <ul> <li> <code>List[str]</code>           \u2013            <p>List of model identifiers (e.g., ['llama-3.1-8b-instant', ...]).</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the API request fails.</p> </li> </ul>"},{"location":"api/clients/groq/#causaliq_knowledge.llm.groq_client.GroqClient.set_cache","title":"set_cache","text":"<pre><code>set_cache(cache: Optional['TokenCache'], use_cache: bool = True) -&gt; None\n</code></pre> <p>Configure caching for this client.</p> <p>Parameters:</p>"},{"location":"api/clients/groq/#causaliq_knowledge.llm.groq_client.GroqClient.set_cache(cache)","title":"<code>cache</code>","text":"(<code>Optional['TokenCache']</code>)           \u2013            <p>TokenCache instance for caching, or None to disable.</p>"},{"location":"api/clients/groq/#causaliq_knowledge.llm.groq_client.GroqClient.set_cache(use_cache)","title":"<code>use_cache</code>","text":"(<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use the cache (default True).</p>"},{"location":"api/clients/groq/#supported-models","title":"Supported Models","text":"<p>Groq provides fast inference for open-source models:</p> Model Description Free Tier <code>llama-3.1-8b-instant</code> Fast Llama 3.1 8B model \u2705 Yes <code>llama-3.1-70b-versatile</code> Larger Llama 3.1 model \u2705 Yes <code>mixtral-8x7b-32768</code> Mixtral MoE model \u2705 Yes <p>See Groq documentation for the full list of available models.</p>"},{"location":"api/clients/mistral/","title":"Mistral Client","text":"<p>Direct Mistral AI API client for Mistral models.</p>"},{"location":"api/clients/mistral/#overview","title":"Overview","text":"<p>Mistral AI is a French AI company known for high-quality open-weight and proprietary models. Their API is OpenAI-compatible, making integration straightforward.</p> <p>Key features:</p> <ul> <li>Mistral Small: Fast, cost-effective for simple tasks</li> <li>Mistral Large: Most capable, best for complex reasoning</li> <li>Codestral: Optimized for code generation</li> <li>Strong EU-based option for data sovereignty</li> <li>OpenAI-compatible API</li> </ul>"},{"location":"api/clients/mistral/#configuration","title":"Configuration","text":"<p>The client requires a <code>MISTRAL_API_KEY</code> environment variable:</p> <pre><code># Linux/macOS\nexport MISTRAL_API_KEY=\"your-api-key\"\n\n# Windows PowerShell\n$env:MISTRAL_API_KEY=\"your-api-key\"\n\n# Windows cmd\nset MISTRAL_API_KEY=your-api-key\n</code></pre> <p>Get your API key from: https://console.mistral.ai</p>"},{"location":"api/clients/mistral/#usage","title":"Usage","text":""},{"location":"api/clients/mistral/#basic-usage","title":"Basic Usage","text":"<pre><code>from causaliq_knowledge.llm import MistralClient, MistralConfig\n\n# Default config (uses MISTRAL_API_KEY env var)\nclient = MistralClient()\n\n# Or with custom config\nconfig = MistralConfig(\n    model=\"mistral-small-latest\",\n    temperature=0.1,\n    max_tokens=500,\n    timeout=30.0,\n)\nclient = MistralClient(config)\n\n# Make a completion request\nmessages = [{\"role\": \"user\", \"content\": \"What is 2 + 2?\"}]\nresponse = client.completion(messages)\nprint(response.content)\n</code></pre>"},{"location":"api/clients/mistral/#using-with-cli","title":"Using with CLI","text":"<pre><code># Query with Mistral\ncqknow query smoking lung_cancer --model mistral/mistral-small-latest\n\n# Use large model for complex queries\ncqknow query income education --model mistral/mistral-large-latest --domain economics\n\n# List available Mistral models\ncqknow models mistral\n</code></pre>"},{"location":"api/clients/mistral/#using-with-llmknowledge-provider","title":"Using with LLMKnowledge Provider","text":"<pre><code>from causaliq_knowledge.llm import LLMKnowledge\n\n# Single model\nprovider = LLMKnowledge(models=[\"mistral/mistral-small-latest\"])\nresult = provider.query_edge(\"smoking\", \"lung_cancer\")\n\n# Multi-model consensus\nprovider = LLMKnowledge(\n    models=[\n        \"mistral/mistral-large-latest\",\n        \"groq/llama-3.1-8b-instant\",\n    ],\n    consensus_strategy=\"weighted_vote\",\n)\n</code></pre>"},{"location":"api/clients/mistral/#available-models","title":"Available Models","text":"Model Description Best For <code>mistral-small-latest</code> Fast, cost-effective Simple tasks <code>mistral-medium-latest</code> Balanced performance General use <code>mistral-large-latest</code> Most capable Complex reasoning <code>codestral-latest</code> Code-optimized Programming tasks <code>open-mistral-nemo</code> 12B open model Budget-friendly <code>open-mixtral-8x7b</code> MoE open model Balanced open model <code>ministral-3b-latest</code> Ultra-small Edge deployment <code>ministral-8b-latest</code> Small Resource-constrained"},{"location":"api/clients/mistral/#pricing","title":"Pricing","text":"<p>Mistral AI offers competitive pricing (as of Jan 2025):</p> Model Input (per 1M tokens) Output (per 1M tokens) mistral-small $0.20 $0.60 mistral-medium $2.70 $8.10 mistral-large $2.00 $6.00 codestral $0.20 $0.60 open-mistral-nemo $0.15 $0.15 ministral-3b $0.04 $0.04 ministral-8b $0.10 $0.10 <p>See Mistral pricing for details.</p>"},{"location":"api/clients/mistral/#api-reference","title":"API Reference","text":""},{"location":"api/clients/mistral/#causaliq_knowledge.llm.mistral_client.MistralConfig","title":"MistralConfig  <code>dataclass</code>","text":"<pre><code>MistralConfig(\n    model: str = \"mistral-small-latest\",\n    temperature: float = 0.1,\n    max_tokens: int = 500,\n    timeout: float = 30.0,\n    api_key: Optional[str] = None,\n)\n</code></pre> <p>Configuration for Mistral AI API client.</p> <p>Extends OpenAICompatConfig with Mistral-specific defaults.</p> <p>Attributes:</p> <ul> <li> <code>model</code>               (<code>str</code>)           \u2013            <p>Mistral model identifier (default: mistral-small-latest).</p> </li> <li> <code>temperature</code>               (<code>float</code>)           \u2013            <p>Sampling temperature (default: 0.1).</p> </li> <li> <code>max_tokens</code>               (<code>int</code>)           \u2013            <p>Maximum response tokens (default: 500).</p> </li> <li> <code>timeout</code>               (<code>float</code>)           \u2013            <p>Request timeout in seconds (default: 30.0).</p> </li> <li> <code>api_key</code>               (<code>Optional[str]</code>)           \u2013            <p>Mistral API key (falls back to MISTRAL_API_KEY env var).</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>__post_init__</code>             \u2013              <p>Set API key from environment if not provided.</p> </li> </ul>"},{"location":"api/clients/mistral/#causaliq_knowledge.llm.mistral_client.MistralConfig.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__() -&gt; None\n</code></pre> <p>Set API key from environment if not provided.</p>"},{"location":"api/clients/mistral/#causaliq_knowledge.llm.mistral_client.MistralClient","title":"MistralClient","text":"<pre><code>MistralClient(config: Optional[MistralConfig] = None)\n</code></pre> <p>Direct Mistral AI API client.</p> <p>Mistral AI is a French company providing high-quality LLMs with an OpenAI-compatible API.</p> Available models <ul> <li>mistral-small-latest: Fast, cost-effective</li> <li>mistral-medium-latest: Balanced performance</li> <li>mistral-large-latest: Most capable</li> <li>codestral-latest: Optimized for code</li> </ul> Example <p>config = MistralConfig(model=\"mistral-small-latest\") client = MistralClient(config) msgs = [{\"role\": \"user\", \"content\": \"Hello\"}] response = client.completion(msgs) print(response.content)</p> <p>Parameters:</p> <ul> <li> </li> </ul> <p>Methods:</p> <ul> <li> <code>cached_completion</code>             \u2013              <p>Make a completion request with caching.</p> </li> <li> <code>complete_json</code>             \u2013              <p>Make a completion request and parse response as JSON.</p> </li> <li> <code>completion</code>             \u2013              <p>Make a chat completion request.</p> </li> <li> <code>is_available</code>             \u2013              <p>Check if the API is available.</p> </li> <li> <code>list_models</code>             \u2013              <p>List available models from the API.</p> </li> <li> <code>set_cache</code>             \u2013              <p>Configure caching for this client.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>cache</code>               (<code>Optional['TokenCache']</code>)           \u2013            <p>Return the configured cache, if any.</p> </li> <li> <code>call_count</code>               (<code>int</code>)           \u2013            <p>Return the number of API calls made.</p> </li> <li> <code>model_name</code>               (<code>str</code>)           \u2013            <p>Return the model name being used.</p> </li> <li> <code>provider_name</code>               (<code>str</code>)           \u2013            <p>Return the provider name.</p> </li> <li> <code>use_cache</code>               (<code>bool</code>)           \u2013            <p>Return whether caching is enabled.</p> </li> </ul>"},{"location":"api/clients/mistral/#causaliq_knowledge.llm.mistral_client.MistralClient(config)","title":"<code>config</code>","text":"(<code>Optional[MistralConfig]</code>, default:                   <code>None</code> )           \u2013            <p>Mistral configuration. If None, uses defaults with    API key from MISTRAL_API_KEY environment variable.</p>"},{"location":"api/clients/mistral/#causaliq_knowledge.llm.mistral_client.MistralClient.cache","title":"cache  <code>property</code>","text":"<pre><code>cache: Optional['TokenCache']\n</code></pre> <p>Return the configured cache, if any.</p>"},{"location":"api/clients/mistral/#causaliq_knowledge.llm.mistral_client.MistralClient.call_count","title":"call_count  <code>property</code>","text":"<pre><code>call_count: int\n</code></pre> <p>Return the number of API calls made.</p>"},{"location":"api/clients/mistral/#causaliq_knowledge.llm.mistral_client.MistralClient.model_name","title":"model_name  <code>property</code>","text":"<pre><code>model_name: str\n</code></pre> <p>Return the model name being used.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>Model identifier string.</p> </li> </ul>"},{"location":"api/clients/mistral/#causaliq_knowledge.llm.mistral_client.MistralClient.provider_name","title":"provider_name  <code>property</code>","text":"<pre><code>provider_name: str\n</code></pre> <p>Return the provider name.</p>"},{"location":"api/clients/mistral/#causaliq_knowledge.llm.mistral_client.MistralClient.use_cache","title":"use_cache  <code>property</code>","text":"<pre><code>use_cache: bool\n</code></pre> <p>Return whether caching is enabled.</p>"},{"location":"api/clients/mistral/#causaliq_knowledge.llm.mistral_client.MistralClient.cached_completion","title":"cached_completion","text":"<pre><code>cached_completion(messages: List[Dict[str, str]], **kwargs: Any) -&gt; LLMResponse\n</code></pre> <p>Make a completion request with caching.</p> <p>If caching is enabled and a cached response exists, returns the cached response without making an API call. Otherwise, makes the API call and caches the result.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>LLMResponse</code>           \u2013            <p>LLMResponse with the generated content and metadata.</p> </li> </ul>"},{"location":"api/clients/mistral/#causaliq_knowledge.llm.mistral_client.MistralClient.cached_completion(messages)","title":"<code>messages</code>","text":"(<code>List[Dict[str, str]]</code>)           \u2013            <p>List of message dicts with \"role\" and \"content\" keys.</p>"},{"location":"api/clients/mistral/#causaliq_knowledge.llm.mistral_client.MistralClient.cached_completion(**kwargs)","title":"<code>**kwargs</code>","text":"(<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Provider-specific options (temperature, max_tokens, etc.) Also accepts request_id (str) for identifying requests in exports. Note: request_id is NOT part of the cache key.</p>"},{"location":"api/clients/mistral/#causaliq_knowledge.llm.mistral_client.MistralClient.complete_json","title":"complete_json","text":"<pre><code>complete_json(\n    messages: List[Dict[str, str]], **kwargs: Any\n) -&gt; tuple[Optional[Dict[str, Any]], LLMResponse]\n</code></pre> <p>Make a completion request and parse response as JSON.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>tuple[Optional[Dict[str, Any]], LLMResponse]</code>           \u2013            <p>Tuple of (parsed JSON dict or None, raw LLMResponse).</p> </li> </ul>"},{"location":"api/clients/mistral/#causaliq_knowledge.llm.mistral_client.MistralClient.complete_json(messages)","title":"<code>messages</code>","text":"(<code>List[Dict[str, str]]</code>)           \u2013            <p>List of message dicts with \"role\" and \"content\" keys.</p>"},{"location":"api/clients/mistral/#causaliq_knowledge.llm.mistral_client.MistralClient.complete_json(**kwargs)","title":"<code>**kwargs</code>","text":"(<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Override config options passed to completion().</p>"},{"location":"api/clients/mistral/#causaliq_knowledge.llm.mistral_client.MistralClient.completion","title":"completion","text":"<pre><code>completion(messages: List[Dict[str, str]], **kwargs: Any) -&gt; LLMResponse\n</code></pre> <p>Make a chat completion request.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>LLMResponse</code>           \u2013            <p>LLMResponse with the generated content and metadata.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the API request fails.</p> </li> </ul>"},{"location":"api/clients/mistral/#causaliq_knowledge.llm.mistral_client.MistralClient.completion(messages)","title":"<code>messages</code>","text":"(<code>List[Dict[str, str]]</code>)           \u2013            <p>List of message dicts with \"role\" and \"content\" keys.</p>"},{"location":"api/clients/mistral/#causaliq_knowledge.llm.mistral_client.MistralClient.completion(**kwargs)","title":"<code>**kwargs</code>","text":"(<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Override config options (temperature, max_tokens).</p>"},{"location":"api/clients/mistral/#causaliq_knowledge.llm.mistral_client.MistralClient.is_available","title":"is_available","text":"<pre><code>is_available() -&gt; bool\n</code></pre> <p>Check if the API is available.</p> <p>Returns:</p> <ul> <li> <code>bool</code>           \u2013            <p>True if API key is configured.</p> </li> </ul>"},{"location":"api/clients/mistral/#causaliq_knowledge.llm.mistral_client.MistralClient.list_models","title":"list_models","text":"<pre><code>list_models() -&gt; List[str]\n</code></pre> <p>List available models from the API.</p> <p>Queries the API to get models accessible with the current API key, then filters using _filter_models().</p> <p>Returns:</p> <ul> <li> <code>List[str]</code>           \u2013            <p>List of model identifiers.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the API request fails.</p> </li> </ul>"},{"location":"api/clients/mistral/#causaliq_knowledge.llm.mistral_client.MistralClient.set_cache","title":"set_cache","text":"<pre><code>set_cache(cache: Optional['TokenCache'], use_cache: bool = True) -&gt; None\n</code></pre> <p>Configure caching for this client.</p> <p>Parameters:</p>"},{"location":"api/clients/mistral/#causaliq_knowledge.llm.mistral_client.MistralClient.set_cache(cache)","title":"<code>cache</code>","text":"(<code>Optional['TokenCache']</code>)           \u2013            <p>TokenCache instance for caching, or None to disable.</p>"},{"location":"api/clients/mistral/#causaliq_knowledge.llm.mistral_client.MistralClient.set_cache(use_cache)","title":"<code>use_cache</code>","text":"(<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use the cache (default True).</p>"},{"location":"api/clients/ollama/","title":"Ollama Client API Reference","text":"<p>Local Ollama API client for running Llama and other open-source models locally. This client implements the BaseLLMClient interface using httpx to communicate with a locally running Ollama server.</p>"},{"location":"api/clients/ollama/#overview","title":"Overview","text":"<p>The Ollama client provides:</p> <ul> <li>Local LLM inference without API keys or internet access</li> <li>Implements the <code>BaseLLMClient</code> abstract interface</li> <li>Support for Llama 3.2, Llama 3.1, Mistral, and other models</li> <li>JSON response parsing with error handling</li> <li>Call counting for usage tracking</li> <li>Availability checking via <code>is_available()</code> method</li> </ul>"},{"location":"api/clients/ollama/#prerequisites","title":"Prerequisites","text":"<ol> <li>Install Ollama from ollama.com/download</li> <li>Pull a model:    <pre><code>ollama pull llama3.2:1b    # Small, fast (~1.3GB)\nollama pull llama3.2       # Medium (~2GB)\nollama pull llama3.1:8b    # Larger, better quality (~4.7GB)\n</code></pre></li> <li>Ensure Ollama is running (it usually auto-starts after installation)</li> </ol>"},{"location":"api/clients/ollama/#usage","title":"Usage","text":"<pre><code>from causaliq_knowledge.llm import OllamaClient, OllamaConfig\n\n# Create client with default config (llama3.2:1b on localhost:11434)\nclient = OllamaClient()\n\n# Or with custom config\nconfig = OllamaConfig(\n    model=\"llama3.1:8b\",\n    temperature=0.1,\n    max_tokens=500,\n    timeout=120.0,  # Local inference can be slow\n)\nclient = OllamaClient(config=config)\n\n# Check if Ollama is available\nif client.is_available():\n    # Make a completion request\n    messages = [\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"What is 2+2?\"},\n    ]\n    response = client.completion(messages)\n    print(response.content)\nelse:\n    print(\"Ollama not running or model not installed\")\n</code></pre>"},{"location":"api/clients/ollama/#using-with-llmknowledge-provider","title":"Using with LLMKnowledge Provider","text":"<pre><code>from causaliq_knowledge.llm import LLMKnowledge\n\n# Use local Ollama for causal queries\nprovider = LLMKnowledge(models=[\"ollama/llama3.2:1b\"])\nresult = provider.query_edge(\"smoking\", \"lung_cancer\")\nprint(f\"Exists: {result.exists}, Confidence: {result.confidence}\")\n\n# Mix local and cloud models for consensus\nprovider = LLMKnowledge(\n    models=[\n        \"ollama/llama3.2:1b\",\n        \"groq/llama-3.1-8b-instant\",\n    ],\n    consensus_strategy=\"weighted_vote\"\n)\n</code></pre>"},{"location":"api/clients/ollama/#ollamaconfig","title":"OllamaConfig","text":""},{"location":"api/clients/ollama/#causaliq_knowledge.llm.ollama_client.OllamaConfig","title":"OllamaConfig  <code>dataclass</code>","text":"<pre><code>OllamaConfig(\n    model: str = \"llama3.2:1b\",\n    temperature: float = 0.1,\n    max_tokens: int = 500,\n    timeout: float = 120.0,\n    api_key: Optional[str] = None,\n    base_url: str = \"http://localhost:11434\",\n)\n</code></pre> <p>Configuration for Ollama API client.</p> <p>Extends LLMConfig with Ollama-specific defaults.</p> <p>Attributes:</p> <ul> <li> <code>model</code>               (<code>str</code>)           \u2013            <p>Ollama model identifier (default: llama3.2:1b).</p> </li> <li> <code>temperature</code>               (<code>float</code>)           \u2013            <p>Sampling temperature (default: 0.1).</p> </li> <li> <code>max_tokens</code>               (<code>int</code>)           \u2013            <p>Maximum response tokens (default: 500).</p> </li> <li> <code>timeout</code>               (<code>float</code>)           \u2013            <p>Request timeout in seconds (default: 120.0, local).</p> </li> <li> <code>api_key</code>               (<code>Optional[str]</code>)           \u2013            <p>Not used for Ollama (local server).</p> </li> <li> <code>base_url</code>               (<code>str</code>)           \u2013            <p>Ollama server URL (default: http://localhost:11434).</p> </li> </ul>"},{"location":"api/clients/ollama/#causaliq_knowledge.llm.ollama_client.OllamaConfig.api_key","title":"api_key  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>api_key: Optional[str] = None\n</code></pre>"},{"location":"api/clients/ollama/#causaliq_knowledge.llm.ollama_client.OllamaConfig.base_url","title":"base_url  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>base_url: str = 'http://localhost:11434'\n</code></pre>"},{"location":"api/clients/ollama/#causaliq_knowledge.llm.ollama_client.OllamaConfig.max_tokens","title":"max_tokens  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_tokens: int = 500\n</code></pre>"},{"location":"api/clients/ollama/#causaliq_knowledge.llm.ollama_client.OllamaConfig.model","title":"model  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model: str = 'llama3.2:1b'\n</code></pre>"},{"location":"api/clients/ollama/#causaliq_knowledge.llm.ollama_client.OllamaConfig.temperature","title":"temperature  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>temperature: float = 0.1\n</code></pre>"},{"location":"api/clients/ollama/#causaliq_knowledge.llm.ollama_client.OllamaConfig.timeout","title":"timeout  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>timeout: float = 120.0\n</code></pre>"},{"location":"api/clients/ollama/#ollamaclient","title":"OllamaClient","text":""},{"location":"api/clients/ollama/#causaliq_knowledge.llm.ollama_client.OllamaClient","title":"OllamaClient","text":"<pre><code>OllamaClient(config: Optional[OllamaConfig] = None)\n</code></pre> <p>Local Ollama API client.</p> <p>Implements the BaseLLMClient interface for locally running Ollama server. Uses httpx for HTTP requests to the local Ollama API.</p> <p>Ollama provides an OpenAI-compatible API for running open-source models like Llama locally without requiring API keys or internet access.</p> Example <p>config = OllamaConfig(model=\"llama3.2:1b\") client = OllamaClient(config) msgs = [{\"role\": \"user\", \"content\": \"Hello\"}] response = client.completion(msgs) print(response.content)</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>_build_cache_key</code>             \u2013              <p>Build a deterministic cache key for the request.</p> </li> <li> <code>cached_completion</code>             \u2013              <p>Make a completion request with caching.</p> </li> <li> <code>complete_json</code>             \u2013              <p>Make a completion request and parse response as JSON.</p> </li> <li> <code>completion</code>             \u2013              <p>Make a chat completion request to Ollama.</p> </li> <li> <code>is_available</code>             \u2013              <p>Check if Ollama server is running and model is available.</p> </li> <li> <code>list_models</code>             \u2013              <p>List installed models from Ollama.</p> </li> <li> <code>set_cache</code>             \u2013              <p>Configure caching for this client.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>_total_calls</code>           \u2013            </li> <li> <code>cache</code>               (<code>Optional['TokenCache']</code>)           \u2013            <p>Return the configured cache, if any.</p> </li> <li> <code>call_count</code>               (<code>int</code>)           \u2013            <p>Return the number of API calls made.</p> </li> <li> <code>config</code>           \u2013            </li> <li> <code>model_name</code>               (<code>str</code>)           \u2013            <p>Return the model name being used.</p> </li> <li> <code>provider_name</code>               (<code>str</code>)           \u2013            <p>Return the provider name.</p> </li> <li> <code>use_cache</code>               (<code>bool</code>)           \u2013            <p>Return whether caching is enabled.</p> </li> </ul>"},{"location":"api/clients/ollama/#causaliq_knowledge.llm.ollama_client.OllamaClient(config)","title":"<code>config</code>","text":"(<code>Optional[OllamaConfig]</code>, default:                   <code>None</code> )           \u2013            <p>Ollama configuration. If None, uses defaults connecting    to localhost:11434 with llama3.2:1b model.</p>"},{"location":"api/clients/ollama/#causaliq_knowledge.llm.ollama_client.OllamaClient._total_calls","title":"_total_calls  <code>instance-attribute</code>","text":"<pre><code>_total_calls = 0\n</code></pre>"},{"location":"api/clients/ollama/#causaliq_knowledge.llm.ollama_client.OllamaClient.cache","title":"cache  <code>property</code>","text":"<pre><code>cache: Optional['TokenCache']\n</code></pre> <p>Return the configured cache, if any.</p>"},{"location":"api/clients/ollama/#causaliq_knowledge.llm.ollama_client.OllamaClient.call_count","title":"call_count  <code>property</code>","text":"<pre><code>call_count: int\n</code></pre> <p>Return the number of API calls made.</p>"},{"location":"api/clients/ollama/#causaliq_knowledge.llm.ollama_client.OllamaClient.config","title":"config  <code>instance-attribute</code>","text":"<pre><code>config = config or OllamaConfig()\n</code></pre>"},{"location":"api/clients/ollama/#causaliq_knowledge.llm.ollama_client.OllamaClient.model_name","title":"model_name  <code>property</code>","text":"<pre><code>model_name: str\n</code></pre> <p>Return the model name being used.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>Model identifier string.</p> </li> </ul>"},{"location":"api/clients/ollama/#causaliq_knowledge.llm.ollama_client.OllamaClient.provider_name","title":"provider_name  <code>property</code>","text":"<pre><code>provider_name: str\n</code></pre> <p>Return the provider name.</p>"},{"location":"api/clients/ollama/#causaliq_knowledge.llm.ollama_client.OllamaClient.use_cache","title":"use_cache  <code>property</code>","text":"<pre><code>use_cache: bool\n</code></pre> <p>Return whether caching is enabled.</p>"},{"location":"api/clients/ollama/#causaliq_knowledge.llm.ollama_client.OllamaClient._build_cache_key","title":"_build_cache_key","text":"<pre><code>_build_cache_key(\n    messages: List[Dict[str, str]],\n    temperature: Optional[float] = None,\n    max_tokens: Optional[int] = None,\n) -&gt; str\n</code></pre> <p>Build a deterministic cache key for the request.</p> <p>Creates a SHA-256 hash from the model, messages, temperature, and max_tokens. The hash is truncated to 16 hex characters (64 bits).</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>16-character hex string cache key.</p> </li> </ul>"},{"location":"api/clients/ollama/#causaliq_knowledge.llm.ollama_client.OllamaClient._build_cache_key(messages)","title":"<code>messages</code>","text":"(<code>List[Dict[str, str]]</code>)           \u2013            <p>List of message dicts with \"role\" and \"content\" keys.</p>"},{"location":"api/clients/ollama/#causaliq_knowledge.llm.ollama_client.OllamaClient._build_cache_key(temperature)","title":"<code>temperature</code>","text":"(<code>Optional[float]</code>, default:                   <code>None</code> )           \u2013            <p>Sampling temperature (defaults to config value).</p>"},{"location":"api/clients/ollama/#causaliq_knowledge.llm.ollama_client.OllamaClient._build_cache_key(max_tokens)","title":"<code>max_tokens</code>","text":"(<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>Maximum tokens (defaults to config value).</p>"},{"location":"api/clients/ollama/#causaliq_knowledge.llm.ollama_client.OllamaClient.cached_completion","title":"cached_completion","text":"<pre><code>cached_completion(messages: List[Dict[str, str]], **kwargs: Any) -&gt; LLMResponse\n</code></pre> <p>Make a completion request with caching.</p> <p>If caching is enabled and a cached response exists, returns the cached response without making an API call. Otherwise, makes the API call and caches the result.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>LLMResponse</code>           \u2013            <p>LLMResponse with the generated content and metadata.</p> </li> </ul>"},{"location":"api/clients/ollama/#causaliq_knowledge.llm.ollama_client.OllamaClient.cached_completion(messages)","title":"<code>messages</code>","text":"(<code>List[Dict[str, str]]</code>)           \u2013            <p>List of message dicts with \"role\" and \"content\" keys.</p>"},{"location":"api/clients/ollama/#causaliq_knowledge.llm.ollama_client.OllamaClient.cached_completion(**kwargs)","title":"<code>**kwargs</code>","text":"(<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Provider-specific options (temperature, max_tokens, etc.) Also accepts request_id (str) for identifying requests in exports. Note: request_id is NOT part of the cache key.</p>"},{"location":"api/clients/ollama/#causaliq_knowledge.llm.ollama_client.OllamaClient.complete_json","title":"complete_json","text":"<pre><code>complete_json(\n    messages: List[Dict[str, str]], **kwargs: Any\n) -&gt; tuple[Optional[Dict[str, Any]], LLMResponse]\n</code></pre> <p>Make a completion request and parse response as JSON.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>tuple[Optional[Dict[str, Any]], LLMResponse]</code>           \u2013            <p>Tuple of (parsed JSON dict or None, raw LLMResponse).</p> </li> </ul>"},{"location":"api/clients/ollama/#causaliq_knowledge.llm.ollama_client.OllamaClient.complete_json(messages)","title":"<code>messages</code>","text":"(<code>List[Dict[str, str]]</code>)           \u2013            <p>List of message dicts with \"role\" and \"content\" keys.</p>"},{"location":"api/clients/ollama/#causaliq_knowledge.llm.ollama_client.OllamaClient.complete_json(**kwargs)","title":"<code>**kwargs</code>","text":"(<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Override config options passed to completion().</p>"},{"location":"api/clients/ollama/#causaliq_knowledge.llm.ollama_client.OllamaClient.completion","title":"completion","text":"<pre><code>completion(messages: List[Dict[str, str]], **kwargs: Any) -&gt; LLMResponse\n</code></pre> <p>Make a chat completion request to Ollama.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>LLMResponse</code>           \u2013            <p>LLMResponse with the generated content and metadata.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the API request fails or Ollama is not running.</p> </li> </ul>"},{"location":"api/clients/ollama/#causaliq_knowledge.llm.ollama_client.OllamaClient.completion(messages)","title":"<code>messages</code>","text":"(<code>List[Dict[str, str]]</code>)           \u2013            <p>List of message dicts with \"role\" and \"content\" keys.</p>"},{"location":"api/clients/ollama/#causaliq_knowledge.llm.ollama_client.OllamaClient.completion(**kwargs)","title":"<code>**kwargs</code>","text":"(<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Override config options (temperature, max_tokens).</p>"},{"location":"api/clients/ollama/#causaliq_knowledge.llm.ollama_client.OllamaClient.is_available","title":"is_available","text":"<pre><code>is_available() -&gt; bool\n</code></pre> <p>Check if Ollama server is running and model is available.</p> <p>Returns:</p> <ul> <li> <code>bool</code>           \u2013            <p>True if Ollama is running and the configured model exists.</p> </li> </ul>"},{"location":"api/clients/ollama/#causaliq_knowledge.llm.ollama_client.OllamaClient.list_models","title":"list_models","text":"<pre><code>list_models() -&gt; List[str]\n</code></pre> <p>List installed models from Ollama.</p> <p>Queries the local Ollama server to get installed models. Unlike cloud providers, this returns only models the user has explicitly pulled/installed.</p> <p>Returns:</p> <ul> <li> <code>List[str]</code>           \u2013            <p>List of model identifiers (e.g., ['llama3.2:1b', ...]).</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If Ollama server is not running.</p> </li> </ul>"},{"location":"api/clients/ollama/#causaliq_knowledge.llm.ollama_client.OllamaClient.set_cache","title":"set_cache","text":"<pre><code>set_cache(cache: Optional['TokenCache'], use_cache: bool = True) -&gt; None\n</code></pre> <p>Configure caching for this client.</p> <p>Parameters:</p>"},{"location":"api/clients/ollama/#causaliq_knowledge.llm.ollama_client.OllamaClient.set_cache(cache)","title":"<code>cache</code>","text":"(<code>Optional['TokenCache']</code>)           \u2013            <p>TokenCache instance for caching, or None to disable.</p>"},{"location":"api/clients/ollama/#causaliq_knowledge.llm.ollama_client.OllamaClient.set_cache(use_cache)","title":"<code>use_cache</code>","text":"(<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use the cache (default True).</p>"},{"location":"api/clients/ollama/#supported-models","title":"Supported Models","text":"<p>Ollama supports many open-source models. Recommended for causal queries:</p> Model Size RAM Needed Quality <code>llama3.2:1b</code> ~1.3GB 4GB+ Good for simple queries <code>llama3.2</code> ~2GB 6GB+ Better reasoning <code>llama3.1:8b</code> ~4.7GB 10GB+ Best quality <code>mistral</code> ~4GB 8GB+ Good alternative <p>See Ollama Library for all available models.</p>"},{"location":"api/clients/ollama/#troubleshooting","title":"Troubleshooting","text":"<p>\"Could not connect to Ollama\"</p> <ul> <li>Ensure Ollama is installed and running</li> <li>Run <code>ollama serve</code> in a terminal, or start the Ollama app</li> <li>Check that nothing else is using port 11434</li> </ul> <p>\"Model not found\"</p> <ul> <li>Run <code>ollama pull &lt;model-name&gt;</code> to download the model</li> <li>Run <code>ollama list</code> to see installed models</li> </ul> <p>Slow responses</p> <ul> <li>Local inference is CPU/GPU bound</li> <li>Use smaller models like <code>llama3.2:1b</code></li> <li>Increase the timeout in <code>OllamaConfig</code></li> <li>Consider using GPU acceleration if available</li> </ul>"},{"location":"api/clients/openai/","title":"OpenAI Client API Reference","text":"<p>Direct OpenAI API client for GPT models. This client implements the BaseLLMClient interface using httpx to communicate directly with the OpenAI API.</p>"},{"location":"api/clients/openai/#overview","title":"Overview","text":"<p>The OpenAI client provides:</p> <ul> <li>Direct HTTP communication with OpenAI's API</li> <li>Implements the <code>BaseLLMClient</code> abstract interface</li> <li>JSON response parsing with error handling</li> <li>Call counting for usage tracking</li> <li>Cost estimation for API calls</li> <li>Configurable timeout and retry settings</li> </ul>"},{"location":"api/clients/openai/#usage","title":"Usage","text":"<pre><code>from causaliq_knowledge.llm import OpenAIClient, OpenAIConfig\n\n# Create client with custom config\nconfig = OpenAIConfig(\n    model=\"gpt-4o-mini\",\n    temperature=0.1,\n    max_tokens=500,\n)\nclient = OpenAIClient(config=config)\n\n# Make a completion request\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"What is 2+2?\"},\n]\nresponse = client.completion(messages)\nprint(response.content)\n\n# Parse JSON response\njson_data = response.parse_json()\n</code></pre>"},{"location":"api/clients/openai/#environment-variables","title":"Environment Variables","text":"<p>The OpenAI client requires the <code>OPENAI_API_KEY</code> environment variable to be set:</p> <pre><code>export OPENAI_API_KEY=your_api_key_here\n</code></pre>"},{"location":"api/clients/openai/#openaiconfig","title":"OpenAIConfig","text":""},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIConfig","title":"OpenAIConfig  <code>dataclass</code>","text":"<pre><code>OpenAIConfig(\n    model: str = \"gpt-4o-mini\",\n    temperature: float = 0.1,\n    max_tokens: int = 500,\n    timeout: float = 30.0,\n    api_key: Optional[str] = None,\n)\n</code></pre> <p>Configuration for OpenAI API client.</p> <p>Extends OpenAICompatConfig with OpenAI-specific defaults.</p> <p>Attributes:</p> <ul> <li> <code>model</code>               (<code>str</code>)           \u2013            <p>OpenAI model identifier (default: gpt-4o-mini).</p> </li> <li> <code>temperature</code>               (<code>float</code>)           \u2013            <p>Sampling temperature (default: 0.1).</p> </li> <li> <code>max_tokens</code>               (<code>int</code>)           \u2013            <p>Maximum response tokens (default: 500).</p> </li> <li> <code>timeout</code>               (<code>float</code>)           \u2013            <p>Request timeout in seconds (default: 30.0).</p> </li> <li> <code>api_key</code>               (<code>Optional[str]</code>)           \u2013            <p>OpenAI API key (falls back to OPENAI_API_KEY env var).</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>__post_init__</code>             \u2013              <p>Set API key from environment if not provided.</p> </li> </ul>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIConfig.api_key","title":"api_key  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>api_key: Optional[str] = None\n</code></pre>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIConfig.max_tokens","title":"max_tokens  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_tokens: int = 500\n</code></pre>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIConfig.model","title":"model  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model: str = 'gpt-4o-mini'\n</code></pre>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIConfig.temperature","title":"temperature  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>temperature: float = 0.1\n</code></pre>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIConfig.timeout","title":"timeout  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>timeout: float = 30.0\n</code></pre>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIConfig.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__() -&gt; None\n</code></pre> <p>Set API key from environment if not provided.</p>"},{"location":"api/clients/openai/#openaiclient","title":"OpenAIClient","text":""},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIClient","title":"OpenAIClient","text":"<pre><code>OpenAIClient(config: Optional[OpenAIConfig] = None)\n</code></pre> <p>Direct OpenAI API client.</p> <p>Implements the BaseLLMClient interface for OpenAI's API. Uses httpx for HTTP requests.</p> Example <p>config = OpenAIConfig(model=\"gpt-4o-mini\") client = OpenAIClient(config) msgs = [{\"role\": \"user\", \"content\": \"Hello\"}] response = client.completion(msgs) print(response.content)</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>_build_cache_key</code>             \u2013              <p>Build a deterministic cache key for the request.</p> </li> <li> <code>_calculate_cost</code>             \u2013              <p>Calculate approximate cost for API call.</p> </li> <li> <code>_default_config</code>             \u2013              <p>Return default OpenAI configuration.</p> </li> <li> <code>_filter_models</code>             \u2013              <p>Filter to OpenAI chat models only.</p> </li> <li> <code>_get_pricing</code>             \u2013              <p>Return OpenAI pricing per 1M tokens.</p> </li> <li> <code>cached_completion</code>             \u2013              <p>Make a completion request with caching.</p> </li> <li> <code>complete_json</code>             \u2013              <p>Make a completion request and parse response as JSON.</p> </li> <li> <code>completion</code>             \u2013              <p>Make a chat completion request.</p> </li> <li> <code>is_available</code>             \u2013              <p>Check if the API is available.</p> </li> <li> <code>list_models</code>             \u2013              <p>List available models from the API.</p> </li> <li> <code>set_cache</code>             \u2013              <p>Configure caching for this client.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>BASE_URL</code>           \u2013            </li> <li> <code>ENV_VAR</code>           \u2013            </li> <li> <code>PROVIDER_NAME</code>           \u2013            </li> <li> <code>_total_calls</code>           \u2013            </li> <li> <code>cache</code>               (<code>Optional['TokenCache']</code>)           \u2013            <p>Return the configured cache, if any.</p> </li> <li> <code>call_count</code>               (<code>int</code>)           \u2013            <p>Return the number of API calls made.</p> </li> <li> <code>config</code>           \u2013            </li> <li> <code>model_name</code>               (<code>str</code>)           \u2013            <p>Return the model name being used.</p> </li> <li> <code>provider_name</code>               (<code>str</code>)           \u2013            <p>Return the provider name.</p> </li> <li> <code>use_cache</code>               (<code>bool</code>)           \u2013            <p>Return whether caching is enabled.</p> </li> </ul>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIClient(config)","title":"<code>config</code>","text":"(<code>Optional[OpenAIConfig]</code>, default:                   <code>None</code> )           \u2013            <p>OpenAI configuration. If None, uses defaults with    API key from OPENAI_API_KEY environment variable.</p>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIClient.BASE_URL","title":"BASE_URL  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>BASE_URL = 'https://api.openai.com/v1'\n</code></pre>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIClient.ENV_VAR","title":"ENV_VAR  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ENV_VAR = 'OPENAI_API_KEY'\n</code></pre>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIClient.PROVIDER_NAME","title":"PROVIDER_NAME  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>PROVIDER_NAME = 'openai'\n</code></pre>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIClient._total_calls","title":"_total_calls  <code>instance-attribute</code>","text":"<pre><code>_total_calls = 0\n</code></pre>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIClient.cache","title":"cache  <code>property</code>","text":"<pre><code>cache: Optional['TokenCache']\n</code></pre> <p>Return the configured cache, if any.</p>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIClient.call_count","title":"call_count  <code>property</code>","text":"<pre><code>call_count: int\n</code></pre> <p>Return the number of API calls made.</p>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIClient.config","title":"config  <code>instance-attribute</code>","text":"<pre><code>config = config or _default_config()\n</code></pre>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIClient.model_name","title":"model_name  <code>property</code>","text":"<pre><code>model_name: str\n</code></pre> <p>Return the model name being used.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>Model identifier string.</p> </li> </ul>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIClient.provider_name","title":"provider_name  <code>property</code>","text":"<pre><code>provider_name: str\n</code></pre> <p>Return the provider name.</p>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIClient.use_cache","title":"use_cache  <code>property</code>","text":"<pre><code>use_cache: bool\n</code></pre> <p>Return whether caching is enabled.</p>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIClient._build_cache_key","title":"_build_cache_key","text":"<pre><code>_build_cache_key(\n    messages: List[Dict[str, str]],\n    temperature: Optional[float] = None,\n    max_tokens: Optional[int] = None,\n) -&gt; str\n</code></pre> <p>Build a deterministic cache key for the request.</p> <p>Creates a SHA-256 hash from the model, messages, temperature, and max_tokens. The hash is truncated to 16 hex characters (64 bits).</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>16-character hex string cache key.</p> </li> </ul>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIClient._build_cache_key(messages)","title":"<code>messages</code>","text":"(<code>List[Dict[str, str]]</code>)           \u2013            <p>List of message dicts with \"role\" and \"content\" keys.</p>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIClient._build_cache_key(temperature)","title":"<code>temperature</code>","text":"(<code>Optional[float]</code>, default:                   <code>None</code> )           \u2013            <p>Sampling temperature (defaults to config value).</p>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIClient._build_cache_key(max_tokens)","title":"<code>max_tokens</code>","text":"(<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>Maximum tokens (defaults to config value).</p>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIClient._calculate_cost","title":"_calculate_cost","text":"<pre><code>_calculate_cost(model: str, input_tokens: int, output_tokens: int) -&gt; float\n</code></pre> <p>Calculate approximate cost for API call.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>float</code>           \u2013            <p>Estimated cost in USD.</p> </li> </ul>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIClient._calculate_cost(model)","title":"<code>model</code>","text":"(<code>str</code>)           \u2013            <p>Model identifier.</p>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIClient._calculate_cost(input_tokens)","title":"<code>input_tokens</code>","text":"(<code>int</code>)           \u2013            <p>Number of input tokens.</p>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIClient._calculate_cost(output_tokens)","title":"<code>output_tokens</code>","text":"(<code>int</code>)           \u2013            <p>Number of output tokens.</p>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIClient._default_config","title":"_default_config","text":"<pre><code>_default_config() -&gt; OpenAIConfig\n</code></pre> <p>Return default OpenAI configuration.</p>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIClient._filter_models","title":"_filter_models","text":"<pre><code>_filter_models(models: List[str]) -&gt; List[str]\n</code></pre> <p>Filter to OpenAI chat models only.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>List[str]</code>           \u2013            <p>Filtered list of GPT and o1/o3 models.</p> </li> </ul>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIClient._filter_models(models)","title":"<code>models</code>","text":"(<code>List[str]</code>)           \u2013            <p>List of all model IDs from API.</p>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIClient._get_pricing","title":"_get_pricing","text":"<pre><code>_get_pricing() -&gt; Dict[str, Dict[str, float]]\n</code></pre> <p>Return OpenAI pricing per 1M tokens.</p> <p>Returns:</p> <ul> <li> <code>Dict[str, Dict[str, float]]</code>           \u2013            <p>Dict mapping model prefixes to input/output costs.</p> </li> </ul>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIClient.cached_completion","title":"cached_completion","text":"<pre><code>cached_completion(messages: List[Dict[str, str]], **kwargs: Any) -&gt; LLMResponse\n</code></pre> <p>Make a completion request with caching.</p> <p>If caching is enabled and a cached response exists, returns the cached response without making an API call. Otherwise, makes the API call and caches the result.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>LLMResponse</code>           \u2013            <p>LLMResponse with the generated content and metadata.</p> </li> </ul>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIClient.cached_completion(messages)","title":"<code>messages</code>","text":"(<code>List[Dict[str, str]]</code>)           \u2013            <p>List of message dicts with \"role\" and \"content\" keys.</p>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIClient.cached_completion(**kwargs)","title":"<code>**kwargs</code>","text":"(<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Provider-specific options (temperature, max_tokens, etc.) Also accepts request_id (str) for identifying requests in exports. Note: request_id is NOT part of the cache key.</p>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIClient.complete_json","title":"complete_json","text":"<pre><code>complete_json(\n    messages: List[Dict[str, str]], **kwargs: Any\n) -&gt; tuple[Optional[Dict[str, Any]], LLMResponse]\n</code></pre> <p>Make a completion request and parse response as JSON.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>tuple[Optional[Dict[str, Any]], LLMResponse]</code>           \u2013            <p>Tuple of (parsed JSON dict or None, raw LLMResponse).</p> </li> </ul>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIClient.complete_json(messages)","title":"<code>messages</code>","text":"(<code>List[Dict[str, str]]</code>)           \u2013            <p>List of message dicts with \"role\" and \"content\" keys.</p>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIClient.complete_json(**kwargs)","title":"<code>**kwargs</code>","text":"(<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Override config options passed to completion().</p>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIClient.completion","title":"completion","text":"<pre><code>completion(messages: List[Dict[str, str]], **kwargs: Any) -&gt; LLMResponse\n</code></pre> <p>Make a chat completion request.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>LLMResponse</code>           \u2013            <p>LLMResponse with the generated content and metadata.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the API request fails.</p> </li> </ul>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIClient.completion(messages)","title":"<code>messages</code>","text":"(<code>List[Dict[str, str]]</code>)           \u2013            <p>List of message dicts with \"role\" and \"content\" keys.</p>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIClient.completion(**kwargs)","title":"<code>**kwargs</code>","text":"(<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Override config options (temperature, max_tokens).</p>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIClient.is_available","title":"is_available","text":"<pre><code>is_available() -&gt; bool\n</code></pre> <p>Check if the API is available.</p> <p>Returns:</p> <ul> <li> <code>bool</code>           \u2013            <p>True if API key is configured.</p> </li> </ul>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIClient.list_models","title":"list_models","text":"<pre><code>list_models() -&gt; List[str]\n</code></pre> <p>List available models from the API.</p> <p>Queries the API to get models accessible with the current API key, then filters using _filter_models().</p> <p>Returns:</p> <ul> <li> <code>List[str]</code>           \u2013            <p>List of model identifiers.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the API request fails.</p> </li> </ul>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIClient.set_cache","title":"set_cache","text":"<pre><code>set_cache(cache: Optional['TokenCache'], use_cache: bool = True) -&gt; None\n</code></pre> <p>Configure caching for this client.</p> <p>Parameters:</p>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIClient.set_cache(cache)","title":"<code>cache</code>","text":"(<code>Optional['TokenCache']</code>)           \u2013            <p>TokenCache instance for caching, or None to disable.</p>"},{"location":"api/clients/openai/#causaliq_knowledge.llm.openai_client.OpenAIClient.set_cache(use_cache)","title":"<code>use_cache</code>","text":"(<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use the cache (default True).</p>"},{"location":"api/clients/openai/#supported-models","title":"Supported Models","text":"<p>OpenAI provides the GPT family of models:</p> Model Description Free Tier <code>gpt-4o</code> GPT-4o - flagship multimodal model \u274c No <code>gpt-4o-mini</code> GPT-4o Mini - affordable and fast \u274c No <code>gpt-4-turbo</code> GPT-4 Turbo - high capability \u274c No <code>gpt-3.5-turbo</code> GPT-3.5 Turbo - fast and economical \u274c No <code>o1</code> o1 - reasoning model \u274c No <code>o1-mini</code> o1 Mini - efficient reasoning \u274c No <p>See OpenAI documentation for the full list of available models and pricing.</p>"},{"location":"api/graph/generator/","title":"Graph Generator","text":"<p>The <code>generator</code> module provides the <code>GraphGenerator</code> class for generating complete causal graphs from network context using LLMs.</p>"},{"location":"api/graph/generator/#import-pattern","title":"Import Pattern","text":"<pre><code>from causaliq_knowledge.graph import (\n    GraphGenerator,\n    GraphGeneratorConfig,\n    NetworkContext,\n    GeneratedGraph,\n    PromptDetail,\n    OutputFormat,\n)\nfrom causaliq_core.cache import TokenCache\n</code></pre>"},{"location":"api/graph/generator/#overview","title":"Overview","text":"<p><code>GraphGenerator</code> orchestrates the full graph generation workflow:</p> <ol> <li>Create a generator with model and configuration</li> <li>Optionally set up caching with <code>TokenCache</code></li> <li>Generate graphs from variable dictionaries or <code>NetworkContext</code> files</li> <li>Receive structured <code>GeneratedGraph</code> objects with edges and metadata</li> </ol>"},{"location":"api/graph/generator/#quick-start","title":"Quick Start","text":"<p>Here's a complete working example:</p> <pre><code>from causaliq_knowledge.graph import (\n    GraphGenerator,\n    GraphGeneratorConfig,\n    NetworkContext,\n    PromptDetail,\n    OutputFormat,\n)\n\n# Create generator with model identifier\n# Format: \"provider/model_name\"\ngenerator = GraphGenerator(model=\"groq/llama-3.1-8b-instant\")\n\n# Option 1: Generate from a list of variables\ngraph = generator.generate_graph(\n    variables=[\n        {\"name\": \"smoking\"},\n        {\"name\": \"lung_cancer\"},\n        {\"name\": \"age\"},\n    ],\n    domain=\"oncology\",\n)\n\n# Option 2: Generate from a network context file\ncontext = NetworkContext.load(\"research/models/my_model.json\")\ngraph = generator.generate_from_context(context)\n\n# Access the results\nprint(f\"Generated {len(graph.edges)} edges\")\nfor edge in graph.edges:\n    print(f\"  {edge.source} -&gt; {edge.target} ({edge.confidence:.2f})\")\n\n# Access metadata\nprint(f\"Model: {graph.metadata.model}\")\nprint(f\"Latency: {graph.metadata.latency_ms}ms\")\nprint(f\"Cost: ${graph.metadata.cost_usd:.4f}\")\n</code></pre>"},{"location":"api/graph/generator/#configuration","title":"Configuration","text":""},{"location":"api/graph/generator/#graphgeneratorconfig","title":"GraphGeneratorConfig","text":"<p>Configuration dataclass for graph generation parameters.</p> <pre><code>from causaliq_knowledge.graph import GraphGeneratorConfig, PromptDetail, OutputFormat\n\nconfig = GraphGeneratorConfig(\n    temperature=0.1,              # LLM sampling temperature\n    max_tokens=2000,              # Maximum response tokens\n    timeout=60.0,                 # Request timeout in seconds\n    output_format=OutputFormat.EDGE_LIST,  # or ADJACENCY_MATRIX\n    prompt_detail=PromptDetail.STANDARD,   # MINIMAL, STANDARD, or RICH\n    use_llm_names=True,           # Use llm_name field from specs\n    request_id=\"\",                # Optional request identifier\n)\n</code></pre> <p>Attributes:</p> Attribute Type Default Description <code>temperature</code> float 0.1 LLM sampling temperature (lower = more deterministic) <code>max_tokens</code> int 2000 Maximum tokens in LLM response <code>timeout</code> float 60.0 Request timeout in seconds <code>output_format</code> OutputFormat EDGE_LIST Response format (EDGE_LIST or ADJACENCY_MATRIX) <code>prompt_detail</code> PromptDetail STANDARD Detail level (MINIMAL, STANDARD, or RICH) <code>use_llm_names</code> bool True Use llm_name instead of benchmark name <code>request_id</code> str \"\" Optional identifier for requests"},{"location":"api/graph/generator/#creating-a-generator","title":"Creating a Generator","text":"<pre><code>from causaliq_knowledge.graph import GraphGenerator, GraphGeneratorConfig\nfrom causaliq_core.cache import TokenCache\n\n# Basic creation with just a model\ngenerator = GraphGenerator(model=\"groq/llama-3.1-8b-instant\")\n\n# With custom configuration\nconfig = GraphGeneratorConfig(\n    temperature=0.2,\n    prompt_detail=PromptDetail.RICH,\n)\ngenerator = GraphGenerator(model=\"gemini/gemini-2.0-flash\", config=config)\n\n# With caching enabled\ncache = TokenCache(db_path=\"graph_cache.db\")\ngenerator = GraphGenerator(\n    model=\"openai/gpt-4o\",\n    config=config,\n    cache=cache,\n)\n\n# Or set cache after creation\ngenerator = GraphGenerator(model=\"anthropic/claude-sonnet-4-20250514\")\ngenerator.set_cache(cache, use_cache=True)\n</code></pre>"},{"location":"api/graph/generator/#generating-from-variables","title":"Generating from Variables","text":"<p>Use <code>generate_graph()</code> when you have a list of variable dictionaries:</p> <pre><code>from causaliq_knowledge.graph import GraphGenerator, PromptDetail\n\ngenerator = GraphGenerator(model=\"groq/llama-3.1-8b-instant\")\n\n# Minimal - just variable names\ngraph = generator.generate_graph(\n    variables=[\n        {\"name\": \"smoking\"},\n        {\"name\": \"lung_cancer\"},\n        {\"name\": \"age\"},\n        {\"name\": \"genetics\"},\n    ],\n    domain=\"oncology\",\n)\n\n# With more context\ngraph = generator.generate_graph(\n    variables=[\n        {\n            \"name\": \"smoking\",\n            \"type\": \"binary\",\n            \"description\": \"Whether the patient smokes\",\n        },\n        {\n            \"name\": \"lung_cancer\",\n            \"type\": \"binary\",\n            \"description\": \"Diagnosis of lung cancer\",\n        },\n    ],\n    domain=\"oncology\",\n    level=PromptDetail.RICH,  # Override config's prompt_detail\n)\n\n# Access results\nfor edge in graph.edges:\n    print(f\"{edge.source} -&gt; {edge.target}\")\n    print(f\"  Confidence: {edge.confidence}\")\n    print(f\"  Rationale: {edge.rationale}\")\n</code></pre>"},{"location":"api/graph/generator/#generating-from-network-context","title":"Generating from Network Context","text":"<p>Use <code>generate_from_context()</code> when you have a JSON network context file:</p> <pre><code>from causaliq_knowledge.graph import GraphGenerator, NetworkContext, PromptDetail\n\ngenerator = GraphGenerator(model=\"gemini/gemini-2.0-flash\")\n\n# Load the network context\ncontext = NetworkContext.load(\"research/models/asia/asia.json\")\n\n# Generate with default settings from config\ngraph = generator.generate_from_context(context)\n\n# Override settings for this specific call\ngraph = generator.generate_from_context(\n    context=context,\n    level=PromptDetail.MINIMAL,\n    use_llm_names=False,  # Use benchmark names instead\n)\n</code></pre>"},{"location":"api/graph/generator/#supported-llm-providers","title":"Supported LLM Providers","text":"<p>GraphGenerator supports all providers via the <code>provider/model</code> format:</p> Provider Example Model String Anthropic <code>anthropic/claude-sonnet-4-20250514</code> DeepSeek <code>deepseek/deepseek-chat</code> Gemini <code>gemini/gemini-2.0-flash</code> Groq <code>groq/llama-3.1-8b-instant</code> Mistral <code>mistral/mistral-large-latest</code> Ollama <code>ollama/llama3.2</code> OpenAI <code>openai/gpt-4o</code> <pre><code># Using different providers\ngen_groq = GraphGenerator(model=\"groq/llama-3.1-8b-instant\")\ngen_gemini = GraphGenerator(model=\"gemini/gemini-2.0-flash\")\ngen_openai = GraphGenerator(model=\"openai/gpt-4o\")\ngen_anthropic = GraphGenerator(model=\"anthropic/claude-sonnet-4-20250514\")\n</code></pre>"},{"location":"api/graph/generator/#caching","title":"Caching","text":"<p>GraphGenerator integrates with <code>TokenCache</code> for response caching:</p> <pre><code>from causaliq_knowledge.graph import GraphGenerator\nfrom causaliq_core.cache import TokenCache\n\n# Create cache and generator\ncache = TokenCache(db_path=\"graph_cache.db\")\ngenerator = GraphGenerator(\n    model=\"gemini/gemini-2.0-flash\",\n    cache=cache,\n)\n\n# First call - hits the LLM\ngraph1 = generator.generate_graph(\n    variables=[{\"name\": \"A\"}, {\"name\": \"B\"}],\n    domain=\"test\",\n)\nprint(f\"From cache: {graph1.metadata.from_cache}\")  # False\n\n# Second call with same inputs - uses cache\ngraph2 = generator.generate_graph(\n    variables=[{\"name\": \"A\"}, {\"name\": \"B\"}],\n    domain=\"test\",\n)\nprint(f\"From cache: {graph2.metadata.from_cache}\")  # True\n\n# Disable caching for specific generator\ngenerator.set_cache(cache, use_cache=False)\n</code></pre>"},{"location":"api/graph/generator/#prompt-detail-levels","title":"Prompt Detail Levels","text":"<p>Control the amount of context provided to the LLM:</p> Level Description <code>PromptDetail.MINIMAL</code> Variable names only <code>PromptDetail.STANDARD</code> Names, types, and brief descriptions <code>PromptDetail.RICH</code> Full descriptions, roles, states, and constraints <pre><code>from causaliq_knowledge.graph import GraphGenerator, GraphGeneratorConfig, PromptDetail\n\n# Set at config level (default for all calls)\nconfig = GraphGeneratorConfig(prompt_detail=PromptDetail.MINIMAL)\ngenerator = GraphGenerator(model=\"groq/llama-3.1-8b-instant\", config=config)\n\n# Override per call\ngraph = generator.generate_graph(\n    variables=[{\"name\": \"A\"}, {\"name\": \"B\"}],\n    domain=\"test\",\n    level=PromptDetail.RICH,  # Use rich for this call only\n)\n</code></pre>"},{"location":"api/graph/generator/#output-formats","title":"Output Formats","text":"<p>Choose between edge list and adjacency matrix output:</p> <pre><code>from causaliq_knowledge.graph import GraphGenerator, GraphGeneratorConfig, OutputFormat\n\n# Edge list format (default)\nconfig = GraphGeneratorConfig(output_format=OutputFormat.EDGE_LIST)\ngenerator = GraphGenerator(model=\"groq/llama-3.1-8b-instant\", config=config)\n\n# Adjacency matrix format\nconfig = GraphGeneratorConfig(output_format=OutputFormat.ADJACENCY_MATRIX)\ngenerator = GraphGenerator(model=\"groq/llama-3.1-8b-instant\", config=config)\n</code></pre>"},{"location":"api/graph/generator/#working-with-results","title":"Working with Results","text":""},{"location":"api/graph/generator/#generatedgraph","title":"GeneratedGraph","text":"<p>The result of generation is a <code>GeneratedGraph</code> object:</p> <pre><code>graph = generator.generate_graph(...)\n\n# Access edges\nfor edge in graph.edges:\n    print(f\"Source: {edge.source}\")\n    print(f\"Target: {edge.target}\")\n    print(f\"Confidence: {edge.confidence}\")\n    print(f\"Rationale: {edge.rationale}\")\n\n# Access metadata\nmeta = graph.metadata\nprint(f\"Model: {meta.model}\")\nprint(f\"Provider: {meta.provider}\")\nprint(f\"Timestamp: {meta.timestamp}\")\nprint(f\"Latency: {meta.latency_ms}ms\")\nprint(f\"Input tokens: {meta.input_tokens}\")\nprint(f\"Output tokens: {meta.output_tokens}\")\nprint(f\"Cost: ${meta.cost_usd:.6f}\")\nprint(f\"From cache: {meta.from_cache}\")\n</code></pre>"},{"location":"api/graph/generator/#generator-statistics","title":"Generator Statistics","text":"<pre><code>generator = GraphGenerator(model=\"groq/llama-3.1-8b-instant\")\n\n# After some generations...\nstats = generator.get_stats()\nprint(f\"Model: {stats['model']}\")\nprint(f\"Call count: {stats['call_count']}\")\nprint(f\"Client call count: {stats['client_call_count']}\")\n</code></pre>"},{"location":"api/graph/generator/#complete-example","title":"Complete Example","text":"<p>Here's a full example showing a typical workflow:</p> <pre><code>\"\"\"Generate a causal graph from a model specification.\"\"\"\n\nfrom pathlib import Path\n\nfrom causaliq_knowledge.graph import (\n    GraphGenerator,\n    GraphGeneratorConfig,\n    ModelLoader,\n    PromptDetail,\n    OutputFormat,\n)\nfrom causaliq_core.cache import TokenCache\n\n\ndef main():\n    # Set up caching\n    cache = TokenCache(db_path=Path(\"cache/graph_cache.db\"))\n\n    # Configure the generator\n    config = GraphGeneratorConfig(\n        temperature=0.1,\n        max_tokens=2000,\n        prompt_detail=PromptDetail.STANDARD,\n        output_format=OutputFormat.EDGE_LIST,\n    )\n\n    # Create generator\n    generator = GraphGenerator(\n        model=\"groq/llama-3.1-8b-instant\",\n        config=config,\n        cache=cache,\n    )\n\n    # Load model specification\n    spec = ModelLoader.load(\"research/models/asia/asia.json\")\n    print(f\"Loaded spec: {spec.name}\")\n    print(f\"Variables: {len(spec.variables)}\")\n\n    # Generate graph\n    graph = generator.generate_from_spec(spec)\n\n    # Display results\n    print(f\"\\nGenerated {len(graph.edges)} edges:\")\n    for edge in graph.edges:\n        print(f\"  {edge.source} -&gt; {edge.target} ({edge.confidence:.2f})\")\n\n    # Show metadata\n    print(f\"\\nMetadata:\")\n    print(f\"  Model: {graph.metadata.provider}/{graph.metadata.model}\")\n    print(f\"  Latency: {graph.metadata.latency_ms}ms\")\n    print(f\"  Tokens: {graph.metadata.input_tokens} in, \"\n          f\"{graph.metadata.output_tokens} out\")\n    print(f\"  Cost: ${graph.metadata.cost_usd:.6f}\")\n    print(f\"  Cached: {graph.metadata.from_cache}\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"api/graph/generator/#api-reference","title":"API Reference","text":"<p>Graph generator using LLM providers.</p> <p>This module provides the GraphGenerator class for generating complete causal graphs from variable specifications using LLM providers.</p> <p>Classes:</p> <ul> <li> <code>GraphGeneratorConfig</code>           \u2013            <p>Configuration for the GraphGenerator.</p> </li> <li> <code>GraphGenerator</code>           \u2013            <p>Generate causal graphs from network context using LLMs.</p> </li> </ul>"},{"location":"api/graph/generator/#causaliq_knowledge.graph.generator.GraphGeneratorConfig","title":"GraphGeneratorConfig  <code>dataclass</code>","text":"<pre><code>GraphGeneratorConfig(\n    temperature: float = 0.1,\n    max_tokens: int = 2000,\n    timeout: float = 60.0,\n    output_format: OutputFormat = EDGE_LIST,\n    prompt_detail: PromptDetail = STANDARD,\n    use_llm_names: bool = True,\n    request_id: str = \"\",\n)\n</code></pre> <p>Configuration for the GraphGenerator.</p> <p>Attributes:</p> <ul> <li> <code>temperature</code>               (<code>float</code>)           \u2013            <p>LLM sampling temperature (lower = more deterministic).</p> </li> <li> <code>max_tokens</code>               (<code>int</code>)           \u2013            <p>Maximum tokens in LLM response.</p> </li> <li> <code>timeout</code>               (<code>float</code>)           \u2013            <p>Request timeout in seconds.</p> </li> <li> <code>output_format</code>               (<code>OutputFormat</code>)           \u2013            <p>Desired output format (edge_list or adjacency_matrix).</p> </li> <li> <code>prompt_detail</code>               (<code>PromptDetail</code>)           \u2013            <p>Detail level for variable information in prompts.</p> </li> <li> <code>use_llm_names</code>               (<code>bool</code>)           \u2013            <p>Use llm_name instead of benchmark name in prompts.</p> </li> <li> <code>request_id</code>               (<code>str</code>)           \u2013            <p>Optional identifier for requests (stored in metadata).</p> </li> </ul>"},{"location":"api/graph/generator/#causaliq_knowledge.graph.generator.GraphGenerator","title":"GraphGenerator","text":"<pre><code>GraphGenerator(\n    model: str = \"groq/llama-3.1-8b-instant\",\n    config: Optional[GraphGeneratorConfig] = None,\n    cache: Optional[\"TokenCache\"] = None,\n)\n</code></pre> <p>Generate causal graphs from network context using LLMs.</p> <p>This class provides methods for generating complete causal graphs from NetworkContext objects or variable dictionaries. It supports all LLM providers available in causaliq-knowledge and integrates with the TokenCache for efficient caching of requests.</p> <p>Attributes:</p> <ul> <li> <code>model</code>               (<code>str</code>)           \u2013            <p>The LLM model identifier (e.g., \"groq/llama-3.1-8b-instant\").</p> </li> <li> <code>config</code>               (<code>GraphGeneratorConfig</code>)           \u2013            <p>Configuration for generation parameters.</p> </li> </ul> Example <p>from causaliq_knowledge.graph import NetworkContext from causaliq_knowledge.graph.generator import GraphGenerator</p> <p>Parameters:</p> <ul> <li> </li> <li> </li> <li> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the model prefix is not supported.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>generate_from_context</code>             \u2013              <p>Generate a causal graph from a NetworkContext.</p> </li> <li> <code>generate_graph</code>             \u2013              <p>Generate a causal graph from variable dictionaries.</p> </li> <li> <code>get_stats</code>             \u2013              <p>Get statistics about generation calls.</p> </li> <li> <code>set_cache</code>             \u2013              <p>Configure caching for this generator.</p> </li> </ul>"},{"location":"api/graph/generator/#causaliq_knowledge.graph.generator.GraphGenerator--load-network-context","title":"Load network context","text":"<p>context = NetworkContext.load(\"asia.json\")</p>"},{"location":"api/graph/generator/#causaliq_knowledge.graph.generator.GraphGenerator--create-generator","title":"Create generator","text":"<p>generator = GraphGenerator(model=\"groq/llama-3.1-8b-instant\")</p>"},{"location":"api/graph/generator/#causaliq_knowledge.graph.generator.GraphGenerator--generate-graph","title":"Generate graph","text":"<p>graph = generator.generate_from_context(context) print(f\"Generated {len(graph.edges)} edges\")</p>"},{"location":"api/graph/generator/#causaliq_knowledge.graph.generator.GraphGenerator(model)","title":"<code>model</code>","text":"(<code>str</code>, default:                   <code>'groq/llama-3.1-8b-instant'</code> )           \u2013            <p>LLM model identifier with provider prefix. Supported: - \"groq/llama-3.1-8b-instant\" (Groq API) - \"gemini/gemini-2.5-flash\" (Google Gemini) - \"openai/gpt-4o\" (OpenAI) - \"anthropic/claude-3-5-sonnet-20241022\" (Anthropic) - \"deepseek/deepseek-chat\" (DeepSeek) - \"mistral/mistral-small-latest\" (Mistral) - \"ollama/llama3.2:1b\" (Local Ollama)</p>"},{"location":"api/graph/generator/#causaliq_knowledge.graph.generator.GraphGenerator(config)","title":"<code>config</code>","text":"(<code>Optional[GraphGeneratorConfig]</code>, default:                   <code>None</code> )           \u2013            <p>Generation configuration. Uses defaults if None.</p>"},{"location":"api/graph/generator/#causaliq_knowledge.graph.generator.GraphGenerator(cache)","title":"<code>cache</code>","text":"(<code>Optional['TokenCache']</code>, default:                   <code>None</code> )           \u2013            <p>TokenCache instance for caching. Disabled if None.</p>"},{"location":"api/graph/generator/#causaliq_knowledge.graph.generator.GraphGenerator.call_count","title":"call_count  <code>property</code>","text":"<pre><code>call_count: int\n</code></pre> <p>Return the number of generation calls made.</p>"},{"location":"api/graph/generator/#causaliq_knowledge.graph.generator.GraphGenerator.config","title":"config  <code>property</code>","text":"<pre><code>config: GraphGeneratorConfig\n</code></pre> <p>Return the generator configuration.</p>"},{"location":"api/graph/generator/#causaliq_knowledge.graph.generator.GraphGenerator.model","title":"model  <code>property</code>","text":"<pre><code>model: str\n</code></pre> <p>Return the model identifier.</p>"},{"location":"api/graph/generator/#causaliq_knowledge.graph.generator.GraphGenerator.generate_from_context","title":"generate_from_context","text":"<pre><code>generate_from_context(\n    context: \"NetworkContext\",\n    level: Optional[PromptDetail] = None,\n    output_format: Optional[OutputFormat] = None,\n    system_prompt: Optional[str] = None,\n    use_llm_names: Optional[bool] = None,\n) -&gt; GeneratedGraph\n</code></pre> <p>Generate a causal graph from a NetworkContext.</p> <p>Convenience method that extracts variables and domain from the context automatically.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>GeneratedGraph</code>           \u2013            <p>GeneratedGraph with proposed edges and metadata.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If LLM response cannot be parsed.</p> </li> </ul>"},{"location":"api/graph/generator/#causaliq_knowledge.graph.generator.GraphGenerator.generate_from_context(context)","title":"<code>context</code>","text":"(<code>'NetworkContext'</code>)           \u2013            <p>The network context.</p>"},{"location":"api/graph/generator/#causaliq_knowledge.graph.generator.GraphGenerator.generate_from_context(level)","title":"<code>level</code>","text":"(<code>Optional[PromptDetail]</code>, default:                   <code>None</code> )           \u2013            <p>View level for context. Uses config default if None.</p>"},{"location":"api/graph/generator/#causaliq_knowledge.graph.generator.GraphGenerator.generate_from_context(output_format)","title":"<code>output_format</code>","text":"(<code>Optional[OutputFormat]</code>, default:                   <code>None</code> )           \u2013            <p>Output format. Uses config default if None.</p>"},{"location":"api/graph/generator/#causaliq_knowledge.graph.generator.GraphGenerator.generate_from_context(system_prompt)","title":"<code>system_prompt</code>","text":"(<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Custom system prompt (optional).</p>"},{"location":"api/graph/generator/#causaliq_knowledge.graph.generator.GraphGenerator.generate_from_context(use_llm_names)","title":"<code>use_llm_names</code>","text":"(<code>Optional[bool]</code>, default:                   <code>None</code> )           \u2013            <p>Use llm_name instead of benchmark name. Uses config default if None.</p>"},{"location":"api/graph/generator/#causaliq_knowledge.graph.generator.GraphGenerator.generate_graph","title":"generate_graph","text":"<pre><code>generate_graph(\n    variables: List[Dict[str, Any]],\n    level: Optional[PromptDetail] = None,\n    domain: Optional[str] = None,\n    output_format: Optional[OutputFormat] = None,\n    system_prompt: Optional[str] = None,\n) -&gt; GeneratedGraph\n</code></pre> <p>Generate a causal graph from variable dictionaries.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>GeneratedGraph</code>           \u2013            <p>GeneratedGraph with proposed edges and metadata.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If LLM response cannot be parsed.</p> </li> </ul>"},{"location":"api/graph/generator/#causaliq_knowledge.graph.generator.GraphGenerator.generate_graph(variables)","title":"<code>variables</code>","text":"(<code>List[Dict[str, Any]]</code>)           \u2013            <p>List of variable dictionaries with at least \"name\".</p>"},{"location":"api/graph/generator/#causaliq_knowledge.graph.generator.GraphGenerator.generate_graph(level)","title":"<code>level</code>","text":"(<code>Optional[PromptDetail]</code>, default:                   <code>None</code> )           \u2013            <p>View level for context. Uses config default if None.</p>"},{"location":"api/graph/generator/#causaliq_knowledge.graph.generator.GraphGenerator.generate_graph(domain)","title":"<code>domain</code>","text":"(<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional domain context for the query.</p>"},{"location":"api/graph/generator/#causaliq_knowledge.graph.generator.GraphGenerator.generate_graph(output_format)","title":"<code>output_format</code>","text":"(<code>Optional[OutputFormat]</code>, default:                   <code>None</code> )           \u2013            <p>Output format. Uses config default if None.</p>"},{"location":"api/graph/generator/#causaliq_knowledge.graph.generator.GraphGenerator.generate_graph(system_prompt)","title":"<code>system_prompt</code>","text":"(<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Custom system prompt (optional).</p>"},{"location":"api/graph/generator/#causaliq_knowledge.graph.generator.GraphGenerator.get_stats","title":"get_stats","text":"<pre><code>get_stats() -&gt; Dict[str, Any]\n</code></pre> <p>Get statistics about generation calls.</p> <p>Returns:</p> <ul> <li> <code>Dict[str, Any]</code>           \u2013            <p>Dict with call_count, model, and client stats.</p> </li> </ul>"},{"location":"api/graph/generator/#causaliq_knowledge.graph.generator.GraphGenerator.set_cache","title":"set_cache","text":"<pre><code>set_cache(cache: Optional['TokenCache'], use_cache: bool = True) -&gt; None\n</code></pre> <p>Configure caching for this generator.</p> <p>Parameters:</p>"},{"location":"api/graph/generator/#causaliq_knowledge.graph.generator.GraphGenerator.set_cache(cache)","title":"<code>cache</code>","text":"(<code>Optional['TokenCache']</code>)           \u2013            <p>TokenCache instance for caching, or None to disable.</p>"},{"location":"api/graph/generator/#causaliq_knowledge.graph.generator.GraphGenerator.set_cache(use_cache)","title":"<code>use_cache</code>","text":"(<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use the cache (default True).</p>"},{"location":"api/graph/models/","title":"Network Context API Reference","text":"<p>Pydantic models for defining network context specifications in JSON format.</p>"},{"location":"api/graph/models/#overview","title":"Overview","text":"<p>Network context specifications define the variables and metadata for a causal network, enabling LLMs to generate causal graphs with appropriate domain context.</p> <pre><code>from causaliq_knowledge.graph import (\n    NetworkContext,\n    NetworkLoadError,\n    VariableSpec,\n    VariableType,\n    VariableRole,\n    PromptDetails,\n    ViewDefinition,\n)\n</code></pre>"},{"location":"api/graph/models/#variabletype","title":"VariableType","text":"<p>Enumeration of supported variable types.</p> <pre><code>class VariableType(str, Enum):\n    BINARY = \"binary\"           # Two states (e.g., yes/no)\n    CATEGORICAL = \"categorical\" # Multiple unordered states\n    ORDINAL = \"ordinal\"         # Multiple ordered states\n    CONTINUOUS = \"continuous\"   # Numeric values\n</code></pre> <p>Example:</p> <pre><code>from causaliq_knowledge.graph import VariableType\n\nvar_type = VariableType.BINARY\nprint(var_type.value)  # \"binary\"\n</code></pre>"},{"location":"api/graph/models/#variablerole","title":"VariableRole","text":"<p>Enumeration of causal roles in the graph structure.</p> <pre><code>class VariableRole(str, Enum):\n    EXOGENOUS = \"exogenous\"   # No parents (root cause)\n    ENDOGENOUS = \"endogenous\" # Has parents (caused by other variables)\n    LATENT = \"latent\"         # Unobserved variable\n</code></pre> <p>Example:</p> <pre><code>from causaliq_knowledge.graph import VariableRole\n\nrole = VariableRole.EXOGENOUS\nprint(role.value)  # \"exogenous\"\n</code></pre>"},{"location":"api/graph/models/#variablespec","title":"VariableSpec","text":"<p>Specification for a single variable in the causal model.</p>"},{"location":"api/graph/models/#attributes","title":"Attributes","text":"Attribute Type Required Description <code>name</code> <code>str</code> Yes Benchmark/literature name for ground truth <code>llm_name</code> <code>str</code> No Name used for LLM queries (defaults to name) <code>type</code> <code>VariableType</code> Yes Variable type (binary, categorical, etc.) <code>display_name</code> <code>str</code> No Human-readable display name <code>aliases</code> <code>list[str]</code> No Alternative names for the variable <code>states</code> <code>list[str]</code> No Possible values for discrete variables <code>role</code> <code>VariableRole</code> No Causal role (exogenous, endogenous, latent) <code>category</code> <code>str</code> No Domain-specific category <code>short_description</code> <code>str</code> No Brief description of the variable <code>extended_description</code> <code>str</code> No Detailed description with domain context <code>base_rate</code> <code>dict[str, float]</code> No Prior probabilities for each state <code>conditional_rates</code> <code>dict</code> No Conditional probabilities <code>sensitivity_hints</code> <code>str</code> No Hints about causal relationships <code>related_domain_knowledge</code> <code>list[str]</code> No Domain knowledge statements <code>references</code> <code>list[str]</code> No Literature references"},{"location":"api/graph/models/#llm-name-vs-benchmark-name","title":"LLM Name vs Benchmark Name","text":"<p>The <code>name</code> and <code>llm_name</code> fields enable semantic disguising to reduce LLM memorisation of well-known benchmark networks:</p> <ul> <li><code>name</code>: The benchmark/literature name used for ground truth evaluation</li> <li><code>llm_name</code>: The name sent to the LLM (defaults to <code>name</code> if not specified)</li> </ul> <p>Example: For the ASIA network's \"Tuberculosis\" variable:</p> <pre><code>VariableSpec(\n    name=\"tub\",              # Original benchmark name\n    llm_name=\"HasTB\",        # Meaningful but non-canonical name for LLM\n    display_name=\"Tuberculosis Status\",\n    type=VariableType.BINARY,\n)\n</code></pre>"},{"location":"api/graph/models/#example","title":"Example","text":"<pre><code>from causaliq_knowledge.graph import VariableSpec, VariableType, VariableRole\n\nsmoking = VariableSpec(\n    name=\"smoke\",\n    llm_name=\"tobacco_history\",\n    type=VariableType.BINARY,\n    states=[\"never\", \"ever\"],\n    role=VariableRole.EXOGENOUS,\n    short_description=\"Patient has history of tobacco smoking.\",\n    extended_description=\"Self-reported smoking history, known risk factor.\",\n    base_rate={\"never\": 0.7, \"ever\": 0.3},\n)\n</code></pre>"},{"location":"api/graph/models/#viewdefinition","title":"ViewDefinition","text":"<p>Configuration for a single context view level.</p>"},{"location":"api/graph/models/#attributes_1","title":"Attributes","text":"Attribute Type Required Description <code>description</code> <code>str</code> No Human-readable description of the view <code>include_fields</code> <code>list[str]</code> Yes Variable fields to include in this view"},{"location":"api/graph/models/#example_1","title":"Example","text":"<pre><code>from causaliq_knowledge.graph import ViewDefinition\n\nminimal_view = ViewDefinition(\n    description=\"Variable names only\",\n    include_fields=[\"name\"]\n)\n\nstandard_view = ViewDefinition(\n    description=\"Names with basic metadata\",\n    include_fields=[\"name\", \"type\", \"short_description\", \"states\"]\n)\n</code></pre>"},{"location":"api/graph/models/#promptdetails","title":"PromptDetails","text":"<p>Container for the three standard prompt detail levels.</p>"},{"location":"api/graph/models/#attributes_2","title":"Attributes","text":"Attribute Type Description <code>minimal</code> <code>ViewDefinition</code> Minimal context (names only) <code>standard</code> <code>ViewDefinition</code> Standard context (names + descriptions) <code>rich</code> <code>ViewDefinition</code> Rich context (full metadata)"},{"location":"api/graph/models/#default-prompt-details","title":"Default Prompt Details","text":"<p>If not specified, the following defaults are used:</p> <pre><code>PromptDetails(\n    minimal=ViewDefinition(include_fields=[\"name\"]),\n    standard=ViewDefinition(\n        include_fields=[\"name\", \"type\", \"short_description\", \"states\"]\n    ),\n    rich=ViewDefinition(\n        include_fields=[\n            \"name\", \"type\", \"role\", \"short_description\",\n            \"extended_description\", \"states\", \"sensitivity_hints\"\n        ]\n    ),\n)\n</code></pre>"},{"location":"api/graph/models/#provenance","title":"Provenance","text":"<p>Provenance information for the model specification.</p>"},{"location":"api/graph/models/#attributes_3","title":"Attributes","text":"Attribute Type Description <code>source_network</code> <code>str</code> Name of the source benchmark network <code>source_reference</code> <code>str</code> Citation for the original source <code>source_url</code> <code>str</code> URL to the source data <code>disguise_strategy</code> <code>str</code> Strategy used for variable name disguising <code>memorization_risk</code> <code>str</code> Risk level for LLM memorisation <code>notes</code> <code>str</code> Additional notes about the source"},{"location":"api/graph/models/#llmguidance","title":"LLMGuidance","text":"<p>Guidance for LLM interactions with the model.</p>"},{"location":"api/graph/models/#attributes_4","title":"Attributes","text":"Attribute Type Description <code>usage_notes</code> <code>list[str]</code> Notes about using this model with LLMs <code>do_not_provide</code> <code>list[str]</code> Information to withhold from LLMs <code>expected_difficulty</code> <code>str</code> Expected difficulty level"},{"location":"api/graph/models/#constraints","title":"Constraints","text":"<p>Structural constraints on the causal graph.</p>"},{"location":"api/graph/models/#attributes_5","title":"Attributes","text":"Attribute Type Description <code>forbidden_edges</code> <code>list[list[str]]</code> Edges that must not exist <code>required_edges</code> <code>list[list[str]]</code> Edges that must exist <code>partial_order</code> <code>list[list[str]]</code> Temporal ordering constraints <code>causal_principles</code> <code>list[CausalPrinciple]</code> Domain causal principles"},{"location":"api/graph/models/#groundtruth","title":"GroundTruth","text":"<p>Ground truth edges for evaluation.</p>"},{"location":"api/graph/models/#attributes_6","title":"Attributes","text":"Attribute Type Description <code>edges_expert</code> <code>list[list[str]]</code> Expert-defined edges <code>edges_experiment</code> <code>list[list[str]]</code> Experimentally-derived edges <code>edges_observational</code> <code>list[list[str]]</code> Observationally-derived edges"},{"location":"api/graph/models/#networkcontext","title":"NetworkContext","text":"<p>Complete network context for LLM-based causal graph generation.</p> <p>Provides domain and variable information needed to generate causal graphs using LLMs. This is not the network itself, but the context required to generate one.</p>"},{"location":"api/graph/models/#attributes_7","title":"Attributes","text":"Attribute Type Required Description <code>schema_version</code> <code>str</code> No Schema version (default: \"2.0\") <code>network</code> <code>str</code> Yes Network identifier (e.g., \"asia\") <code>domain</code> <code>str</code> Yes Domain of the model (e.g., \"pulmonary_oncology\") <code>purpose</code> <code>str</code> No Purpose of the context specification <code>variables</code> <code>list[VariableSpec]</code> Yes List of variable specifications <code>provenance</code> <code>Provenance</code> No Source and provenance information <code>llm_guidance</code> <code>LLMGuidance</code> No Guidance for LLM interactions <code>prompt_details</code> <code>PromptDetails</code> No Prompt detail definitions (uses defaults if omitted) <code>constraints</code> <code>Constraints</code> No Structural constraints <code>causal_principles</code> <code>list[CausalPrinciple]</code> No Domain causal principles <code>ground_truth</code> <code>GroundTruth</code> No Ground truth for evaluation"},{"location":"api/graph/models/#class-methods","title":"Class Methods","text":""},{"location":"api/graph/models/#loadpath-str-path-networkcontext","title":"<code>load(path: str | Path) -&gt; NetworkContext</code>","text":"<p>Load a network context from a JSON file.</p> <pre><code>context = NetworkContext.load(\"models/cancer.json\")\n</code></pre> <p>Raises: <code>NetworkLoadError</code> - If the file cannot be read or validation fails.</p>"},{"location":"api/graph/models/#from_dictdata-dict-source_path-path-none-none-networkcontext","title":"<code>from_dict(data: dict, source_path: Path | None = None) -&gt; NetworkContext</code>","text":"<p>Create a network context from a dictionary.</p> <pre><code>context = NetworkContext.from_dict({\n    \"network\": \"test\",\n    \"domain\": \"testing\",\n    \"variables\": [{\"name\": \"X\", \"type\": \"binary\"}]\n})\n</code></pre>"},{"location":"api/graph/models/#load_and_validatepath-str-path-tuplenetworkcontext-liststr","title":"<code>load_and_validate(path: str | Path) -&gt; tuple[NetworkContext, list[str]]</code>","text":"<p>Load and fully validate a network context, returning warnings.</p> <pre><code>context, warnings = NetworkContext.load_and_validate(\"model.json\")\nfor warning in warnings:\n    print(f\"Warning: {warning}\")\n</code></pre>"},{"location":"api/graph/models/#instance-methods","title":"Instance Methods","text":""},{"location":"api/graph/models/#get_variable_names-liststr","title":"<code>get_variable_names() -&gt; list[str]</code>","text":"<p>Return list of all benchmark variable names.</p> <pre><code>context = NetworkContext.load(\"model.json\")\nnames = context.get_variable_names()\n# [\"smoking\", \"cancer\", \"age\"]\n</code></pre>"},{"location":"api/graph/models/#get_llm_names-liststr","title":"<code>get_llm_names() -&gt; list[str]</code>","text":"<p>Return list of all LLM variable names.</p> <pre><code>context = NetworkContext.load(\"model.json\")\nllm_names = context.get_llm_names()\n# [\"tobacco_use\", \"malignancy\", \"patient_age\"]\n</code></pre>"},{"location":"api/graph/models/#get_variablename-str-variablespec-none","title":"<code>get_variable(name: str) -&gt; VariableSpec | None</code>","text":"<p>Get a variable specification by name.</p> <pre><code>context = NetworkContext.load(\"model.json\")\nsmoking = context.get_variable(\"smoking\")\n</code></pre>"},{"location":"api/graph/models/#get_llm_to_name_mapping-dictstr-str","title":"<code>get_llm_to_name_mapping() -&gt; dict[str, str]</code>","text":"<p>Get mapping from LLM names to benchmark names.</p> <pre><code>mapping = context.get_llm_to_name_mapping()\n# {\"tobacco_use\": \"smoking\", \"malignancy\": \"cancer\"}\n</code></pre>"},{"location":"api/graph/models/#uses_distinct_llm_names-bool","title":"<code>uses_distinct_llm_names() -&gt; bool</code>","text":"<p>Check if any variable has a different llm_name from name.</p> <pre><code>if context.uses_distinct_llm_names():\n    print(\"Context uses LLM name disguising\")\n</code></pre>"},{"location":"api/graph/models/#validate_variables-liststr","title":"<code>validate_variables() -&gt; list[str]</code>","text":"<p>Validate variable specifications and return warnings.</p> <pre><code>warnings = context.validate_variables()\nfor warning in warnings:\n    print(f\"Warning: {warning}\")\n</code></pre>"},{"location":"api/graph/models/#example_2","title":"Example","text":"<pre><code>from causaliq_knowledge.graph import (\n    NetworkContext,\n    VariableSpec,\n    VariableType,\n    VariableRole,\n)\n\ncontext = NetworkContext(\n    network=\"smoking_cancer\",\n    domain=\"epidemiology\",\n    purpose=\"Causal model for smoking and cancer\",\n    variables=[\n        VariableSpec(\n            name=\"smoking\",\n            llm_name=\"tobacco_use\",\n            type=VariableType.BINARY,\n            role=VariableRole.EXOGENOUS,\n            short_description=\"Smoking status\",\n        ),\n        VariableSpec(\n            name=\"cancer\",\n            llm_name=\"malignancy\",\n            type=VariableType.BINARY,\n            role=VariableRole.ENDOGENOUS,\n            short_description=\"Cancer diagnosis\",\n        ),\n    ],\n)\n</code></pre>"},{"location":"api/graph/models/#networkloaderror","title":"NetworkLoadError","text":"<p>Exception raised when network context loading fails.</p>"},{"location":"api/graph/models/#attributes_8","title":"Attributes","text":"Attribute Type Description <code>message</code> <code>str</code> Error description <code>path</code> <code>Path \\| str \\| None</code> Path to the file that failed <code>details</code> <code>str \\| None</code> Additional error details"},{"location":"api/graph/models/#example_3","title":"Example","text":"<pre><code>from causaliq_knowledge.graph import NetworkContext, NetworkLoadError\n\ntry:\n    context = NetworkContext.load(\"nonexistent.json\")\nexcept NetworkLoadError as e:\n    print(f\"Failed to load: {e.message}\")\n    if e.path:\n        print(f\"File: {e.path}\")\n</code></pre>"},{"location":"api/graph/models/#json-schema","title":"JSON Schema","text":"<p>Network context specifications are typically stored as JSON files. See Network Context Format for the complete JSON schema and examples.</p>"},{"location":"api/graph/overview/","title":"Graph Module API Reference","text":"<p>The <code>graph</code> module provides functionality for LLM-based causal graph generation from network context specifications.</p>"},{"location":"api/graph/overview/#quick-start","title":"Quick Start","text":"<p>Generate a causal graph in Python:</p> <pre><code>from causaliq_knowledge.graph import GraphGenerator, NetworkContext\n\n# Create a generator with your chosen model\ngenerator = GraphGenerator(model=\"groq/llama-3.1-8b-instant\")\n\n# Load a network context and generate\ncontext = NetworkContext.load(\"research/models/asia/asia.json\")\ngraph = generator.generate_from_context(context)\n\n# Access the results\nfor edge in graph.edges:\n    print(f\"{edge.source} -&gt; {edge.target}\")\n</code></pre> <p>For complete examples and configuration options, see Graph Generator.</p>"},{"location":"api/graph/overview/#import-patterns","title":"Import Patterns","text":"<p>All graph module classes are available from <code>causaliq_knowledge.graph</code>:</p> <pre><code>from causaliq_knowledge.graph import (\n    # Network context (main model)\n    NetworkContext,\n    NetworkLoadError,\n    # Variable specification\n    VariableSpec,\n    VariableType,\n    VariableRole,\n    # Supporting models\n    PromptDetails,\n    ViewDefinition,\n    Provenance,\n    LLMGuidance,\n    Constraints,\n    CausalPrinciple,\n    GroundTruth,\n    # Filtering\n    ViewFilter,\n    PromptDetail,\n    # Prompts\n    GraphQueryPrompt,\n    OutputFormat,\n    EDGE_LIST_RESPONSE_SCHEMA,\n    ADJACENCY_MATRIX_RESPONSE_SCHEMA,\n    # Response models\n    ProposedEdge,\n    GeneratedGraph,\n    GenerationMetadata,\n    parse_graph_response,\n    # Graph generation\n    GraphGenerator,\n    GraphGeneratorConfig,\n    # Parameters\n    GenerateGraphParams,\n    # Cache integration\n    GraphCompressor,\n)\n</code></pre>"},{"location":"api/graph/overview/#modules","title":"Modules","text":""},{"location":"api/graph/overview/#network-context","title":"Network Context","text":"<p>Pydantic models for defining network context specifications:</p> <ul> <li>NetworkContext - Complete network context with variables and metadata</li> <li>NetworkLoadError - Exception for context loading failures</li> <li>VariableSpec - Single variable definition with type, role, descriptions</li> <li>VariableType - Enum for variable types (binary, categorical, ordinal, continuous)</li> <li>VariableRole - Enum for causal roles (exogenous, endogenous, latent)</li> <li>PromptDetails - Prompt detail definitions for minimal/standard/rich context levels</li> <li>ViewDefinition - Single view configuration with included fields</li> </ul>"},{"location":"api/graph/overview/#view-filter","title":"View Filter","text":"<p>Filtering network context to extract specific context levels:</p> <ul> <li>ViewFilter - Extract minimal/standard/rich views from NetworkContext</li> <li>PromptDetail - Enum for context levels (MINIMAL, STANDARD, RICH)</li> </ul>"},{"location":"api/graph/overview/#graph-prompts","title":"Graph Prompts","text":"<p>Prompt builders for LLM graph generation queries:</p> <ul> <li>GraphQueryPrompt - Builder for system and user prompts</li> <li>OutputFormat - Enum for response formats (edge list, adjacency matrix)</li> <li>Response schemas for validation</li> </ul>"},{"location":"api/graph/overview/#response-models","title":"Response Models","text":"<p>Data models and parsing for LLM graph generation responses:</p> <ul> <li>ProposedEdge - Single proposed causal edge with confidence</li> <li>GeneratedGraph - Complete generated graph with edges and metadata</li> <li>GenerationMetadata - Metadata about the generation process</li> <li>parse_graph_response - Parse LLM responses into structured objects</li> </ul>"},{"location":"api/graph/overview/#graph-generator","title":"Graph Generator","text":"<p>High-level graph generation orchestration:</p> <ul> <li>GraphGenerator - Main class for generating causal graphs via LLMs</li> <li>GraphGeneratorConfig - Configuration for generation parameters</li> <li>Support for all LLM providers with caching integration</li> </ul>"},{"location":"api/graph/prompts/","title":"Graph Prompts API Reference","text":"<p>Prompt templates for LLM graph generation queries.</p>"},{"location":"api/graph/prompts/#overview","title":"Overview","text":"<p>This module provides prompt builders for generating complete causal graphs from variable specifications. These are distinct from the edge-by-edge queries in the <code>llm.prompts</code> module.</p> <pre><code>from causaliq_knowledge.graph.prompts import (\n    GraphQueryPrompt,\n    OutputFormat,\n)\n</code></pre>"},{"location":"api/graph/prompts/#outputformat","title":"OutputFormat","text":"<p>Enumeration of output formats for graph generation responses.</p> <pre><code>class OutputFormat(str, Enum):\n    EDGE_LIST = \"edge_list\"\n    ADJACENCY_MATRIX = \"adjacency_matrix\"\n</code></pre> <p>Values:</p> Value Description <code>EDGE_LIST</code> Graph represented as a list of edges with source, target, and confidence <code>ADJACENCY_MATRIX</code> Graph represented as a matrix where entry (i,j) is the confidence that variable i causes variable j <p>Example:</p> <pre><code>from causaliq_knowledge.graph.prompts import OutputFormat\n\nfmt = OutputFormat.EDGE_LIST\nprint(fmt.value)  # \"edge_list\"\n</code></pre>"},{"location":"api/graph/prompts/#graphqueryprompt","title":"GraphQueryPrompt","text":"<p>Builder for graph generation query prompts. Constructs system and user prompts for querying an LLM to generate a complete causal graph from variable specifications.</p>"},{"location":"api/graph/prompts/#constructor","title":"Constructor","text":"<pre><code>GraphQueryPrompt(\n    variables: list[dict[str, Any]],\n    level: PromptDetail = PromptDetail.STANDARD,\n    domain: Optional[str] = None,\n    output_format: OutputFormat = OutputFormat.EDGE_LIST,\n    system_prompt: Optional[str] = None,\n)\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>variables</code> <code>list[dict[str, Any]]</code> Required List of filtered variable dictionaries <code>level</code> <code>PromptDetail</code> <code>STANDARD</code> The prompt detail level (minimal, standard, rich) <code>domain</code> <code>Optional[str]</code> <code>None</code> Optional domain context for the query <code>output_format</code> <code>OutputFormat</code> <code>EDGE_LIST</code> Desired output format <code>system_prompt</code> <code>Optional[str]</code> <code>None</code> Custom system prompt (uses default if None) <p>Example:</p> <pre><code>from causaliq_knowledge.graph import NetworkContext, ViewFilter, PromptDetail\nfrom causaliq_knowledge.graph.prompts import GraphQueryPrompt\n\ncontext = NetworkContext.load(\"context.json\")\nview_filter = ViewFilter(context)\nvariables = view_filter.filter_variables(PromptDetail.STANDARD)\n\nprompt = GraphQueryPrompt(\n    variables=variables,\n    level=PromptDetail.STANDARD,\n    domain=context.domain,\n)\n</code></pre>"},{"location":"api/graph/prompts/#methods","title":"Methods","text":""},{"location":"api/graph/prompts/#build","title":"build","text":"<p>Build the system and user prompts for the LLM query.</p> <pre><code>def build(self) -&gt; tuple[str, str]\n</code></pre> <p>Returns:</p> <p>A tuple of <code>(system_prompt, user_prompt)</code> strings ready for use with an LLM client.</p> <p>Example:</p> <pre><code>prompt = GraphQueryPrompt(\n    variables=variables,\n    level=PromptDetail.STANDARD,\n)\nsystem, user = prompt.build()\n\n# Use with an LLM client\nresponse = client.query(system_prompt=system, user_prompt=user)\n</code></pre>"},{"location":"api/graph/prompts/#get_variable_names","title":"get_variable_names","text":"<p>Get the list of variable names from the filtered variables.</p> <pre><code>def get_variable_names(self) -&gt; list[str]\n</code></pre> <p>Returns:</p> <p>List of variable names extracted from the variables dictionaries.</p> <p>Example:</p> <pre><code>prompt = GraphQueryPrompt(variables=variables, level=PromptDetail.MINIMAL)\nnames = prompt.get_variable_names()\n# [\"age\", \"income\", \"education\", ...]\n</code></pre>"},{"location":"api/graph/prompts/#from_context-class-method","title":"from_context (class method)","text":"<p>Create a <code>GraphQueryPrompt</code> directly from a <code>NetworkContext</code>. This is a convenience method that handles view filtering automatically.</p> <pre><code>@classmethod\ndef from_context(\n    cls,\n    context: NetworkContext,\n    level: PromptDetail = PromptDetail.STANDARD,\n    output_format: OutputFormat = OutputFormat.EDGE_LIST,\n    system_prompt: Optional[str] = None,\n) -&gt; GraphQueryPrompt\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>context</code> <code>NetworkContext</code> Required The network context <code>level</code> <code>PromptDetail</code> <code>STANDARD</code> The prompt detail level <code>output_format</code> <code>OutputFormat</code> <code>EDGE_LIST</code> Desired output format <code>system_prompt</code> <code>Optional[str]</code> <code>None</code> Custom system prompt <p>Returns:</p> <p>A <code>GraphQueryPrompt</code> instance configured from the network context.</p> <p>Example:</p> <pre><code>from causaliq_knowledge.graph import NetworkContext, PromptDetail\nfrom causaliq_knowledge.graph.prompts import GraphQueryPrompt\n\ncontext = NetworkContext.load(\"context.json\")\nprompt = GraphQueryPrompt.from_context(\n    context,\n    level=PromptDetail.RICH,\n)\nsystem, user = prompt.build()\n</code></pre>"},{"location":"api/graph/prompts/#response-schemas","title":"Response Schemas","text":"<p>The module provides JSON schemas for validating LLM responses.</p>"},{"location":"api/graph/prompts/#edge-list-response-schema","title":"Edge List Response Schema","text":"<pre><code>EDGE_LIST_RESPONSE_SCHEMA = {\n    \"type\": \"object\",\n    \"required\": [\"edges\"],\n    \"properties\": {\n        \"edges\": {\n            \"type\": \"array\",\n            \"items\": {\n                \"type\": \"object\",\n                \"required\": [\"source\", \"target\"],\n                \"properties\": {\n                    \"source\": {\"type\": \"string\"},\n                    \"target\": {\"type\": \"string\"},\n                    \"confidence\": {\n                        \"type\": \"number\",\n                        \"minimum\": 0,\n                        \"maximum\": 1,\n                    },\n                },\n            },\n        },\n        \"reasoning\": {\"type\": \"string\"},\n    },\n}\n</code></pre> <p>Example Response:</p> <pre><code>{\n  \"edges\": [\n    {\"source\": \"age\", \"target\": \"income\", \"confidence\": 0.8},\n    {\"source\": \"education\", \"target\": \"income\", \"confidence\": 0.9}\n  ],\n  \"reasoning\": \"Age and education both influence earning potential.\"\n}\n</code></pre>"},{"location":"api/graph/prompts/#adjacency-matrix-response-schema","title":"Adjacency Matrix Response Schema","text":"<pre><code>ADJACENCY_MATRIX_RESPONSE_SCHEMA = {\n    \"type\": \"object\",\n    \"required\": [\"variables\", \"adjacency_matrix\"],\n    \"properties\": {\n        \"variables\": {\n            \"type\": \"array\",\n            \"items\": {\"type\": \"string\"},\n        },\n        \"adjacency_matrix\": {\n            \"type\": \"array\",\n            \"items\": {\n                \"type\": \"array\",\n                \"items\": {\"type\": \"number\", \"minimum\": 0, \"maximum\": 1},\n            },\n        },\n        \"reasoning\": {\"type\": \"string\"},\n    },\n}\n</code></pre> <p>Example Response:</p> <pre><code>{\n  \"variables\": [\"age\", \"education\", \"income\"],\n  \"adjacency_matrix\": [\n    [0.0, 0.0, 0.8],\n    [0.0, 0.0, 0.9],\n    [0.0, 0.0, 0.0]\n  ],\n  \"reasoning\": \"Age and education both influence income directly.\"\n}\n</code></pre>"},{"location":"api/graph/prompts/#system-prompts","title":"System Prompts","text":"<p>The module provides default system prompts for different output formats:</p> <ul> <li><code>GRAPH_SYSTEM_PROMPT_EDGE_LIST</code>: Instructions for edge list format</li> <li><code>GRAPH_SYSTEM_PROMPT_ADJACENCY</code>: Instructions for adjacency matrix format</li> </ul> <p>These prompts instruct the LLM to:</p> <ul> <li>Respond with valid JSON only</li> <li>Include only direct causal relationships</li> <li>Provide confidence scores from 0.0 to 1.0</li> <li>Consider domain knowledge and temporal ordering</li> <li>Avoid self-loops</li> </ul>"},{"location":"api/graph/prompts/#user-prompt-templates","title":"User Prompt Templates","text":"<p>User prompts are selected based on the <code>PromptDetail</code>:</p> Level Without Domain With Domain <code>MINIMAL</code> Variable names only Variable names with domain context <code>STANDARD</code> Names, types, descriptions Same with domain context <code>RICH</code> Full metadata including roles, categories, hints Same with domain context"},{"location":"api/graph/prompts/#complete-example","title":"Complete Example","text":"<pre><code>from causaliq_knowledge.graph import NetworkContext, PromptDetail\nfrom causaliq_knowledge.graph.prompts import GraphQueryPrompt, OutputFormat\n\n# Load network context\ncontext = NetworkContext.load(\"research/models/cancer/cancer.json\")\n\n# Create prompt with rich context\nprompt = GraphQueryPrompt.from_context(\n    context,\n    level=PromptDetail.RICH,\n    output_format=OutputFormat.EDGE_LIST,\n)\n\n# Build prompts\nsystem_prompt, user_prompt = prompt.build()\n\n# Get variable names for result validation\nvariable_names = prompt.get_variable_names()\n\n# Use with your LLM client\n# response = llm_client.query(system=system_prompt, user=user_prompt)\n</code></pre>"},{"location":"api/graph/response/","title":"Response Models","text":"<p>The <code>response</code> module provides data models and parsing functions for LLM graph generation responses. It handles both edge list and adjacency matrix formats with robust JSON extraction.</p>"},{"location":"api/graph/response/#import-pattern","title":"Import Pattern","text":"<pre><code>from causaliq_knowledge.graph import (\n    ProposedEdge,\n    GeneratedGraph,\n    GenerationMetadata,\n    parse_graph_response,\n)\n</code></pre>"},{"location":"api/graph/response/#data-models","title":"Data Models","text":""},{"location":"api/graph/response/#proposededge","title":"ProposedEdge","text":"<p>A Pydantic model representing a single proposed causal edge from an LLM.</p> <pre><code>from causaliq_knowledge.graph import ProposedEdge\n\nedge = ProposedEdge(\n    source=\"smoking\",\n    target=\"lung_cancer\",\n    confidence=0.95,\n    reasoning=\"Well-established causal relationship from epidemiological studies\"\n)\n</code></pre> <p>Attributes:</p> <ul> <li><code>source</code> (str): Name of the source (cause) variable</li> <li><code>target</code> (str): Name of the target (effect) variable</li> <li><code>confidence</code> (float): Confidence score between 0.0 and 1.0</li> <li><code>reasoning</code> (str, optional): LLM's reasoning for proposing this edge</li> </ul>"},{"location":"api/graph/response/#generationmetadata","title":"GenerationMetadata","text":"<p>A dataclass containing metadata about the graph generation process.</p> <pre><code>from causaliq_knowledge.graph import GenerationMetadata\n\nmetadata = GenerationMetadata(\n    model_name=\"gemini-2.0-flash\",\n    prompt_tokens=450,\n    completion_tokens=320,\n    view_level=\"standard\",\n    disguised=False,\n    output_format=\"edge_list\"\n)\n</code></pre> <p>Attributes:</p> <ul> <li><code>model_name</code> (str): Name of the LLM model used</li> <li><code>prompt_tokens</code> (int, optional): Number of tokens in the prompt</li> <li><code>completion_tokens</code> (int, optional): Number of tokens in the response</li> <li><code>view_level</code> (str, optional): Context level used (minimal/standard/rich)</li> <li><code>disguised</code> (bool): Whether variable names were disguised</li> <li><code>output_format</code> (str): Response format (edge_list/adjacency_matrix)</li> </ul>"},{"location":"api/graph/response/#generatedgraph","title":"GeneratedGraph","text":"<p>A dataclass representing a complete generated causal graph.</p> <pre><code>from causaliq_knowledge.graph import GeneratedGraph, ProposedEdge\n\ngraph = GeneratedGraph(\n    edges=[\n        ProposedEdge(source=\"A\", target=\"B\", confidence=0.9),\n        ProposedEdge(source=\"B\", target=\"C\", confidence=0.85),\n    ],\n    variables=[\"A\", \"B\", \"C\"],\n    reasoning=\"Based on temporal ordering and domain knowledge...\",\n    metadata=None\n)\n</code></pre> <p>Attributes:</p> <ul> <li><code>edges</code> (list[ProposedEdge]): List of proposed causal edges</li> <li><code>variables</code> (list[str]): List of variable names in the graph</li> <li><code>reasoning</code> (str, optional): Overall reasoning for the graph structure</li> <li><code>metadata</code> (GenerationMetadata, optional): Generation metadata</li> </ul>"},{"location":"api/graph/response/#parsing-functions","title":"Parsing Functions","text":""},{"location":"api/graph/response/#parse_graph_response","title":"parse_graph_response","text":"<p>Parse an LLM response string into a <code>GeneratedGraph</code> object.</p> <pre><code>from causaliq_knowledge.graph import parse_graph_response\n\nresponse_text = '''```json\n{\n    \"edges\": [\n        {\"source\": \"A\", \"target\": \"B\", \"confidence\": 0.9},\n        {\"source\": \"B\", \"target\": \"C\", \"confidence\": 0.85}\n    ],\n    \"reasoning\": \"Based on causal principles...\"\n}\n```'''\n\ngraph = parse_graph_response(\n    response_text=response_text,\n    variables=[\"A\", \"B\", \"C\"],\n    output_format=\"edge_list\"\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>response_text</code> (str): Raw LLM response text (may include markdown)</li> <li><code>variables</code> (list[str]): Expected variable names for validation</li> <li><code>output_format</code> (str): Expected format (\"edge_list\" or \"adjacency_matrix\")</li> </ul> <p>Returns: <code>GeneratedGraph</code> object</p> <p>Raises: <code>ValueError</code> if JSON parsing fails or format is invalid</p>"},{"location":"api/graph/response/#response-formats","title":"Response Formats","text":"<p>The module supports two response formats:</p>"},{"location":"api/graph/response/#edge-list-format","title":"Edge List Format","text":"<pre><code>{\n    \"edges\": [\n        {\n            \"source\": \"variable_a\",\n            \"target\": \"variable_b\",\n            \"confidence\": 0.9,\n            \"reasoning\": \"Optional per-edge reasoning\"\n        }\n    ],\n    \"reasoning\": \"Overall graph reasoning\"\n}\n</code></pre>"},{"location":"api/graph/response/#adjacency-matrix-format","title":"Adjacency Matrix Format","text":"<pre><code>{\n    \"variables\": [\"A\", \"B\", \"C\"],\n    \"adjacency_matrix\": [\n        [0.0, 0.9, 0.0],\n        [0.0, 0.0, 0.85],\n        [0.0, 0.0, 0.0]\n    ],\n    \"reasoning\": \"Overall graph reasoning\"\n}\n</code></pre> <p>Values in the adjacency matrix represent confidence scores. A value at position <code>[i][j]</code> indicates an edge from <code>variables[i]</code> to <code>variables[j]</code>.</p>"},{"location":"api/graph/response/#api-reference","title":"API Reference","text":"<p>Response models and parsing for LLM graph generation.</p> <p>This module provides Pydantic models for representing LLM-generated causal graphs and functions for parsing LLM responses in both edge list and adjacency matrix formats.</p> <p>Classes:</p> <ul> <li> <code>ProposedEdge</code>           \u2013            <p>A proposed causal edge from LLM graph generation.</p> </li> <li> <code>GenerationMetadata</code>           \u2013            <p>Metadata about a graph generation request.</p> </li> <li> <code>GeneratedGraph</code>           \u2013            <p>A complete causal graph generated by an LLM.</p> </li> </ul> <p>Functions:</p> <ul> <li> <code>parse_graph_response</code>             \u2013              <p>Parse an LLM response into a GeneratedGraph.</p> </li> </ul>"},{"location":"api/graph/response/#causaliq_knowledge.graph.response.ProposedEdge","title":"ProposedEdge","text":"<p>A proposed causal edge from LLM graph generation.</p> <p>Represents a single directed edge in the proposed causal graph, with confidence score and optional reasoning.</p> <p>Attributes:</p> <ul> <li> <code>source</code>               (<code>str</code>)           \u2013            <p>The name of the source variable (cause).</p> </li> <li> <code>target</code>               (<code>str</code>)           \u2013            <p>The name of the target variable (effect).</p> </li> <li> <code>confidence</code>               (<code>float</code>)           \u2013            <p>Confidence score from 0.0 to 1.0.</p> </li> <li> <code>reasoning</code>               (<code>Optional[str]</code>)           \u2013            <p>Optional explanation for this specific edge.</p> </li> </ul> Example <p>edge = ProposedEdge( ...     source=\"smoking\", ...     target=\"lung_cancer\", ...     confidence=0.95, ... ) print(f\"{edge.source} -&gt; {edge.target}: {edge.confidence}\") smoking -&gt; lung_cancer: 0.95</p> <p>Methods:</p> <ul> <li> <code>clamp_confidence</code>             \u2013              <p>Clamp confidence values to [0.0, 1.0] range.</p> </li> </ul>"},{"location":"api/graph/response/#causaliq_knowledge.graph.response.ProposedEdge.clamp_confidence","title":"clamp_confidence  <code>classmethod</code>","text":"<pre><code>clamp_confidence(v: Any) -&gt; float\n</code></pre> <p>Clamp confidence values to [0.0, 1.0] range.</p>"},{"location":"api/graph/response/#causaliq_knowledge.graph.response.GenerationMetadata","title":"GenerationMetadata  <code>dataclass</code>","text":"<pre><code>GenerationMetadata(\n    model: str,\n    provider: str = \"\",\n    timestamp: datetime = (lambda: now(utc))(),\n    llm_timestamp: datetime = (lambda: now(utc))(),\n    llm_latency_ms: int = 0,\n    input_tokens: int = 0,\n    output_tokens: int = 0,\n    from_cache: bool = False,\n    messages: List[Dict[str, Any]] = list(),\n    temperature: float = 0.1,\n    max_tokens: int = 2000,\n    finish_reason: str = \"stop\",\n    llm_cost_usd: float = 0.0,\n)\n</code></pre> <p>Metadata about a graph generation request.</p> <p>Attributes:</p> <ul> <li> <code>model</code>               (<code>str</code>)           \u2013            <p>The LLM model used for generation.</p> </li> <li> <code>provider</code>               (<code>str</code>)           \u2013            <p>The LLM provider (e.g., \"groq\", \"gemini\").</p> </li> <li> <code>timestamp</code>               (<code>datetime</code>)           \u2013            <p>When this request was made (current request time).</p> </li> <li> <code>llm_timestamp</code>               (<code>datetime</code>)           \u2013            <p>When the LLM originally responded (from cache if maybe).</p> </li> <li> <code>llm_latency_ms</code>               (<code>int</code>)           \u2013            <p>Original LLM response latency in milliseconds.</p> </li> <li> <code>input_tokens</code>               (<code>int</code>)           \u2013            <p>Number of input tokens used.</p> </li> <li> <code>output_tokens</code>               (<code>int</code>)           \u2013            <p>Number of output tokens generated.</p> </li> <li> <code>from_cache</code>               (<code>bool</code>)           \u2013            <p>Whether the response was from cache.</p> </li> <li> <code>messages</code>               (<code>List[Dict[str, Any]]</code>)           \u2013            <p>The messages sent to the LLM.</p> </li> <li> <code>temperature</code>               (<code>float</code>)           \u2013            <p>Sampling temperature used.</p> </li> <li> <code>max_tokens</code>               (<code>int</code>)           \u2013            <p>Maximum tokens requested.</p> </li> <li> <code>finish_reason</code>               (<code>str</code>)           \u2013            <p>Why generation stopped (stop, length, etc.).</p> </li> <li> <code>llm_cost_usd</code>               (<code>float</code>)           \u2013            <p>Cost when the LLM request was originally made.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>to_dict</code>             \u2013              <p>Convert generation metadata to a dictionary.</p> </li> </ul>"},{"location":"api/graph/response/#causaliq_knowledge.graph.response.GenerationMetadata.initial_cost_usd","title":"initial_cost_usd  <code>property</code>","text":"<pre><code>initial_cost_usd: float\n</code></pre> <p>Alias for llm_cost_usd (backward compatibility).</p>"},{"location":"api/graph/response/#causaliq_knowledge.graph.response.GenerationMetadata.latency_ms","title":"latency_ms  <code>property</code>","text":"<pre><code>latency_ms: int\n</code></pre> <p>Alias for llm_latency_ms (backward compatibility).</p>"},{"location":"api/graph/response/#causaliq_knowledge.graph.response.GenerationMetadata.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict[str, Any]\n</code></pre> <p>Convert generation metadata to a dictionary.</p> <p>Returns a dictionary suitable for JSON serialisation, containing all generation provenance information.</p> <p>Returns:</p> <ul> <li> <code>Dict[str, Any]</code>           \u2013            <p>Dictionary with all metadata fields.</p> </li> </ul>"},{"location":"api/graph/response/#causaliq_knowledge.graph.response.GeneratedGraph","title":"GeneratedGraph  <code>dataclass</code>","text":"<pre><code>GeneratedGraph(\n    edges: List[ProposedEdge],\n    variables: List[str],\n    reasoning: str = \"\",\n    metadata: Optional[GenerationMetadata] = None,\n    raw_response: Optional[Dict[str, Any]] = None,\n)\n</code></pre> <p>A complete causal graph generated by an LLM.</p> <p>Represents the full output from an LLM graph generation query, including all proposed edges, metadata, and the LLM's reasoning.</p> <p>Attributes:</p> <ul> <li> <code>edges</code>               (<code>List[ProposedEdge]</code>)           \u2013            <p>List of proposed causal edges.</p> </li> <li> <code>variables</code>               (<code>List[str]</code>)           \u2013            <p>List of variable names in the graph.</p> </li> <li> <code>reasoning</code>               (<code>str</code>)           \u2013            <p>Overall reasoning provided by the LLM.</p> </li> <li> <code>metadata</code>               (<code>Optional[GenerationMetadata]</code>)           \u2013            <p>Generation metadata (model, timing, etc.).</p> </li> <li> <code>raw_response</code>               (<code>Optional[Dict[str, Any]]</code>)           \u2013            <p>The original LLM response for debugging.</p> </li> </ul> Example <p>edge1 = ProposedEdge( ...     source=\"age\", target=\"income\", confidence=0.7 ... ) edge2 = ProposedEdge( ...     source=\"education\", target=\"income\", confidence=0.9 ... ) graph = GeneratedGraph( ...     edges=[edge1, edge2], ...     variables=[\"age\", \"education\", \"income\"], ...     reasoning=\"Age and education both influence income.\", ...     metadata=GenerationMetadata(model=\"llama-3.1-8b-instant\"), ... ) print(f\"Generated {len(graph.edges)} edges\") Generated 2 edges</p> <p>Methods:</p> <ul> <li> <code>filter_by_confidence</code>             \u2013              <p>Return a new graph with only edges above the threshold.</p> </li> <li> <code>get_adjacency_matrix</code>             \u2013              <p>Convert edges to an adjacency matrix.</p> </li> <li> <code>get_edge_list</code>             \u2013              <p>Get edges as a list of tuples.</p> </li> </ul>"},{"location":"api/graph/response/#causaliq_knowledge.graph.response.GeneratedGraph.filter_by_confidence","title":"filter_by_confidence","text":"<pre><code>filter_by_confidence(threshold: float = 0.5) -&gt; 'GeneratedGraph'\n</code></pre> <p>Return a new graph with only edges above the threshold.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>'GeneratedGraph'</code>           \u2013            <p>New GeneratedGraph with filtered edges.</p> </li> </ul>"},{"location":"api/graph/response/#causaliq_knowledge.graph.response.GeneratedGraph.filter_by_confidence(threshold)","title":"<code>threshold</code>","text":"(<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Minimum confidence score to include.</p>"},{"location":"api/graph/response/#causaliq_knowledge.graph.response.GeneratedGraph.get_adjacency_matrix","title":"get_adjacency_matrix","text":"<pre><code>get_adjacency_matrix() -&gt; List[List[float]]\n</code></pre> <p>Convert edges to an adjacency matrix.</p> <p>Creates a square matrix where entry (i,j) represents the confidence that variable i causes variable j.</p> <p>Returns:</p> <ul> <li> <code>List[List[float]]</code>           \u2013            <p>Square matrix of confidence scores.</p> </li> </ul>"},{"location":"api/graph/response/#causaliq_knowledge.graph.response.GeneratedGraph.get_edge_list","title":"get_edge_list","text":"<pre><code>get_edge_list() -&gt; List[tuple[str, str, float]]\n</code></pre> <p>Get edges as a list of tuples.</p> <p>Returns:</p> <ul> <li> <code>List[tuple[str, str, float]]</code>           \u2013            <p>List of (source, target, confidence) tuples.</p> </li> </ul>"},{"location":"api/graph/response/#causaliq_knowledge.graph.response.parse_graph_response","title":"parse_graph_response","text":"<pre><code>parse_graph_response(\n    response_text: str, variables: List[str], output_format: str = \"edge_list\"\n) -&gt; GeneratedGraph\n</code></pre> <p>Parse an LLM response into a GeneratedGraph.</p> <p>Handles JSON extraction from markdown code blocks and parses according to the specified output format.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>GeneratedGraph</code>           \u2013            <p>GeneratedGraph with parsed edges and metadata.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If JSON parsing fails or format is invalid.</p> </li> </ul>"},{"location":"api/graph/response/#causaliq_knowledge.graph.response.parse_graph_response(response_text)","title":"<code>response_text</code>","text":"(<code>str</code>)           \u2013            <p>Raw text response from the LLM.</p>"},{"location":"api/graph/response/#causaliq_knowledge.graph.response.parse_graph_response(variables)","title":"<code>variables</code>","text":"(<code>List[str]</code>)           \u2013            <p>List of valid variable names.</p>"},{"location":"api/graph/response/#causaliq_knowledge.graph.response.parse_graph_response(output_format)","title":"<code>output_format</code>","text":"(<code>str</code>, default:                   <code>'edge_list'</code> )           \u2013            <p>Expected format (\"edge_list\" or \"adjacency_matrix\").</p>"},{"location":"api/graph/view_filter/","title":"View Filter API Reference","text":"<p>Filtering network context to extract specific context levels for LLM queries.</p>"},{"location":"api/graph/view_filter/#overview","title":"Overview","text":"<p>The <code>ViewFilter</code> class extracts variable information according to view definitions (minimal, standard, rich) from a network context. This allows controlling how much context is provided to LLMs.</p> <pre><code>from causaliq_knowledge.graph import ViewFilter, PromptDetail\n</code></pre>"},{"location":"api/graph/view_filter/#promptdetail","title":"PromptDetail","text":"<p>Enumeration of context levels for filtering.</p> <pre><code>class PromptDetail(str, Enum):\n    MINIMAL = \"minimal\"   # Variable names only\n    STANDARD = \"standard\" # Names with basic descriptions\n    RICH = \"rich\"         # Full metadata and context\n</code></pre> <p>Example:</p> <pre><code>from causaliq_knowledge.graph import PromptDetail\n\nlevel = PromptDetail.STANDARD\nprint(level.value)  # \"standard\"\n</code></pre>"},{"location":"api/graph/view_filter/#viewfilter","title":"ViewFilter","text":"<p>Filter network context to extract specific view levels.</p>"},{"location":"api/graph/view_filter/#constructor","title":"Constructor","text":"<pre><code>ViewFilter(context: NetworkContext, *, use_llm_names: bool = True)\n</code></pre> <p>Parameters:</p> Parameter Type Default Description <code>context</code> <code>NetworkContext</code> - The network context to filter <code>use_llm_names</code> <code>bool</code> <code>True</code> Output llm_name as 'name' field <p>Example:</p> <pre><code>from causaliq_knowledge.graph import NetworkContext, ViewFilter\n\ncontext = NetworkContext.load(\"model.json\")\nview_filter = ViewFilter(context)\n\n# Use benchmark names instead of LLM names\nview_filter = ViewFilter(context, use_llm_names=False)\n</code></pre>"},{"location":"api/graph/view_filter/#properties","title":"Properties","text":""},{"location":"api/graph/view_filter/#context-networkcontext","title":"<code>context -&gt; NetworkContext</code>","text":"<p>Return the network context.</p> <pre><code>filter = ViewFilter(context)\nprint(filter.context.domain)\n</code></pre>"},{"location":"api/graph/view_filter/#methods","title":"Methods","text":""},{"location":"api/graph/view_filter/#get_include_fieldslevel-promptdetail-liststr","title":"<code>get_include_fields(level: PromptDetail) -&gt; list[str]</code>","text":"<p>Get the fields to include for a given view level.</p> <p>Parameters:</p> Parameter Type Description <code>level</code> <code>PromptDetail</code> The prompt detail level <p>Returns: <code>list[str]</code> - Field names to include.</p> <p>Example:</p> <pre><code>from causaliq_knowledge.graph import ViewFilter, PromptDetail\n\nfilter = ViewFilter(context)\n\n# Get fields for minimal view\nfields = filter.get_include_fields(PromptDetail.MINIMAL)\n# [\"name\"]\n\n# Get fields for standard view\nfields = filter.get_include_fields(PromptDetail.STANDARD)\n# [\"name\", \"type\", \"short_description\", \"states\"]\n</code></pre>"},{"location":"api/graph/view_filter/#filter_variablevariable-variablespec-level-promptdetail-dict","title":"<code>filter_variable(variable: VariableSpec, level: PromptDetail) -&gt; dict</code>","text":"<p>Filter a single variable to include only specified fields.</p> <p>Parameters:</p> Parameter Type Description <code>variable</code> <code>VariableSpec</code> The variable to filter <code>level</code> <code>PromptDetail</code> The prompt detail level <p>Returns: <code>dict</code> - Dictionary with only the included fields.</p> <p>Example:</p> <pre><code>filter = ViewFilter(context)\nvar = context.variables[0]\n\nminimal = filter.filter_variable(var, PromptDetail.MINIMAL)\n# {\"name\": \"smoking\"}\n\nstandard = filter.filter_variable(var, PromptDetail.STANDARD)\n# {\"name\": \"smoking\", \"type\": \"binary\", \"short_description\": \"...\"}\n</code></pre>"},{"location":"api/graph/view_filter/#filter_variableslevel-promptdetail-listdict","title":"<code>filter_variables(level: PromptDetail) -&gt; list[dict]</code>","text":"<p>Filter all variables to the specified view level.</p> <p>Parameters:</p> Parameter Type Description <code>level</code> <code>PromptDetail</code> The prompt detail level <p>Returns: <code>list[dict]</code> - List of filtered variable dictionaries.</p> <p>Example:</p> <pre><code>filter = ViewFilter(context)\n\n# Get minimal view of all variables\nminimal_vars = filter.filter_variables(PromptDetail.MINIMAL)\n# [{\"name\": \"smoking\"}, {\"name\": \"cancer\"}, ...]\n\n# Get rich view with full context\nrich_vars = filter.filter_variables(PromptDetail.RICH)\n</code></pre>"},{"location":"api/graph/view_filter/#get_variable_names-liststr","title":"<code>get_variable_names() -&gt; list[str]</code>","text":"<p>Get all variable names from the context.</p> <p>Returns: <code>list[str]</code> - List of variable names.</p> <pre><code>filter = ViewFilter(context)\nnames = filter.get_variable_names()\n# [\"smoking\", \"cancer\", \"age\"]\n</code></pre>"},{"location":"api/graph/view_filter/#get_domain-str","title":"<code>get_domain() -&gt; str</code>","text":"<p>Get the domain from the context.</p> <p>Returns: <code>str</code> - The domain string.</p> <pre><code>filter = ViewFilter(context)\ndomain = filter.get_domain()\n# \"epidemiology\"\n</code></pre>"},{"location":"api/graph/view_filter/#get_context_summarylevel-promptdetail-dict","title":"<code>get_context_summary(level: PromptDetail) -&gt; dict</code>","text":"<p>Get a complete context summary for LLM prompts.</p> <p>Parameters:</p> Parameter Type Description <code>level</code> <code>PromptDetail</code> The prompt detail level for variable filtering <p>Returns: <code>dict</code> - Dictionary with domain, network, and filtered variables.</p> <p>Example:</p> <pre><code>filter = ViewFilter(context)\nsummary = filter.get_context_summary(PromptDetail.STANDARD)\n\n# {\n#     \"domain\": \"epidemiology\",\n#     \"network\": \"cancer\",\n#     \"variables\": [\n#         {\"name\": \"smoking\", \"type\": \"binary\", ...},\n#         {\"name\": \"cancer\", \"type\": \"binary\", ...},\n#     ]\n# }\n</code></pre>"},{"location":"api/graph/view_filter/#usage-patterns","title":"Usage Patterns","text":""},{"location":"api/graph/view_filter/#generating-llm-context","title":"Generating LLM Context","text":"<pre><code>from causaliq_knowledge.graph import NetworkContext, ViewFilter, PromptDetail\nimport json\n\n# Load network context and create filter\ncontext = NetworkContext.load(\"model.json\")\nfilter = ViewFilter(context)\n\n# Get context for LLM prompt\nsummary = filter.get_context_summary(PromptDetail.STANDARD)\n\n# Format for prompt\nprompt = f\"\"\"\nDomain: {summary['domain']}\n\nVariables:\n{json.dumps(summary['variables'], indent=2)}\n\nPlease generate a causal graph for these variables.\n\"\"\"\n</code></pre>"},{"location":"api/graph/view_filter/#comparing-prompt-detail-levels","title":"Comparing Prompt Detail Levels","text":"<pre><code>from causaliq_knowledge.graph import ViewFilter, PromptDetail\n\nfilter = ViewFilter(context)\n\n# Compare information at different levels\nfor level in PromptDetail:\n    vars = filter.filter_variables(level)\n    fields = set()\n    for v in vars:\n        fields.update(v.keys())\n    print(f\"{level.value}: {sorted(fields)}\")\n\n# minimal: ['name']\n# standard: ['name', 'short_description', 'states', 'type']\n# rich: ['extended_description', 'name', 'role', 'short_description', ...]\n</code></pre>"},{"location":"api/graph/view_filter/#custom-prompt-details","title":"Custom Prompt Details","text":"<p>Network context can define custom prompt detail configurations:</p> <pre><code>{\n    \"prompt_details\": {\n        \"minimal\": {\n            \"include_fields\": [\"name\"]\n        },\n        \"standard\": {\n            \"include_fields\": [\"name\", \"type\", \"short_description\"]\n        },\n        \"rich\": {\n            \"include_fields\": [\n                \"name\", \"type\", \"role\", \"short_description\",\n                \"extended_description\", \"sensitivity_hints\"\n            ]\n        }\n    }\n}\n</code></pre>"},{"location":"api/llm/cache/","title":"LLM Cache","text":"<p>LLM-specific cache compressor and data structures for storing and retrieving LLM requests and responses with rich metadata.</p> <p>Package Separation</p> <p>This module stays in <code>causaliq-knowledge</code> as it contains LLM-specific logic. The core cache infrastructure (<code>TokenCache</code>, <code>Compressor</code>, <code>JsonCompressor</code>) is in <code>causaliq-core</code>. Import from <code>causaliq_core.cache</code>.</p>"},{"location":"api/llm/cache/#overview","title":"Overview","text":"<p>The LLM cache module provides:</p> <ul> <li>LLMCompressor - Extends JsonCompressor with LLM-specific convenience methods</li> <li>LLMCacheEntry - Complete cache entry with request, response, and metadata</li> <li>LLMResponse - Response data (content, finish reason, model version)</li> <li>LLMMetadata - Rich metadata (provider, tokens, latency, cost)</li> <li>LLMTokenUsage - Token usage statistics</li> </ul>"},{"location":"api/llm/cache/#design-philosophy","title":"Design Philosophy","text":"<p>The LLM cache separates concerns:</p> Component Package <code>TokenCache</code> <code>causaliq_core.cache</code> <code>Compressor</code> <code>causaliq_core.cache.compressors</code> <code>JsonCompressor</code> <code>causaliq_core.cache.compressors</code> <code>LLMCompressor</code> <code>causaliq_knowledge.llm.cache</code> <code>LLMCacheEntry</code> <code>causaliq_knowledge.llm.cache</code> <p>This allows the base cache to be reused across projects while keeping LLM-specific logic in the appropriate package.</p>"},{"location":"api/llm/cache/#usage","title":"Usage","text":""},{"location":"api/llm/cache/#creating-cache-entries","title":"Creating Cache Entries","text":"<p>Use the <code>LLMCacheEntry.create()</code> factory method for convenient entry creation:</p> <pre><code>from causaliq_knowledge.llm.cache import LLMCacheEntry\n\nentry = LLMCacheEntry.create(\n    model=\"gpt-4\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are helpful.\"},\n        {\"role\": \"user\", \"content\": \"Hello!\"},\n    ],\n    content=\"Hi there! How can I help you today?\",\n    temperature=0.7,\n    max_tokens=1000,\n    provider=\"openai\",\n    latency_ms=850,\n    input_tokens=25,\n    output_tokens=15,\n    cost_usd=0.002,\n)\n</code></pre>"},{"location":"api/llm/cache/#compressing-and-storing-entries","title":"Compressing and Storing Entries","text":"<pre><code>from causaliq_core.cache import TokenCache\nfrom causaliq_knowledge.llm.cache import LLMCacheEntry, LLMCompressor\n\nwith TokenCache(\":memory:\") as cache:\n    compressor = LLMCompressor()\n\n    # Create an entry\n    entry = LLMCacheEntry.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": \"What is Python?\"}],\n        content=\"Python is a programming language.\",\n        provider=\"openai\",\n    )\n\n    # Compress to bytes\n    blob = compressor.compress_entry(entry, cache)\n\n    # Store in cache\n    cache.put(\"request-hash\", \"llm\", blob)\n</code></pre>"},{"location":"api/llm/cache/#retrieving-and-decompressing-entries","title":"Retrieving and Decompressing Entries","text":"<pre><code>from causaliq_core.cache import TokenCache\nfrom causaliq_knowledge.llm.cache import LLMCompressor\n\nwith TokenCache(\"cache.db\") as cache:\n    compressor = LLMCompressor()\n\n    # Retrieve from cache\n    blob = cache.get(\"request-hash\", \"llm\")\n    if blob:\n        # Decompress to LLMCacheEntry\n        entry = compressor.decompress_entry(blob, cache)\n        print(f\"Response: {entry.response.content}\")\n        print(f\"Latency: {entry.metadata.latency_ms}ms\")\n</code></pre>"},{"location":"api/llm/cache/#exporting-and-importing-entries","title":"Exporting and Importing Entries","text":"<p>Export entries to JSON for inspection or migration:</p> <pre><code>from pathlib import Path\nfrom causaliq_knowledge.llm.cache import LLMCacheEntry, LLMCompressor\n\ncompressor = LLMCompressor()\n\n# Create entry\nentry = LLMCacheEntry.create(\n    model=\"claude-3\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n    content=\"Hi!\",\n    provider=\"anthropic\",\n)\n\n# Export to JSON file\ncompressor.export_entry(entry, Path(\"entry.json\"))\n\n# Import from JSON file\nrestored = compressor.import_entry(Path(\"entry.json\"))\n</code></pre>"},{"location":"api/llm/cache/#using-with-tokencache-auto-compression","title":"Using with TokenCache Auto-Compression","text":"<p>Register the compressor for automatic compression/decompression:</p> <pre><code>from causaliq_core.cache import TokenCache\nfrom causaliq_knowledge.llm.cache import LLMCacheEntry, LLMCompressor\n\nwith TokenCache(\":memory:\") as cache:\n    # Register compressor for \"llm\" entry type\n    cache.register_compressor(\"llm\", LLMCompressor())\n\n    entry = LLMCacheEntry.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n        content=\"Hi!\",\n    )\n\n    # Store with auto-compression\n    cache.put_data(\"hash123\", \"llm\", entry.to_dict())\n\n    # Retrieve with auto-decompression\n    data = cache.get_data(\"hash123\", \"llm\")\n    restored = LLMCacheEntry.from_dict(data)\n</code></pre>"},{"location":"api/llm/cache/#using-cached_completion-with-basellmclient","title":"Using cached_completion with BaseLLMClient","text":"<p>The recommended way to use caching is via <code>BaseLLMClient.cached_completion()</code>:</p> <pre><code>from causaliq_core.cache import TokenCache\nfrom causaliq_knowledge.llm import GroqClient, LLMConfig\n\nwith TokenCache(\"llm_cache.db\") as cache:\n    client = GroqClient(LLMConfig(model=\"llama-3.1-8b-instant\"))\n    client.set_cache(cache)\n\n    # First call - makes API request, caches response with latency\n    response = client.cached_completion(\n        [{\"role\": \"user\", \"content\": \"What is Python?\"}]\n    )\n\n    # Second call - returns from cache, no API call\n    response = client.cached_completion(\n        [{\"role\": \"user\", \"content\": \"What is Python?\"}]\n    )\n</code></pre> <p>This automatically:</p> <ul> <li>Generates a deterministic cache key (SHA-256 of model + messages + params)</li> <li>Checks cache before making API calls</li> <li>Captures latency with <code>time.perf_counter()</code></li> <li>Stores response with full metadata</li> </ul>"},{"location":"api/llm/cache/#importing-pre-cached-responses","title":"Importing Pre-Cached Responses","text":"<p>Load cached responses from JSON files for testing or migration:</p> <pre><code>from pathlib import Path\nfrom causaliq_core.cache import TokenCache\nfrom causaliq_knowledge.llm.cache import LLMCompressor\n\nwith TokenCache(\"llm_cache.db\") as cache:\n    cache.register_compressor(\"llm\", LLMCompressor())\n\n    # Import all LLM entries from directory\n    count = cache.import_entries(Path(\"./cached_responses\"), \"llm\")\n    print(f\"Imported {count} cached LLM responses\")\n</code></pre>"},{"location":"api/llm/cache/#data-structures","title":"Data Structures","text":""},{"location":"api/llm/cache/#llmtokenusage","title":"LLMTokenUsage","text":"<p>Token usage statistics for billing and analysis:</p> <pre><code>from causaliq_knowledge.llm.cache import LLMTokenUsage\n\nusage = LLMTokenUsage(\n    input=100,   # Prompt tokens\n    output=50,   # Completion tokens\n    total=150,   # Total tokens\n)\n</code></pre>"},{"location":"api/llm/cache/#llmmetadata","title":"LLMMetadata","text":"<p>Rich metadata for debugging and analytics:</p> <pre><code>from causaliq_knowledge.llm.cache import LLMMetadata, LLMTokenUsage\n\nmetadata = LLMMetadata(\n    provider=\"openai\",\n    timestamp=\"2024-01-15T10:30:00+00:00\",\n    latency_ms=850,\n    tokens=LLMTokenUsage(input=100, output=50, total=150),\n    cost_usd=0.005,\n    cache_hit=False,\n)\n\n# Convert to/from dict\ndata = metadata.to_dict()\nrestored = LLMMetadata.from_dict(data)\n</code></pre>"},{"location":"api/llm/cache/#llmresponse","title":"LLMResponse","text":"<p>Response content and generation info:</p> <pre><code>from causaliq_knowledge.llm.cache import LLMResponse\n\nresponse = LLMResponse(\n    content=\"The answer is 42.\",\n    finish_reason=\"stop\",\n    model_version=\"gpt-4-0125-preview\",\n)\n\n# Convert to/from dict\ndata = response.to_dict()\nrestored = LLMResponse.from_dict(data)\n</code></pre>"},{"location":"api/llm/cache/#llmcacheentry","title":"LLMCacheEntry","text":"<p>Complete cache entry combining request and response:</p> <pre><code>from causaliq_knowledge.llm.cache import (\n    LLMCacheEntry, LLMResponse, LLMMetadata\n)\n\nentry = LLMCacheEntry(\n    model=\"gpt-4\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n    temperature=0.7,\n    max_tokens=1000,\n    response=LLMResponse(content=\"Hi!\"),\n    metadata=LLMMetadata(provider=\"openai\"),\n)\n\n# Preferred: use factory method\nentry = LLMCacheEntry.create(\n    model=\"gpt-4\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n    content=\"Hi!\",\n    provider=\"openai\",\n)\n\n# Convert to/from dict\ndata = entry.to_dict()\nrestored = LLMCacheEntry.from_dict(data)\n</code></pre>"},{"location":"api/llm/cache/#api-reference","title":"API Reference","text":""},{"location":"api/llm/cache/#llmcompressor","title":"LLMCompressor","text":""},{"location":"api/llm/cache/#causaliq_knowledge.llm.cache.LLMCompressor","title":"LLMCompressor","text":"<p>Compressor for LLM cache entries.</p> <p>Extends JsonCompressor with LLM-specific convenience methods for compressing/decompressing LLMCacheEntry objects.</p> <p>The compressor stores data in the standard JSON tokenised format, achieving 50-70% compression through the shared token dictionary.</p> Example <p>from causaliq_core.cache import TokenCache from causaliq_knowledge.llm.cache import ( ...     LLMCompressor, LLMCacheEntry, ... ) with TokenCache(\":memory:\") as cache: ...     compressor = LLMCompressor() ...     entry = LLMCacheEntry.create( ...         model=\"gpt-4\", ...         messages=[{\"role\": \"user\", \"content\": \"Hello\"}], ...         content=\"Hi there!\", ...         provider=\"openai\", ...     ) ...     blob = compressor.compress(entry.to_dict(), cache) ...     data = compressor.decompress(blob, cache) ...     restored = LLMCacheEntry.from_dict(data)</p> <p>Methods:</p> <ul> <li> <code>compress_entry</code>             \u2013              <p>Compress an LLMCacheEntry to bytes.</p> </li> <li> <code>decompress_entry</code>             \u2013              <p>Decompress bytes to an LLMCacheEntry.</p> </li> <li> <code>export_entry</code>             \u2013              <p>Export an LLMCacheEntry to a JSON file.</p> </li> <li> <code>import_entry</code>             \u2013              <p>Import an LLMCacheEntry from a JSON file.</p> </li> <li> <code>generate_export_filename</code>             \u2013              <p>Generate a human-readable filename for export.</p> </li> </ul>"},{"location":"api/llm/cache/#causaliq_knowledge.llm.cache.LLMCompressor.compress_entry","title":"compress_entry","text":"<pre><code>compress_entry(entry: LLMCacheEntry, cache: TokenCache) -&gt; bytes\n</code></pre> <p>Compress an LLMCacheEntry to bytes.</p> <p>Convenience method that handles to_dict conversion.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>bytes</code>           \u2013            <p>Compressed bytes.</p> </li> </ul>"},{"location":"api/llm/cache/#causaliq_knowledge.llm.cache.LLMCompressor.compress_entry(entry)","title":"<code>entry</code>","text":"(<code>LLMCacheEntry</code>)           \u2013            <p>The cache entry to compress.</p>"},{"location":"api/llm/cache/#causaliq_knowledge.llm.cache.LLMCompressor.compress_entry(cache)","title":"<code>cache</code>","text":"(<code>TokenCache</code>)           \u2013            <p>TokenCache for token dictionary.</p>"},{"location":"api/llm/cache/#causaliq_knowledge.llm.cache.LLMCompressor.decompress_entry","title":"decompress_entry","text":"<pre><code>decompress_entry(blob: bytes, cache: TokenCache) -&gt; LLMCacheEntry\n</code></pre> <p>Decompress bytes to an LLMCacheEntry.</p> <p>Convenience method that handles from_dict conversion.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>LLMCacheEntry</code>           \u2013            <p>Decompressed LLMCacheEntry.</p> </li> </ul>"},{"location":"api/llm/cache/#causaliq_knowledge.llm.cache.LLMCompressor.decompress_entry(blob)","title":"<code>blob</code>","text":"(<code>bytes</code>)           \u2013            <p>Compressed bytes.</p>"},{"location":"api/llm/cache/#causaliq_knowledge.llm.cache.LLMCompressor.decompress_entry(cache)","title":"<code>cache</code>","text":"(<code>TokenCache</code>)           \u2013            <p>TokenCache for token dictionary.</p>"},{"location":"api/llm/cache/#causaliq_knowledge.llm.cache.LLMCompressor.export_entry","title":"export_entry","text":"<pre><code>export_entry(entry: LLMCacheEntry, path: Path) -&gt; None\n</code></pre> <p>Export an LLMCacheEntry to a JSON file.</p> <p>Uses to_export_dict() to parse JSON content for readability.</p> <p>Parameters:</p>"},{"location":"api/llm/cache/#causaliq_knowledge.llm.cache.LLMCompressor.export_entry(entry)","title":"<code>entry</code>","text":"(<code>LLMCacheEntry</code>)           \u2013            <p>The cache entry to export.</p>"},{"location":"api/llm/cache/#causaliq_knowledge.llm.cache.LLMCompressor.export_entry(path)","title":"<code>path</code>","text":"(<code>Path</code>)           \u2013            <p>Destination file path.</p>"},{"location":"api/llm/cache/#causaliq_knowledge.llm.cache.LLMCompressor.import_entry","title":"import_entry","text":"<pre><code>import_entry(path: Path) -&gt; LLMCacheEntry\n</code></pre> <p>Import an LLMCacheEntry from a JSON file.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>LLMCacheEntry</code>           \u2013            <p>Imported LLMCacheEntry.</p> </li> </ul>"},{"location":"api/llm/cache/#causaliq_knowledge.llm.cache.LLMCompressor.import_entry(path)","title":"<code>path</code>","text":"(<code>Path</code>)           \u2013            <p>Source file path.</p>"},{"location":"api/llm/cache/#causaliq_knowledge.llm.cache.LLMCompressor.generate_export_filename","title":"generate_export_filename","text":"<pre><code>generate_export_filename(entry: LLMCacheEntry, cache_key: str) -&gt; str\n</code></pre> <p>Generate a human-readable filename for export.</p> <p>Creates a filename using request_id, timestamp, and provider:     {request_id}{yyyy-mm-dd-hhmmss}.json</p> <p>If request_id is not set, falls back to a short hash prefix.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>Human-readable filename with .json extension.</p> </li> </ul> Example <p>compressor = LLMCompressor() entry = LLMCacheEntry.create( ...     model=\"gpt-4\", ...     messages=[{\"role\": \"user\", \"content\": \"test\"}], ...     content=\"Response\", ...     provider=\"openai\", ...     request_id=\"expt23\", ... )</p>"},{"location":"api/llm/cache/#causaliq_knowledge.llm.cache.LLMCompressor.generate_export_filename(entry)","title":"<code>entry</code>","text":"(<code>LLMCacheEntry</code>)           \u2013            <p>The cache entry to generate filename for.</p>"},{"location":"api/llm/cache/#causaliq_knowledge.llm.cache.LLMCompressor.generate_export_filename(cache_key)","title":"<code>cache_key</code>","text":"(<code>str</code>)           \u2013            <p>The cache key (hash) for fallback uniqueness.</p>"},{"location":"api/llm/cache/#causaliq_knowledge.llm.cache.LLMCompressor.generate_export_filename--returns-something-like-expt23_2026-01-29-143052_openaijson","title":"Returns something like: expt23_2026-01-29-143052_openai.json","text":""},{"location":"api/llm/cache/#llmcacheentry_1","title":"LLMCacheEntry","text":""},{"location":"api/llm/cache/#causaliq_knowledge.llm.cache.LLMCacheEntry","title":"LLMCacheEntry  <code>dataclass</code>","text":"<pre><code>LLMCacheEntry(\n    model: str = \"\",\n    messages: list[dict[str, Any]] = list(),\n    temperature: float = 0.0,\n    max_tokens: int | None = None,\n    response: LLMResponse = LLMResponse(),\n    metadata: LLMMetadata = LLMMetadata(),\n)\n</code></pre> <p>Complete LLM cache entry with request, response, and metadata.</p> <p>Attributes:</p> <ul> <li> <code>model</code>               (<code>str</code>)           \u2013            <p>The model name requested.</p> </li> <li> <code>messages</code>               (<code>list[dict[str, Any]]</code>)           \u2013            <p>The conversation messages.</p> </li> <li> <code>temperature</code>               (<code>float</code>)           \u2013            <p>Sampling temperature.</p> </li> <li> <code>max_tokens</code>               (<code>int | None</code>)           \u2013            <p>Maximum tokens in response.</p> </li> <li> <code>response</code>               (<code>LLMResponse</code>)           \u2013            <p>The LLM response data.</p> </li> <li> <code>metadata</code>               (<code>LLMMetadata</code>)           \u2013            <p>Rich metadata for analysis.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>create</code>             \u2013              <p>Create a cache entry with common parameters.</p> </li> <li> <code>to_dict</code>             \u2013              <p>Convert to dictionary for JSON serialisation.</p> </li> <li> <code>to_export_dict</code>             \u2013              <p>Convert to dictionary for export with readable formatting.</p> </li> <li> <code>from_dict</code>             \u2013              <p>Create from dictionary.</p> </li> </ul>"},{"location":"api/llm/cache/#causaliq_knowledge.llm.cache.LLMCacheEntry.create","title":"create  <code>classmethod</code>","text":"<pre><code>create(\n    model: str,\n    messages: list[dict[str, Any]],\n    content: str,\n    *,\n    temperature: float = 0.0,\n    max_tokens: int | None = None,\n    finish_reason: str = \"stop\",\n    model_version: str = \"\",\n    provider: str = \"\",\n    latency_ms: int = 0,\n    input_tokens: int = 0,\n    output_tokens: int = 0,\n    cost_usd: float = 0.0,\n    request_id: str = \"\"\n) -&gt; LLMCacheEntry\n</code></pre> <p>Create a cache entry with common parameters.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>LLMCacheEntry</code>           \u2013            <p>Configured LLMCacheEntry.</p> </li> </ul>"},{"location":"api/llm/cache/#causaliq_knowledge.llm.cache.LLMCacheEntry.create(model)","title":"<code>model</code>","text":"(<code>str</code>)           \u2013            <p>The model name requested.</p>"},{"location":"api/llm/cache/#causaliq_knowledge.llm.cache.LLMCacheEntry.create(messages)","title":"<code>messages</code>","text":"(<code>list[dict[str, Any]]</code>)           \u2013            <p>The conversation messages.</p>"},{"location":"api/llm/cache/#causaliq_knowledge.llm.cache.LLMCacheEntry.create(content)","title":"<code>content</code>","text":"(<code>str</code>)           \u2013            <p>The response content.</p>"},{"location":"api/llm/cache/#causaliq_knowledge.llm.cache.LLMCacheEntry.create(temperature)","title":"<code>temperature</code>","text":"(<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>Sampling temperature.</p>"},{"location":"api/llm/cache/#causaliq_knowledge.llm.cache.LLMCacheEntry.create(max_tokens)","title":"<code>max_tokens</code>","text":"(<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Maximum tokens in response.</p>"},{"location":"api/llm/cache/#causaliq_knowledge.llm.cache.LLMCacheEntry.create(finish_reason)","title":"<code>finish_reason</code>","text":"(<code>str</code>, default:                   <code>'stop'</code> )           \u2013            <p>Why generation stopped.</p>"},{"location":"api/llm/cache/#causaliq_knowledge.llm.cache.LLMCacheEntry.create(model_version)","title":"<code>model_version</code>","text":"(<code>str</code>, default:                   <code>''</code> )           \u2013            <p>Actual model version.</p>"},{"location":"api/llm/cache/#causaliq_knowledge.llm.cache.LLMCacheEntry.create(provider)","title":"<code>provider</code>","text":"(<code>str</code>, default:                   <code>''</code> )           \u2013            <p>LLM provider name.</p>"},{"location":"api/llm/cache/#causaliq_knowledge.llm.cache.LLMCacheEntry.create(latency_ms)","title":"<code>latency_ms</code>","text":"(<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Response time in milliseconds.</p>"},{"location":"api/llm/cache/#causaliq_knowledge.llm.cache.LLMCacheEntry.create(input_tokens)","title":"<code>input_tokens</code>","text":"(<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Number of input tokens.</p>"},{"location":"api/llm/cache/#causaliq_knowledge.llm.cache.LLMCacheEntry.create(output_tokens)","title":"<code>output_tokens</code>","text":"(<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Number of output tokens.</p>"},{"location":"api/llm/cache/#causaliq_knowledge.llm.cache.LLMCacheEntry.create(cost_usd)","title":"<code>cost_usd</code>","text":"(<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>Estimated cost in USD.</p>"},{"location":"api/llm/cache/#causaliq_knowledge.llm.cache.LLMCacheEntry.create(request_id)","title":"<code>request_id</code>","text":"(<code>str</code>, default:                   <code>''</code> )           \u2013            <p>Optional identifier for the request (not part of hash).</p>"},{"location":"api/llm/cache/#causaliq_knowledge.llm.cache.LLMCacheEntry.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; dict[str, Any]\n</code></pre> <p>Convert to dictionary for JSON serialisation.</p>"},{"location":"api/llm/cache/#causaliq_knowledge.llm.cache.LLMCacheEntry.to_export_dict","title":"to_export_dict","text":"<pre><code>to_export_dict() -&gt; dict[str, Any]\n</code></pre> <p>Convert to dictionary for export with readable formatting.</p> <ul> <li>Message content with newlines is split into arrays of lines</li> <li>Response JSON content is parsed into a proper JSON structure</li> </ul>"},{"location":"api/llm/cache/#causaliq_knowledge.llm.cache.LLMCacheEntry.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(data: dict[str, Any]) -&gt; LLMCacheEntry\n</code></pre> <p>Create from dictionary.</p> <p>Handles both internal format (string content) and export format (array of lines for content).</p>"},{"location":"api/llm/cache/#llmresponse_1","title":"LLMResponse","text":""},{"location":"api/llm/cache/#causaliq_knowledge.llm.cache.LLMResponse","title":"LLMResponse  <code>dataclass</code>","text":"<pre><code>LLMResponse(content: str = '', finish_reason: str = 'stop', model_version: str = '')\n</code></pre> <p>LLM response data for caching.</p> <p>Attributes:</p> <ul> <li> <code>content</code>               (<code>str</code>)           \u2013            <p>The full text response from the LLM.</p> </li> <li> <code>finish_reason</code>               (<code>str</code>)           \u2013            <p>Why generation stopped (stop, length, etc.).</p> </li> <li> <code>model_version</code>               (<code>str</code>)           \u2013            <p>Actual model version used.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>to_dict</code>             \u2013              <p>Convert to dictionary for JSON serialisation.</p> </li> <li> <code>to_export_dict</code>             \u2013              <p>Convert to dictionary for export, parsing JSON content if valid.</p> </li> <li> <code>from_dict</code>             \u2013              <p>Create from dictionary.</p> </li> </ul>"},{"location":"api/llm/cache/#causaliq_knowledge.llm.cache.LLMResponse.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; dict[str, Any]\n</code></pre> <p>Convert to dictionary for JSON serialisation.</p>"},{"location":"api/llm/cache/#causaliq_knowledge.llm.cache.LLMResponse.to_export_dict","title":"to_export_dict","text":"<pre><code>to_export_dict() -&gt; dict[str, Any]\n</code></pre> <p>Convert to dictionary for export, parsing JSON content if valid.</p> <p>Unlike to_dict(), this attempts to parse the content as JSON for more readable exported files.</p>"},{"location":"api/llm/cache/#causaliq_knowledge.llm.cache.LLMResponse.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(data: dict[str, Any]) -&gt; LLMResponse\n</code></pre> <p>Create from dictionary.</p>"},{"location":"api/llm/cache/#llmmetadata_1","title":"LLMMetadata","text":""},{"location":"api/llm/cache/#causaliq_knowledge.llm.cache.LLMMetadata","title":"LLMMetadata  <code>dataclass</code>","text":"<pre><code>LLMMetadata(\n    provider: str = \"\",\n    timestamp: str = \"\",\n    latency_ms: int = 0,\n    tokens: LLMTokenUsage = LLMTokenUsage(),\n    cost_usd: float = 0.0,\n    cache_hit: bool = False,\n    request_id: str = \"\",\n)\n</code></pre> <p>Metadata for a cached LLM response.</p> <p>Attributes:</p> <ul> <li> <code>provider</code>               (<code>str</code>)           \u2013            <p>LLM provider name (openai, anthropic, etc.).</p> </li> <li> <code>timestamp</code>               (<code>str</code>)           \u2013            <p>When the original request was made (ISO format).</p> </li> <li> <code>latency_ms</code>               (<code>int</code>)           \u2013            <p>Response time in milliseconds.</p> </li> <li> <code>tokens</code>               (<code>LLMTokenUsage</code>)           \u2013            <p>Token usage statistics.</p> </li> <li> <code>cost_usd</code>               (<code>float</code>)           \u2013            <p>Estimated cost of the request in USD.</p> </li> <li> <code>cache_hit</code>               (<code>bool</code>)           \u2013            <p>Whether this was served from cache.</p> </li> <li> <code>request_id</code>               (<code>str</code>)           \u2013            <p>Optional identifier for the request (not in cache key).</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>to_dict</code>             \u2013              <p>Convert to dictionary for JSON serialisation.</p> </li> <li> <code>from_dict</code>             \u2013              <p>Create from dictionary.</p> </li> </ul>"},{"location":"api/llm/cache/#causaliq_knowledge.llm.cache.LLMMetadata.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; dict[str, Any]\n</code></pre> <p>Convert to dictionary for JSON serialisation.</p>"},{"location":"api/llm/cache/#causaliq_knowledge.llm.cache.LLMMetadata.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(data: dict[str, Any]) -&gt; LLMMetadata\n</code></pre> <p>Create from dictionary.</p>"},{"location":"api/llm/cache/#llmtokenusage_1","title":"LLMTokenUsage","text":""},{"location":"api/llm/cache/#causaliq_knowledge.llm.cache.LLMTokenUsage","title":"LLMTokenUsage  <code>dataclass</code>","text":"<pre><code>LLMTokenUsage(input: int = 0, output: int = 0, total: int = 0)\n</code></pre> <p>Token usage statistics for an LLM request.</p> <p>Attributes:</p> <ul> <li> <code>input</code>               (<code>int</code>)           \u2013            <p>Number of tokens in the prompt.</p> </li> <li> <code>output</code>               (<code>int</code>)           \u2013            <p>Number of tokens in the completion.</p> </li> <li> <code>total</code>               (<code>int</code>)           \u2013            <p>Total tokens (input + output).</p> </li> </ul>"},{"location":"architecture/llm_integration/","title":"LLM Integration Design Note","text":""},{"location":"architecture/llm_integration/#overview","title":"Overview","text":"<p>This document describes how causaliq-knowledge integrates with Large Language Models (LLMs) to provide knowledge about causal relationships. The primary use case for v0.1.0 is answering queries about edge existence and edge orientation to support graph averaging in causaliq-analysis.</p>"},{"location":"architecture/llm_integration/#how-it-works","title":"How it works","text":""},{"location":"architecture/llm_integration/#query-flow","title":"Query Flow","text":"<ol> <li>Consumer requests knowledge about a potential edge (e.g., \"Does smoking cause cancer?\")</li> <li>KnowledgeProvider receives the query with optional context</li> <li>LLM client formats the query using structured prompts</li> <li>One or more LLMs are queried (configurable)</li> <li>Responses are parsed into structured <code>EdgeKnowledge</code> objects</li> <li>Multi-LLM consensus combines responses (if multiple models used)</li> <li>Result returned with confidence score and reasoning</li> </ol>"},{"location":"architecture/llm_integration/#core-interface","title":"Core Interface","text":"<pre><code>from abc import ABC, abstractmethod\nfrom pydantic import BaseModel\n\nclass EdgeKnowledge(BaseModel):\n    \"\"\"Structured knowledge about a potential causal edge.\"\"\"\n    exists: bool | None           # True, False, or None (uncertain)\n    direction: str | None         # \"a_to_b\", \"b_to_a\", \"undirected\", None\n    confidence: float             # 0.0 to 1.0\n    reasoning: str                # Human-readable explanation\n    model: str | None = None      # Which LLM provided this (for logging)\n\nclass KnowledgeProvider(ABC):\n    \"\"\"Abstract interface for all knowledge sources.\"\"\"\n\n    @abstractmethod\n    def query_edge(\n        self,\n        node_a: str,\n        node_b: str,\n        context: dict | None = None\n    ) -&gt; EdgeKnowledge:\n        \"\"\"\n        Query whether a causal edge exists between two nodes.\n\n        Args:\n            node_a: Name of first variable\n            node_b: Name of second variable  \n            context: Optional context (domain, variable descriptions, etc.)\n\n        Returns:\n            EdgeKnowledge with existence, direction, confidence, reasoning\n        \"\"\"\n        pass\n</code></pre>"},{"location":"architecture/llm_integration/#llm-implementation","title":"LLM Implementation","text":"<pre><code>class LLMKnowledge(KnowledgeProvider):\n    \"\"\"LLM-based knowledge provider using vendor-specific API clients.\"\"\"\n\n    def __init__(\n        self,\n        models: list[str] = [\"groq/llama-3.1-8b-instant\"],\n        consensus_strategy: str = \"weighted_vote\",\n        temperature: float = 0.1,\n        max_tokens: int = 500,\n    ):\n        \"\"\"\n        Initialize LLM knowledge provider.\n\n        Args:\n            models: List of model identifiers with provider prefix.\n                   e.g., [\"groq/llama-3.1-8b-instant\", \"gemini/gemini-2.5-flash\"]\n            consensus_strategy: How to combine multi-model responses\n                               \"weighted_vote\" or \"highest_confidence\"\n            temperature: LLM temperature (0.0-1.0)\n            max_tokens: Maximum tokens in response\n        \"\"\"\n        ...\n</code></pre>"},{"location":"architecture/llm_integration/#llm-provider-configuration","title":"LLM Provider Configuration","text":""},{"location":"architecture/llm_integration/#architectural-decision-vendor-specific-apis","title":"Architectural Decision: Vendor-Specific APIs","text":"<p>We use direct vendor-specific API clients rather than wrapper libraries like LiteLLM or LangChain. Each provider has a dedicated client class that uses httpx for HTTP communication.</p> <p>Benefits of this approach:</p> <ul> <li>Reliability: No wrapper bugs or version conflicts</li> <li>Minimal dependencies: Only httpx required for HTTP</li> <li>Full control: Direct access to vendor-specific features</li> <li>Better debugging: Clear stack traces without abstraction layers</li> <li>Predictable behavior: No surprises from wrapper library updates</li> </ul>"},{"location":"architecture/llm_integration/#supported-providers","title":"Supported Providers","text":"Provider Client Class Model Examples API Key Variable Groq <code>GroqClient</code> <code>groq/llama-3.1-8b-instant</code> <code>GROQ_API_KEY</code> Google Gemini <code>GeminiClient</code> <code>gemini/gemini-2.5-flash</code> <code>GEMINI_API_KEY</code> OpenAI <code>OpenAIClient</code> <code>openai/gpt-4o-mini</code> <code>OPENAI_API_KEY</code> Anthropic <code>AnthropicClient</code> <code>anthropic/claude-sonnet-4-20250514</code> <code>ANTHROPIC_API_KEY</code> DeepSeek <code>DeepSeekClient</code> <code>deepseek/deepseek-chat</code> <code>DEEPSEEK_API_KEY</code> Mistral <code>MistralClient</code> <code>mistral/mistral-small-latest</code> <code>MISTRAL_API_KEY</code> Ollama <code>OllamaClient</code> <code>ollama/llama3</code> N/A (local) <p>Additional providers can be added by implementing new client classes following the same pattern.</p>"},{"location":"architecture/llm_integration/#cost-considerations","title":"Cost Considerations","text":"<p>For edge queries (~500 tokens each):</p> Provider Model Cost per 1000 queries Quality Speed Groq llama-3.1-8b-instant Free tier Good Very fast Google gemini-2.5-flash Free tier Good Fast Ollama llama3 Free (local) Good Depends on HW DeepSeek deepseek-chat ~$0.07 Excellent Fast Mistral mistral-small-latest ~$0.50 Good Fast OpenAI gpt-4o-mini ~$0.15 Excellent Fast Anthropic claude-sonnet-4-20250514 ~$1.50 Excellent Fast <p>Recommendation: Use Groq free tier for development and testing. Ollama is great for local development. Both Groq and Gemini offer generous free tiers suitable for most research use cases.</p>"},{"location":"architecture/llm_integration/#prompt-design","title":"Prompt Design","text":""},{"location":"architecture/llm_integration/#edge-existence-query","title":"Edge Existence Query","text":"<pre><code>System: You are an expert in causal reasoning and domain knowledge. \nYour task is to assess whether a causal relationship exists between two variables.\nRespond in JSON format with: exists (true/false/null), direction (a_to_b/b_to_a/undirected/null), confidence (0-1), reasoning (string).\n\nUser: In the domain of {domain}, does a causal relationship exist between \"{node_a}\" and \"{node_b}\"?\nConsider:\n- Direct causation (A causes B)\n- Reverse causation (B causes A)  \n- Bidirectional/feedback relationships\n- No causal relationship (correlation only or independence)\n\nVariable context:\n{variable_descriptions}\n</code></pre>"},{"location":"architecture/llm_integration/#response-format","title":"Response Format","text":"<pre><code>{\n  \"exists\": true,\n  \"direction\": \"a_to_b\",\n  \"confidence\": 0.85,\n  \"reasoning\": \"Smoking is an established cause of lung cancer through well-documented biological mechanisms including DNA damage from carcinogens in tobacco smoke.\"\n}\n</code></pre>"},{"location":"architecture/llm_integration/#multi-llm-consensus","title":"Multi-LLM Consensus","text":"<p>When multiple models are configured, responses are combined:</p>"},{"location":"architecture/llm_integration/#weighted-vote-strategy-default","title":"Weighted Vote Strategy (default)","text":"<pre><code>def weighted_vote(responses: list[EdgeKnowledge]) -&gt; EdgeKnowledge:\n    \"\"\"Combine responses weighted by confidence.\"\"\"\n    # For existence: weighted majority vote\n    # For direction: weighted majority among those agreeing on existence\n    # Final confidence: average confidence of agreeing models\n    # Reasoning: concatenate key points from each model\n</code></pre>"},{"location":"architecture/llm_integration/#highest-confidence-strategy","title":"Highest Confidence Strategy","text":"<pre><code>def highest_confidence(responses: list[EdgeKnowledge]) -&gt; EdgeKnowledge:\n    \"\"\"Return response with highest confidence.\"\"\"\n    return max(responses, key=lambda r: r.confidence)\n</code></pre>"},{"location":"architecture/llm_integration/#integration-with-graph-averaging","title":"Integration with Graph Averaging","text":"<p>The primary consumer is <code>causaliq_analysis.graph.average()</code>:</p> <pre><code># Current output from average()\ndf = average(traces, sample_size=1000)\n# Returns: node_a, node_b, p_a_to_b, p_b_to_a, p_undirected, p_no_edge\n\n# Entropy calculation identifies uncertain edges\ndef edge_entropy(row):\n    probs = [row.p_a_to_b, row.p_b_to_a, row.p_undirected, row.p_no_edge]\n    probs = [p for p in probs if p &gt; 0]\n    return -sum(p * math.log2(p) for p in probs)\n\ndf[\"entropy\"] = df.apply(edge_entropy, axis=1)\nuncertain_edges = df[df[\"entropy\"] &gt; 1.5]  # High uncertainty\n\n# Query LLM for uncertain edges\nknowledge = LLMKnowledge(models=[\"groq/llama-3.1-8b-instant\"])\nfor _, row in uncertain_edges.iterrows():\n    result = knowledge.query_edge(row.node_a, row.node_b)\n    # Combine statistical and LLM probabilities...\n</code></pre>"},{"location":"architecture/llm_integration/#design-rationale","title":"Design Rationale","text":""},{"location":"architecture/llm_integration/#why-vendor-specific-apis-not-litellmlangchain","title":"Why Vendor-Specific APIs (not LiteLLM/LangChain)?","text":"<ol> <li>Minimal dependencies: Only httpx for HTTP, no wrapper libraries</li> <li>Reliability: No wrapper bugs or version conflicts to debug</li> <li>Full control: Direct access to vendor-specific features and error handling</li> <li>Predictable: Behavior doesn't change when wrapper library updates</li> <li>Debuggable: Clear stack traces without abstraction layers</li> <li>Lightweight: ~5KB of client code vs ~50MB of wrapper dependencies</li> </ol>"},{"location":"architecture/llm_integration/#why-structured-json-responses","title":"Why structured JSON responses?","text":"<ol> <li>Reliable parsing: Avoids regex/heuristic extraction</li> <li>Validation: Pydantic ensures response integrity</li> <li>Consistency: Same structure regardless of model</li> </ol>"},{"location":"architecture/llm_integration/#why-multi-model-consensus","title":"Why multi-model consensus?","text":"<ol> <li>Reduced hallucination: Multiple models catch individual errors</li> <li>Confidence calibration: Agreement increases confidence</li> <li>Robustness: Not dependent on single provider availability</li> </ol>"},{"location":"architecture/llm_integration/#error-handling-and-resilience","title":"Error Handling and Resilience","text":""},{"location":"architecture/llm_integration/#api-failures","title":"API Failures","text":"<ul> <li>Automatic retry with timeout handling</li> <li>Fallback to next model in list if primary fails</li> <li>Return <code>EdgeKnowledge(exists=None, confidence=0.0)</code> if all fail</li> </ul>"},{"location":"architecture/llm_integration/#invalid-responses","title":"Invalid Responses","text":"<ul> <li>Pydantic validation catches malformed JSON</li> <li>Default to <code>exists=None</code> if parsing fails</li> <li>Log warnings for debugging</li> </ul>"},{"location":"architecture/llm_integration/#rate-limiting","title":"Rate Limiting","text":"<ul> <li>Vendor clients handle rate limit errors gracefully</li> <li>Configure timeout per client</li> </ul>"},{"location":"architecture/llm_integration/#performance","title":"Performance","text":""},{"location":"architecture/llm_integration/#latency","title":"Latency","text":"<ul> <li>Single query: 0.5-2s depending on model/provider</li> <li>Batch queries: Can parallelize across edges (async)</li> <li>Cached queries: &lt;10ms</li> </ul>"},{"location":"architecture/llm_integration/#throughput-v030-with-caching","title":"Throughput (v0.3.0 with caching)","text":"<ul> <li>First query to new edge: 1-2s</li> <li>Cached query: &lt;10ms</li> <li>1000 unique edges: ~20-30 minutes (sequential), ~5 min (parallel)</li> </ul>"},{"location":"architecture/llm_integration/#future-extensions","title":"Future Extensions","text":""},{"location":"architecture/llm_integration/#v030-caching","title":"v0.3.0: Caching","text":"<ul> <li>Disk-based cache keyed by (node_a, node_b, context_hash)</li> <li>Semantic similarity cache for similar variable names</li> </ul>"},{"location":"architecture/llm_integration/#v040-rich-context","title":"v0.4.0: Rich Context","text":"<ul> <li>Variable descriptions and roles</li> <li>Domain-specific literature retrieval (RAG)</li> <li>Conversation history for follow-up queries</li> </ul>"},{"location":"architecture/llm_integration/#v050-algorithm-integration","title":"v0.5.0: Algorithm Integration","text":"<ul> <li>Direct integration with structure learning search</li> <li>Knowledge-guided constraint generation</li> </ul>"},{"location":"architecture/overview/","title":"Architecture Vision for causaliq-knowledge","text":""},{"location":"architecture/overview/#causaliq-ecosystem","title":"CausalIQ Ecosystem","text":"<p>causaliq-knowledge is a component of the overall CausalIQ ecosystem architecture.</p> <p>This package provides knowledge services to other CausalIQ packages, enabling them to incorporate LLM-derived and human-specified knowledge into causal discovery and inference workflows.</p>"},{"location":"architecture/overview/#architectural-principles","title":"Architectural Principles","text":""},{"location":"architecture/overview/#simplicity-first","title":"Simplicity First","text":"<ul> <li>Use lightweight libraries over heavy frameworks</li> <li>Start with minimal viable features, extend incrementally</li> <li>Prefer explicit code over framework \"magic\"</li> <li>Use vendor-specific APIs rather than abstraction wrappers</li> </ul>"},{"location":"architecture/overview/#cost-efficiency","title":"Cost Efficiency","text":"<ul> <li>Built-in cost tracking and budget management (critical for independent research)</li> <li>Caching of LLM queries and responses to avoid redundant API calls</li> <li>Support for cheap/free providers (Groq, Gemini free tiers)</li> </ul>"},{"location":"architecture/overview/#transparency-and-reproducibility","title":"Transparency and Reproducibility","text":"<ul> <li>Cache all LLM interactions for experiment reproducibility</li> <li>Provide reasoning/explanations with all knowledge outputs</li> <li>Log confidence levels to enable uncertainty-aware decisions</li> </ul>"},{"location":"architecture/overview/#clean-interfaces","title":"Clean Interfaces","text":"<ul> <li>Abstract <code>KnowledgeProvider</code> interface allows multiple implementations</li> <li>LLM-based, rule-based, and human-input knowledge sources use same interface</li> <li>Easy integration with causaliq-analysis and causaliq-discovery</li> </ul>"},{"location":"architecture/overview/#architecture-components","title":"Architecture Components","text":""},{"location":"architecture/overview/#core-components-v010","title":"Core Components (v0.1.0)","text":"<pre><code>causaliq_knowledge/\n\u251c\u2500\u2500 __init__.py              # Package exports\n\u251c\u2500\u2500 cli.py                   # Command-line interface\n\u251c\u2500\u2500 base.py                  # Abstract KnowledgeProvider interface\n\u251c\u2500\u2500 models.py                # Pydantic models (EdgeKnowledge, etc.)\n\u2514\u2500\u2500 llm/\n    \u251c\u2500\u2500 __init__.py          # LLM module exports\n    \u251c\u2500\u2500 groq_client.py       # Direct Groq API client\n    \u251c\u2500\u2500 gemini_client.py     # Direct Google Gemini API client\n    \u251c\u2500\u2500 prompts.py           # Prompt templates for edge queries\n    \u2514\u2500\u2500 provider.py          # LLMKnowledge implementation\n</code></pre>"},{"location":"architecture/overview/#data-flow","title":"Data Flow","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                 Consuming Package (e.g., causaliq-analysis)     \u2502\n\u2502                                                                 \u2502\n\u2502   uncertain_edges = df[df[\"entropy\"] &gt; threshold]               \u2502\n\u2502                          \u2502                                      \u2502\n\u2502                          \u25bc                                      \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502   \u2502              causaliq-knowledge                          \u2502   \u2502\n\u2502   \u2502                                                          \u2502   \u2502\n\u2502   \u2502   knowledge.query_edge(\"smoking\", \"cancer\")              \u2502   \u2502\n\u2502   \u2502       \u2502                                                  \u2502   \u2502\n\u2502   \u2502       \u25bc                                                  \u2502   \u2502\n\u2502   \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u2502   \u2502\n\u2502   \u2502   \u2502  LLM 1    \u2502    \u2502  LLM 2    \u2502    \u2502  Cache    \u2502       \u2502   \u2502\n\u2502   \u2502   \u2502 (GPT-4o)  \u2502    \u2502 (Llama3)  \u2502    \u2502 (disk)    \u2502       \u2502   \u2502\n\u2502   \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       \u2502   \u2502\n\u2502   \u2502       \u2502                 \u2502                \u2502               \u2502   \u2502\n\u2502   \u2502       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518               \u2502   \u2502\n\u2502   \u2502                         \u2502                                \u2502   \u2502\n\u2502   \u2502                         \u25bc                                \u2502   \u2502\n\u2502   \u2502               EdgeKnowledge(                             \u2502   \u2502\n\u2502   \u2502                   exists=True,                           \u2502   \u2502\n\u2502   \u2502                   direction=\"a_to_b\",                    \u2502   \u2502\n\u2502   \u2502                   confidence=0.85,                       \u2502   \u2502\n\u2502   \u2502                   reasoning=\"Established medical...\"     \u2502   \u2502\n\u2502   \u2502               )                                          \u2502   \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2502                          \u2502                                      \u2502\n\u2502                          \u25bc                                      \u2502\n\u2502   Combine with statistical probabilities                        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture/overview/#graph-generation-components-v040","title":"Graph Generation Components (v0.4.0)","text":"<pre><code>causaliq_knowledge/\n\u251c\u2500\u2500 action.py                # CausalIQ workflow action integration\n\u251c\u2500\u2500 cli/                     # Command-line interface\n\u2502   \u251c\u2500\u2500 main.py              # Core CLI entry point\n\u2502   \u251c\u2500\u2500 cache.py             # Cache management commands\n\u2502   \u251c\u2500\u2500 generate.py          # Graph generation commands\n\u2502   \u2514\u2500\u2500 models.py            # Model listing command\n\u2514\u2500\u2500 graph/\n    \u251c\u2500\u2500 __init__.py          # Module exports\n    \u251c\u2500\u2500 models.py            # NetworkContext, VariableSpec, etc.\n    \u251c\u2500\u2500 generator.py         # GraphGenerator class\n    \u251c\u2500\u2500 view_filter.py       # ViewFilter for context levels\n    \u251c\u2500\u2500 prompts.py           # GraphQueryPrompt builder\n    \u251c\u2500\u2500 response.py          # GeneratedGraph, ProposedEdge\n    \u251c\u2500\u2500 params.py            # GenerateGraphParams\n    \u2514\u2500\u2500 cache.py             # GraphCompressor for workflow cache\n</code></pre>"},{"location":"architecture/overview/#graph-generation-data-flow","title":"Graph Generation Data Flow","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    Graph Generation Flow                         \u2502\n\u2502                                                                 \u2502\n\u2502   network_context.json                                          \u2502\n\u2502       \u2502                                                         \u2502\n\u2502       \u25bc                                                         \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                             \u2502\n\u2502   \u2502NetworkContext \u2502  Load and validate JSON context             \u2502\n\u2502   \u2502   .load()     \u2502                                             \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                             \u2502\n\u2502       \u2502                                                         \u2502\n\u2502       \u25bc                                                         \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                  \u2502\n\u2502   \u2502  ViewFilter   \u2502\u2500\u2500\u2500\u25b6\u2502 PromptDetail.MINIMAL \u2502  Names only     \u2502\n\u2502   \u2502               \u2502    \u2502 PromptDetail.STANDARD\u2502  + descriptions \u2502\n\u2502   \u2502               \u2502    \u2502 PromptDetail.RICH    \u2502  Full metadata  \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                  \u2502\n\u2502       \u2502                                                         \u2502\n\u2502       \u25bc                                                         \u2502\n\u2502   name/llm_name mapping:  smoke \u2192 tobacco_history               \u2502\n\u2502   (via VariableSpec.llm_name field)                             \u2502\n\u2502       \u2502                                                         \u2502\n\u2502       \u25bc                                                         \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                             \u2502\n\u2502   \u2502GraphGenerator \u2502  Generate causal graph                      \u2502\n\u2502   \u2502  + LLM Client \u2502  (multiple provider support)                \u2502\n\u2502   \u2502  + TokenCache \u2502  (cached responses)                         \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                             \u2502\n\u2502       \u2502                                                         \u2502\n\u2502       \u25bc                                                         \u2502\n\u2502   llm_name \u2192 name mapping:  tobacco_history \u2192 smoke             \u2502\n\u2502   (automatic translation in response)                           \u2502\n\u2502       \u2502                                                         \u2502\n\u2502       \u25bc                                                         \u2502\n\u2502   GeneratedGraph(edges=[(\"smoke\", \"cancer\"), ...])              \u2502\n\u2502       \u2502                                                         \u2502\n\u2502       \u25bc                                                         \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                             \u2502\n\u2502   \u2502GraphCompressor\u2502  Compress for Workflow Cache                \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture/overview/#technology-choices","title":"Technology Choices","text":""},{"location":"architecture/overview/#vendor-specific-apis-over-wrapper-libraries","title":"Vendor-Specific APIs over Wrapper Libraries","text":"<p>We use direct vendor-specific API clients rather than wrapper libraries like LiteLLM or LangChain. This architectural decision provides:</p> Aspect Direct APIs Wrapper Libraries Reliability \u2705 Full control, predictable \u274c Wrapper bugs, version drift Debugging \u2705 Clear stack traces \u274c Abstraction layers Dependencies \u2705 Minimal (httpx only) \u274c Heavy transitive deps API Coverage \u2705 Full vendor features \u274c Lowest common denominator Maintenance \u2705 We control updates \u274c Wait for wrapper updates <p>Why Not LiteLLM?</p> <ul> <li>Adds 50+ transitive dependencies</li> <li>Version conflicts with other packages</li> <li>Wrapper bugs mask vendor API issues</li> <li>We only need 2-3 providers, not 100+</li> </ul> <p>Why Not LangChain?</p> <ul> <li>Massive dependency footprint (~100MB+)</li> <li>Over-engineered for simple structured queries  </li> <li>Rapid breaking changes between versions</li> <li>May reconsider for v0.4.0+ RAG features only</li> </ul>"},{"location":"architecture/overview/#current-provider-clients","title":"Current Provider Clients","text":"<ul> <li>GroqClient: Direct Groq API via httpx (free tier, fast inference)</li> <li>GeminiClient: Direct Google Gemini API via httpx (generous free tier)</li> </ul>"},{"location":"architecture/overview/#key-dependencies","title":"Key Dependencies","text":"<ul> <li>httpx: HTTP client for API calls</li> <li>pydantic: Structured response validation</li> <li>click: Command-line interface</li> <li>diskcache (v0.3.0): Persistent query caching</li> </ul>"},{"location":"architecture/overview/#integration-points","title":"Integration Points","text":""},{"location":"architecture/overview/#with-causaliq-analysis","title":"With causaliq-analysis","text":"<p>The primary integration point is the <code>average()</code> function which produces edge probability tables. Future versions will accept a <code>knowledge</code> parameter:</p> <pre><code># Future usage (v0.5.0 of causaliq-analysis)\nfrom causaliq_knowledge import LLMKnowledge\n\nknowledge = LLMKnowledge(models=[\"gpt-4o-mini\"])\ndf = average(traces, sample_size=1000, knowledge=knowledge)\n</code></pre>"},{"location":"architecture/overview/#with-causaliq-discovery","title":"With causaliq-discovery","text":"<p>Structure learning algorithms will use knowledge to guide search in uncertain areas of the graph space.</p>"},{"location":"architecture/overview/#see-also","title":"See Also","text":"<ul> <li>LLM Integration Design Note - Detailed design for LLM queries</li> <li>Roadmap - Release planning</li> </ul>"},{"location":"architecture/testing_strategy/","title":"Testing Strategy Design Note","text":""},{"location":"architecture/testing_strategy/#overview","title":"Overview","text":"<p>Testing LLM-dependent code presents unique challenges: API calls cost money, responses are non-deterministic, and external services may be unavailable. This document describes the testing strategy for causaliq-knowledge.</p>"},{"location":"architecture/testing_strategy/#testing-pyramid","title":"Testing Pyramid","text":"<pre><code>                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502   Functional    \u2502  \u2190 Cached responses\n                    \u2502     Tests       \u2502     Real scenarios, reproducible\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n               \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n               \u2502     Integration Tests     \u2502  \u2190 Real API calls (optional)\n               \u2502   (with live LLM APIs)    \u2502     Expensive, non-deterministic\n               \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502                  Unit Tests                      \u2502  \u2190 Mocked LLM responses\n    \u2502           (mocked LLM responses)                 \u2502     Fast, free, deterministic\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture/testing_strategy/#test-categories","title":"Test Categories","text":""},{"location":"architecture/testing_strategy/#1-unit-tests-always-run-in-ci","title":"1. Unit Tests (Always Run in CI)","text":"<p>Unit tests mock all LLM calls, making them:</p> <ul> <li>Fast: No network latency</li> <li>Free: No API costs</li> <li>Deterministic: Same result every time</li> <li>Isolated: No external dependencies</li> </ul> <pre><code># tests/unit/graph/test_generator.py\nimport pytest\nfrom unittest.mock import MagicMock\n\n\n# Test graph generator creates edges from LLM response.\ndef test_generator_creates_edges_from_response(monkeypatch):\n    \"\"\"Test that valid LLM JSON is correctly parsed into edges.\"\"\"\n    from causaliq_knowledge.graph import GraphGenerator, GraphGeneratorConfig\n    from causaliq_knowledge.graph import ModelSpec, VariableSpec\n\n    # Create test model spec\n    spec = ModelSpec(\n        name=\"test\",\n        variables=[\n            VariableSpec(id=\"A\", name=\"smoking\"),\n            VariableSpec(id=\"B\", name=\"cancer\"),\n        ],\n    )\n\n    config = GraphGeneratorConfig(\n        llm_model=\"groq/llama-3.1-8b-instant\",\n        prompt_detail=\"standard\",\n    )\n\n    # Mock the LLM client\n    mock_response = {\n        \"edges\": [\n            {\"source\": \"A\", \"target\": \"B\", \"confidence\": 0.85}\n        ]\n    }\n\n    generator = GraphGenerator(config)\n    # Mock internal client call\n    generator._client = MagicMock()\n    generator._client.complete_json.return_value = (mock_response, None)\n\n    result = generator.generate(spec)\n\n    assert len(result.edges) == 1\n    assert result.edges[0].source == \"A\"\n    assert result.edges[0].target == \"B\"\n</code></pre>"},{"location":"architecture/testing_strategy/#2-integration-tests-optional-manual-or-ci-with-secrets","title":"2. Integration Tests (Optional, Manual or CI with Secrets)","text":"<p>Integration tests use real LLM APIs to validate actual behaviour:</p> <ul> <li>Expensive: May cost money per call (though free tiers available)</li> <li>Non-deterministic: LLM responses vary</li> <li>Slow: Network latency</li> <li>Validates real integration: Catches API changes</li> </ul> <pre><code># tests/integration/test_graph_generation_live.py\nimport pytest\nimport os\n\npytestmark = pytest.mark.skipif(\n    not os.getenv(\"GROQ_API_KEY\"),\n    reason=\"GROQ_API_KEY not set\"\n)\n\n\n@pytest.mark.slow\n@pytest.mark.integration\ndef test_groq_generates_valid_graph():\n    \"\"\"Validate real Groq API returns parseable graph.\"\"\"\n    from causaliq_knowledge.graph import GraphGenerator, GraphGeneratorConfig\n    from causaliq_knowledge.graph import NetworkContext\n\n    context = NetworkContext.load(\"tests/data/simple_context.json\")\n\n    config = GraphGeneratorConfig(\n        temperature=0.1,\n        prompt_detail=\"standard\",\n    )\n\n    generator = GraphGenerator(\n        model=\"groq/llama-3.1-8b-instant\",\n        config=config,\n    )\n    result = generator.generate_from_context(context)\n\n    # Don't assert specific values - LLM may vary\n    # Just validate structure and reasonable bounds\n    assert len(result.edges) &gt;= 0\n    for edge in result.edges:\n        assert 0.0 &lt;= edge.confidence &lt;= 1.0\n</code></pre>"},{"location":"architecture/testing_strategy/#3-functional-tests-with-cached-responses","title":"3. Functional Tests with Cached Responses","text":"<p>Functional tests use cached LLM responses for reproducible testing:</p> <ul> <li>Realistic: Uses actual LLM responses (captured once)</li> <li>Deterministic: Same cached response every time</li> <li>Free: No API calls after initial capture</li> <li>Fast: Disk read instead of network call</li> </ul>"},{"location":"architecture/testing_strategy/#how-it-works","title":"How It Works","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                     Test Fixture Generation                      \u2502\n\u2502                      (run once, manually)                        \u2502\n\u2502                                                                  \u2502\n\u2502   1. Run graph generation against real LLMs                      \u2502\n\u2502   2. Cache stores responses in tests/data/functional/cache/     \u2502\n\u2502   3. Commit cache files to git                                   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                     Functional Tests (CI)                        \u2502\n\u2502                                                                  \u2502\n\u2502   1. Load cached responses from tests/data/functional/cache/    \u2502\n\u2502   2. GraphGenerator configured to use cache-only mode           \u2502\n\u2502   3. Tests run with real LLM responses, no API calls            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture/testing_strategy/#ci-configuration","title":"CI Configuration","text":""},{"location":"architecture/testing_strategy/#pytest-markers","title":"pytest Markers","text":"<pre><code># pyproject.toml\n[tool.pytest.ini_options]\nmarkers = [\n    \"slow: marks tests as slow (deselect with '-m \\\"not slow\\\"')\",\n    \"integration: marks tests requiring live external APIs\",\n    \"functional: marks functional tests using cached responses\",\n]\naddopts = \"-ra -q --strict-markers -m 'not slow and not integration'\"\n</code></pre>"},{"location":"architecture/testing_strategy/#github-actions-strategy","title":"GitHub Actions Strategy","text":"Test Type When API Keys Cost Unit Every push/PR No Free Functional Every push/PR Uses cache Free Integration Main branch only, optional GitHub Secrets ~$0.01/run <pre><code># .github/workflows/ci.yml (conceptual)\njobs:\n  unit-and-functional:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - name: Run unit and functional tests\n        run: pytest tests/unit tests/functional\n\n  integration:\n    runs-on: ubuntu-latest\n    if: github.ref == 'refs/heads/main'  # Only on main\n    steps:\n      - uses: actions/checkout@v4\n      - name: Run integration tests\n        env:\n          GROQ_API_KEY: ${{ secrets.GROQ_API_KEY }}\n        run: pytest tests/integration -m integration\n</code></pre>"},{"location":"architecture/testing_strategy/#test-data-management","title":"Test Data Management","text":""},{"location":"architecture/testing_strategy/#directory-structure","title":"Directory Structure","text":"<pre><code>tests/\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 unit/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 graph/\n\u2502   \u2502   \u251c\u2500\u2500 test_generator.py   # Graph generation\n\u2502   \u2502   \u251c\u2500\u2500 test_models.py      # Network context models\n\u2502   \u2502   \u2514\u2500\u2500 test_view_filter.py # View filtering\n\u2502   \u2514\u2500\u2500 llm/\n\u2502       \u251c\u2500\u2500 test_clients.py     # LLM clients\n\u2502       \u2514\u2500\u2500 test_config.py      # Configuration\n\u251c\u2500\u2500 integration/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2514\u2500\u2500 test_graph_live.py      # Real API calls\n\u251c\u2500\u2500 functional/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2514\u2500\u2500 test_graph_cached.py    # Using cached responses\n\u2514\u2500\u2500 data/\n    \u251c\u2500\u2500 network_contexts/       # Test network context files\n    \u2502   \u251c\u2500\u2500 simple.json\n    \u2502   \u2514\u2500\u2500 cancer.json\n    \u2514\u2500\u2500 functional/\n        \u2514\u2500\u2500 cache/              # Committed to git\n            \u2514\u2500\u2500 groq/\n                \u2514\u2500\u2500 simple_graph.json\n</code></pre>"},{"location":"architecture/testing_strategy/#benefits-of-this-strategy","title":"Benefits of This Strategy","text":"Benefit How Achieved Fast CI Unit tests are mocked, functional use cache Low cost Only integration tests (optional) call APIs Reproducible Cached responses are deterministic Realistic Functional tests use real LLM responses Stable experiments Same cache = same results across runs Version controlled Cache files in git track response changes"},{"location":"architecture/testing_strategy/#future-considerations","title":"Future Considerations","text":""},{"location":"architecture/testing_strategy/#cache-invalidation-for-tests","title":"Cache Invalidation for Tests","text":"<p>When updating test fixtures:</p> <ol> <li>Delete relevant cache files</li> <li>Run fixture generation script</li> <li>Review new responses</li> <li>Commit updated cache files</li> </ol>"},{"location":"architecture/testing_strategy/#model-version-tracking","title":"Model Version Tracking","text":"<p>Cache files should include model version to detect when responses might change due to model updates.</p>"},{"location":"userguide/introduction/","title":"CausalIQ Knowledge User Guide","text":""},{"location":"userguide/introduction/#what-is-causaliq-knowledge","title":"What is CausalIQ Knowledge?","text":"<p>CausalIQ Knowledge is a Python package that provides LLM-based graph generation for causal discovery workflows. It enables you to generate complete causal graphs from network context specifications using Large Language Models (LLMs), providing prior knowledge structures that can be compared against data-driven discoveries.</p>"},{"location":"userguide/introduction/#primary-use-case","title":"Primary Use Case","text":"<p>CausalIQ Knowledge generates causal graphs from network context files that describe variables, their types, and domain context. This is useful for:</p> <ol> <li>Creating prior knowledge graphs for causal discovery algorithms</li> <li>Comparing LLM-generated graphs against ground truth benchmarks</li> <li>Exploring how different LLMs reason about causal relationships</li> </ol>"},{"location":"userguide/introduction/#quick-start","title":"Quick Start","text":""},{"location":"userguide/introduction/#installation","title":"Installation","text":"<pre><code>pip install causaliq-knowledge\n</code></pre>"},{"location":"userguide/introduction/#command-line-usage","title":"Command Line Usage","text":"<p>Generate a causal graph from a network context file:</p> <pre><code># Generate graph with caching\ncqknow generate_graph -s context.json -o results/ -c cache.db\n\n# Use a specific LLM model\ncqknow generate_graph -s context.json -o results/ -c cache.db -m gemini/gemini-2.5-flash\n\n# Rich context for detailed prompts\ncqknow generate_graph -s context.json -o results/ -c cache.db -p rich\n</code></pre>"},{"location":"userguide/introduction/#python-api-usage","title":"Python API Usage","text":"<pre><code>from causaliq_knowledge.graph import GraphGenerator, GraphGeneratorConfig\nfrom causaliq_knowledge.graph import NetworkContext\n\n# Load network context\ncontext = NetworkContext.load(\"context.json\")\n\n# Configure generator\nconfig = GraphGeneratorConfig(\n    temperature=0.1,\n    prompt_detail=\"standard\",\n)\n\n# Generate graph\ngenerator = GraphGenerator(model=\"groq/llama-3.1-8b-instant\", config=config)\nresult = generator.generate_from_context(context)\n\nprint(f\"Generated {len(result.edges)} edges\")\nfor edge in result.edges:\n    print(f\"  {edge.source} -&gt; {edge.target} ({edge.confidence})\")\n</code></pre>"},{"location":"userguide/introduction/#llm-provider-setup","title":"LLM Provider Setup","text":"<p>CausalIQ Knowledge uses direct vendor-specific API clients (not wrapper libraries) to communicate with LLM providers. This approach provides reliability and minimal dependencies. Currently supported:</p> <ul> <li>Groq: Free tier with fast inference</li> <li>Google Gemini: Generous free tier</li> <li>OpenAI: GPT-4o and other models</li> <li>Anthropic: Claude models</li> <li>DeepSeek: DeepSeek-V3 and R1 models</li> <li>Mistral: Mistral AI models</li> <li>Ollama: Local LLMs (free, runs locally)</li> </ul>"},{"location":"userguide/introduction/#free-options","title":"Free Options","text":""},{"location":"userguide/introduction/#groq-free-tier-very-fast","title":"Groq (Free Tier - Very Fast)","text":"<p>Groq offers a generous free tier with extremely fast inference:</p> <ol> <li>Sign up at console.groq.com</li> <li>Create an API key</li> <li>Set the environment variable (see Storing API Keys)</li> <li>Use in code:</li> </ol> <pre><code>cqknow generate_graph -n context.json -o results/ -c cache.db -m groq/llama-3.1-8b-instant\n</code></pre> <p>Available Groq models: <code>groq/llama-3.1-8b-instant</code>, <code>groq/llama-3.1-70b-versatile</code>, <code>groq/mixtral-8x7b-32768</code></p>"},{"location":"userguide/introduction/#google-gemini-free-tier","title":"Google Gemini (Free Tier)","text":"<p>Google offers free access to Gemini models:</p> <ol> <li>Sign up at aistudio.google.com/apikey</li> <li>Create an API key</li> <li>Set <code>GEMINI_API_KEY</code> environment variable</li> <li>Use in code:</li> </ol> <pre><code>cqknow generate_graph -n context.json -o results/ -c cache.db -m gemini/gemini-2.5-flash\n</code></pre>"},{"location":"userguide/introduction/#openai","title":"OpenAI","text":"<p>OpenAI provides GPT-4o and other models:</p> <ol> <li>Sign up at platform.openai.com</li> <li>Create an API key</li> <li>Set <code>OPENAI_API_KEY</code> environment variable</li> </ol> <pre><code>cqknow generate_graph -n context.json -o results/ -c cache.db -m openai/gpt-4o-mini\n</code></pre>"},{"location":"userguide/introduction/#anthropic","title":"Anthropic","text":"<p>Anthropic provides Claude models:</p> <ol> <li>Sign up at console.anthropic.com</li> <li>Create an API key</li> <li>Set <code>ANTHROPIC_API_KEY</code> environment variable</li> </ol> <pre><code>cqknow generate_graph -n context.json -o results/ -c cache.db -m anthropic/claude-sonnet-4-20250514\n</code></pre>"},{"location":"userguide/introduction/#deepseek","title":"DeepSeek","text":"<p>DeepSeek offers high-quality models at competitive prices:</p> <ol> <li>Sign up at platform.deepseek.com</li> <li>Create an API key</li> <li>Set <code>DEEPSEEK_API_KEY</code> environment variable</li> </ol> <pre><code>cqknow generate_graph -n context.json -o results/ -c cache.db -m deepseek/deepseek-chat\n</code></pre>"},{"location":"userguide/introduction/#mistral","title":"Mistral","text":"<p>Mistral AI provides models with EU data sovereignty:</p> <ol> <li>Sign up at console.mistral.ai</li> <li>Create an API key</li> <li>Set <code>MISTRAL_API_KEY</code> environment variable</li> </ol> <pre><code>cqknow generate_graph -n context.json -o results/ -c cache.db -m mistral/mistral-small-latest\n</code></pre>"},{"location":"userguide/introduction/#ollama-local","title":"Ollama (Local)","text":"<p>Run models locally with Ollama (no API key needed):</p> <ol> <li>Install Ollama from ollama.ai</li> <li>Pull a model: <code>ollama pull llama3</code></li> <li>Use in code:</li> </ol> <pre><code>cqknow generate_graph -n context.json -o results/ -c cache.db -m ollama/llama3\n</code></pre>"},{"location":"userguide/introduction/#storing-api-keys","title":"Storing API Keys","text":""},{"location":"userguide/introduction/#option-1-user-environment-variables-recommended","title":"Option 1: User Environment Variables (Recommended)","text":"<p>Set permanently for your user account on Windows:</p> <pre><code>[Environment]::SetEnvironmentVariable(\"GROQ_API_KEY\", \"your-key\", \"User\")\n[Environment]::SetEnvironmentVariable(\"GEMINI_API_KEY\", \"your-key\", \"User\")\n</code></pre> <p>On Linux/macOS, add to your <code>~/.bashrc</code> or <code>~/.zshrc</code>:</p> <pre><code>export GROQ_API_KEY=\"your-key\"\nexport GEMINI_API_KEY=\"your-key\"\n</code></pre> <p>Restart your terminal for changes to take effect.</p>"},{"location":"userguide/introduction/#option-2-project-env-file","title":"Option 2: Project <code>.env</code> File","text":"<p>Create a <code>.env</code> file in your project root:</p> <pre><code>GROQ_API_KEY=your-key-here\nGEMINI_API_KEY=your-key-here\n</code></pre> <p>Important: Add <code>.env</code> to your <code>.gitignore</code> so keys aren't committed to version control!</p>"},{"location":"userguide/introduction/#option-3-password-manager","title":"Option 3: Password Manager","text":"<p>Store API keys in a password manager (LastPass, 1Password, Bitwarden, etc.) as a Secure Note. This provides:</p> <ul> <li>Encrypted backup of your keys</li> <li>Access from any machine</li> <li>Secure sharing with team members if needed</li> </ul> <p>Copy keys from your password manager when setting environment variables.</p>"},{"location":"userguide/introduction/#verifying-your-setup","title":"Verifying Your Setup","text":"<p>Test your configuration with the CLI:</p> <pre><code># Generate a test graph (requires a network context file)\ncqknow generate_graph -n context.json -o none -c none -m groq/llama-3.1-8b-instant\n\n# Or view cache statistics\ncqknow cache stats ./cache.db\n</code></pre>"},{"location":"userguide/introduction/#graph-generation","title":"Graph Generation","text":"<p>CausalIQ Knowledge can generate complete causal graphs from network context files using LLMs. This is useful for creating prior knowledge structures or comparing LLM-generated graphs against ground truth.</p>"},{"location":"userguide/introduction/#command-line-interface","title":"Command Line Interface","text":"<pre><code>cqknow generate_graph -n &lt;context.json&gt; -o &lt;output&gt; -c &lt;cache&gt; [options]\n</code></pre>"},{"location":"userguide/introduction/#basic-examples","title":"Basic Examples","text":"<pre><code># Generate a graph, save to Workflow Cache with LLM caching\ncqknow generate_graph -n context.json -o workflow.db -c cache.db\n\n# Generate to a directory (GraphML + JSON files)\ncqknow generate_graph -n context.json -o results/ -c cache.db\n\n# Generate without caching, print adjacency matrix to stdout\ncqknow generate_graph -n context.json -o none -c none\n\n# Use a specific LLM model\ncqknow generate_graph -n context.json -o workflow.db -c cache.db -m gemini/gemini-2.5-flash\n\n# Use rich context level for more detailed prompts\ncqknow generate_graph -n context.json -o workflow.db -c cache.db -p rich\n\n# Test benchmark memorisation with original variable names\ncqknow generate_graph -n context.json -o workflow.db -c cache.db --use-benchmark-names\n</code></pre>"},{"location":"userguide/introduction/#cli-options","title":"CLI Options","text":"Option Short Default Description <code>--network-context</code> <code>-n</code> (required) Path to network context JSON file <code>--output</code> <code>-o</code> (required) Output: Workflow Cache <code>.db</code>, directory, or <code>none</code> <code>--llm-cache</code> <code>-c</code> (required) Cache: <code>.db</code> file path or <code>none</code> to disable <code>--prompt-detail</code> <code>-p</code> <code>standard</code> Detail level: <code>minimal</code>, <code>standard</code>, or <code>rich</code> <code>--llm-model</code> <code>-m</code> <code>groq/llama-3.1-8b-instant</code> LLM model with provider prefix <code>--llm-temperature</code> <code>-t</code> <code>0.1</code> Temperature (0.0-2.0), lower = deterministic <code>--use-benchmark-names</code> <code>False</code> Use benchmark names (test memorisation)"},{"location":"userguide/introduction/#prompt-detail-levels","title":"Prompt Detail Levels","text":"<p>The <code>--prompt-detail</code> option controls how much context is provided to the LLM:</p> <ul> <li>minimal: Variable names only - tests LLM's general knowledge</li> <li>standard: Names with types, states, and short descriptions</li> <li>rich: Full context including extended descriptions and sensitivity hints</li> </ul>"},{"location":"userguide/introduction/#output-behaviour","title":"Output Behaviour","text":"<ul> <li>With <code>-o workflow.db</code>: Writes graph to Workflow Cache database</li> <li>With <code>-o results/</code>: Writes <code>graph.graphml</code>, <code>metadata.json</code>, and   <code>confidences.json</code> to the directory</li> <li>With <code>-o none</code>: Prints adjacency matrix to stdout</li> </ul>"},{"location":"userguide/introduction/#caching","title":"Caching","text":"<p>LLM responses are cached to avoid redundant API calls. The cache stores request/response pairs keyed by model, prompt, and parameters.</p> <pre><code># Enable caching (recommended for development)\ncqknow generate_graph -n context.json -o graph.json -c cache.db\n\n# Disable caching (for fresh responses)\ncqknow generate_graph -n context.json -o graph.json -c none\n</code></pre>"},{"location":"userguide/introduction/#using-with-causaliq-workflows","title":"Using with CausalIQ Workflows","text":"<p>CausalIQ Knowledge integrates with causaliq-workflow for reproducible, automated causal discovery experiments.</p>"},{"location":"userguide/introduction/#installation_1","title":"Installation","text":"<p>Ensure both packages are installed:</p> <pre><code>pip install causaliq-knowledge causaliq-workflow\n</code></pre>"},{"location":"userguide/introduction/#basic-workflow-example","title":"Basic Workflow Example","text":"<p>Create a workflow file <code>generate_graph.yaml</code>:</p> <pre><code>description: \"Generate causal graph using LLM\"\nid: \"llm-graph-generation\"\n\nsteps:\n  - name: \"Generate Graph\"\n    uses: \"causaliq-knowledge\"\n    with:\n      action: \"generate_graph\"\n      network_context: \"models/cancer.json\"\n      output: \"results/cancer_graph.json\"\n      llm_cache: \"cache/cancer.db\"\n      llm_model: \"groq/llama-3.1-8b-instant\"\n      prompt_detail: \"standard\"\n</code></pre> <p>Run the workflow:</p> <pre><code># Dry-run (validate without executing)\ncausaliq-workflow generate_graph.yaml --mode dry-run\n\n# Execute the workflow\ncausaliq-workflow generate_graph.yaml --mode run\n</code></pre>"},{"location":"userguide/introduction/#workflow-action-parameters","title":"Workflow Action Parameters","text":"Parameter Required Default Description <code>action</code> Yes - Must be <code>generate_graph</code> <code>network_context</code> Yes - Path to network context JSON <code>output</code> Yes - Output <code>.json</code> file path or <code>none</code> <code>llm_cache</code> Yes - Cache <code>.db</code> file path or <code>none</code> <code>llm_model</code> No <code>groq/llama-3.1-8b-instant</code> LLM model identifier <code>prompt_detail</code> No <code>standard</code> <code>minimal</code>, <code>standard</code>, or <code>rich</code> <code>use_benchmark_names</code> No <code>false</code> Use original benchmark names <code>llm_temperature</code> No <code>0.1</code> LLM temperature (0.0-2.0)"},{"location":"userguide/introduction/#matrix-expansion-example","title":"Matrix Expansion Example","text":"<p>Run the same analysis across multiple models and prompt detail levels:</p> <pre><code>description: \\\"Compare LLM graph generation across models\\\"\nid: \\\"llm-comparison\\\"\nworkflow_cache: \\\"results/{{id}}_cache.db\\\"\n\nmatrix:\n  model:\n    - \\\"groq/llama-3.1-8b-instant\\\"\n    - \\\"gemini/gemini-2.5-flash\\\"\n  detail:\n    - \\\"minimal\\\"\n    - \\\"standard\\\"\n    - \\\"rich\\\"\n\nsteps:\n  - name: \\\"Generate Graph\\\"\n    uses: \"causaliq-knowledge\"\n    with:\n      action: \"generate_graph\"\n      network_context: \"models/cancer.json\"\n      llm_cache: \"cache/llm_cache.db\"\n      llm_model: \"{{model}}\"\n      prompt_detail: \"{{detail}}\"\n</code></pre> <p>This generates 6 graphs (2 models \u00d7 3 detail levels), all stored in the Workflow Cache with matrix values as keys.</p>"},{"location":"userguide/introduction/#multi-network-comparison","title":"Multi-Network Comparison","text":"<p>Compare graph generation across different causal networks:</p> <pre><code>description: \"Generate graphs for multiple networks\"\nid: \"multi-network\"\nworkflow_cache: \"results/{{id}}_cache.db\"\n\nmatrix:\n  network:\n    - \"asia\"\n    - \"cancer\"\n    - \"earthquake\"\n\nsteps:\n  - name: \"Generate Graph\"\n    uses: \"causaliq-knowledge\"\n    with:\n      action: \"generate_graph\"\n      network_context: \"models/{{network}}/{{network}}.json\"\n      llm_cache: \"cache/{{network}}_llm.db\"\n      llm_model: \"groq/llama-3.1-8b-instant\"\n</code></pre>"},{"location":"userguide/introduction/#workflow-output","title":"Workflow Output","text":"<p>When a workflow step completes, the action returns:</p> <pre><code>{\n  \"status\": \"success\",\n  \"edges_count\": 5,\n  \"variables_count\": 8,\n  \"output_file\": \"results/cancer_graph.json\",\n  \"cache_stats\": {\n    \"cache_hits\": 2,\n    \"cache_misses\": 6\n  }\n}\n</code></pre> <p>For more information on model specification format, see Model Specification Format.</p>"},{"location":"userguide/introduction/#cache-management","title":"Cache Management","text":"<p>CausalIQ Knowledge includes a caching system that stores LLM responses to avoid redundant API calls. You can inspect, export, and import your cache using the CLI:</p> <pre><code># View cache statistics\ncqknow cache stats ./llm_cache.db\n\n# Export cache entries to human-readable JSON files\ncqknow cache export ./llm_cache.db ./export_dir\n\n# Export to a zip archive for sharing\ncqknow cache export ./llm_cache.db ./export.zip\n\n# Import cache entries (auto-detects entry types)\ncqknow cache import ./new_cache.db ./export.zip\n</code></pre> <p>Exported files use the naming format <code>{id}_{timestamp}_{provider}.json</code>, for example <code>cli_2026-01-29-143052_groq.json</code>. The <code>id</code> comes from the <code>--id</code> option when generating graphs (default: \"cli\").</p> <p>The cache stores:</p> <ul> <li>Entry count: Number of cached LLM responses</li> <li>Token count: Total tokens across all cached entries</li> </ul>"},{"location":"userguide/introduction/#whats-next","title":"What's Next?","text":"<ul> <li>Architecture Overview - Understand how the package works</li> <li>LLM Integration Design - Detailed design documentation</li> <li>API Reference - Full API documentation</li> </ul>"},{"location":"userguide/model_specification/","title":"Network Context Format","text":"<p>This guide describes the JSON format for network context files used in causaliq-knowledge graph generation.</p>"},{"location":"userguide/model_specification/#overview","title":"Overview","text":"<p>Network context files define the variables, metadata, and constraints for a causal network. They enable LLMs to generate causal graphs with appropriate domain context while allowing control over how much information is provided.</p>"},{"location":"userguide/model_specification/#minimal-example","title":"Minimal Example","text":"<p>The simplest valid network context:</p> <pre><code>{\n    \"network\": \"simple\",\n    \"domain\": \"test\",\n    \"variables\": [\n        {\"name\": \"X\", \"type\": \"binary\"},\n        {\"name\": \"Y\", \"type\": \"binary\"}\n    ]\n}\n</code></pre>"},{"location":"userguide/model_specification/#complete-example","title":"Complete Example","text":"<p>A comprehensive network context with all optional fields:</p> <pre><code>{\n    \"schema_version\": \"2.0\",\n    \"network\": \"smoking_cancer\",\n    \"domain\": \"epidemiology\",\n    \"purpose\": \"Causal model for smoking and lung cancer relationship\",\n\n    \"provenance\": {\n        \"source_network\": \"LUNG\",\n        \"source_reference\": \"Lauritzen &amp; Spiegelhalter (1988)\",\n        \"source_url\": \"https://example.com/lung-network\",\n        \"memorization_risk\": \"high\",\n        \"notes\": \"Well-known benchmark, LLMs may have memorised structure\"\n    },\n\n    \"llm_guidance\": {\n        \"usage_notes\": [\n            \"Focus on biological plausibility\",\n            \"Consider temporal ordering of variables\"\n        ],\n        \"do_not_provide\": [\n            \"Ground truth edges\",\n            \"Canonical variable names\"\n        ],\n        \"expected_difficulty\": \"medium\"\n    },\n\n    \"prompt_details\": {\n        \"minimal\": {\n            \"description\": \"Variable names only\",\n            \"include_fields\": [\"name\"]\n        },\n        \"standard\": {\n            \"description\": \"Names with basic metadata\",\n            \"include_fields\": [\"name\", \"type\", \"short_description\", \"states\"]\n        },\n        \"rich\": {\n            \"description\": \"Full context for complex reasoning\",\n            \"include_fields\": [\n                \"name\", \"type\", \"role\", \"short_description\",\n                \"extended_description\", \"states\", \"sensitivity_hints\"\n            ]\n        }\n    },\n\n    \"variables\": [\n        {\n            \"name\": \"smoke\",\n            \"llm_name\": \"tobacco_history\",\n            \"display_name\": \"Smoking Status\",\n            \"type\": \"binary\",\n            \"states\": [\"never\", \"ever\"],\n            \"role\": \"exogenous\",\n            \"category\": \"exposure\",\n            \"short_description\": \"Patient tobacco smoking history.\",\n            \"extended_description\": \"Self-reported lifetime smoking history. Known major risk factor for lung cancer with dose-response relationship.\",\n            \"base_rate\": {\"never\": 0.65, \"ever\": 0.35},\n            \"sensitivity_hints\": \"Strong causal effect on respiratory outcomes.\",\n            \"related_domain_knowledge\": [\n                \"Smoking contains carcinogens that damage lung tissue\",\n                \"Risk increases with duration and intensity\"\n            ],\n            \"references\": [\"Doll &amp; Hill (1950)\", \"IARC Monograph\"]\n        },\n        {\n            \"name\": \"cancer\",\n            \"llm_name\": \"lung_malignancy\",\n            \"display_name\": \"Lung Cancer Status\",\n            \"type\": \"binary\",\n            \"states\": [\"negative\", \"positive\"],\n            \"role\": \"endogenous\",\n            \"category\": \"outcome\",\n            \"short_description\": \"Lung cancer diagnosis.\",\n            \"extended_description\": \"Confirmed lung cancer diagnosis. Primary outcome variable in smoking studies.\",\n            \"sensitivity_hints\": \"Caused by multiple factors including smoking and genetics.\"\n        },\n        {\n            \"name\": \"genetics\",\n            \"llm_name\": \"genetic_predisposition\",\n            \"type\": \"categorical\",\n            \"states\": [\"low\", \"medium\", \"high\"],\n            \"role\": \"exogenous\",\n            \"category\": \"genetic\",\n            \"short_description\": \"Genetic predisposition to lung cancer.\"\n        }\n    ],\n\n    \"constraints\": {\n        \"forbidden_edges\": [\n            [\"cancer\", \"smoke\"],\n            [\"cancer\", \"genetics\"]\n        ],\n        \"required_edges\": [],\n        \"partial_order\": [\n            [\"smoke\", \"cancer\"],\n            [\"genetics\", \"cancer\"]\n        ]\n    },\n\n    \"ground_truth\": {\n        \"edges_expert\": [\n            [\"smoke\", \"cancer\"],\n            [\"genetics\", \"cancer\"]\n        ]\n    }\n}\n</code></pre>"},{"location":"userguide/model_specification/#field-reference","title":"Field Reference","text":""},{"location":"userguide/model_specification/#root-fields","title":"Root Fields","text":"Field Type Required Description <code>schema_version</code> string No Schema version (default: \"2.0\") <code>network</code> string Yes Identifier for the network (e.g., \"asia\") <code>domain</code> string Yes Domain (e.g., \"epidemiology\", \"genetics\") <code>purpose</code> string No Purpose or description of the context <code>provenance</code> object No Source and provenance information <code>llm_guidance</code> object No Guidance for LLM interactions <code>prompt_details</code> object No Custom prompt detail definitions <code>variables</code> array Yes List of variable specifications <code>constraints</code> object No Structural constraints <code>causal_principles</code> array No Domain causal principles <code>ground_truth</code> object No Ground truth for evaluation"},{"location":"userguide/model_specification/#variable-fields","title":"Variable Fields","text":"Field Type Required Description <code>name</code> string Yes Benchmark/literature name for ground truth <code>llm_name</code> string No Name used for LLM queries (defaults to name) <code>type</code> string Yes One of: binary, categorical, ordinal, continuous <code>display_name</code> string No Human-readable name for reports <code>aliases</code> array No Alternative names"},{"location":"userguide/model_specification/#semantic-disguising-with-name-vs-llm_name","title":"Semantic Disguising with name vs llm_name","text":"<p>The <code>name</code> and <code>llm_name</code> fields enable semantic disguising - using meaningful but non-canonical names to reduce LLM memorisation whilst still allowing evaluation against benchmarks.</p> <p>Example: For the ASIA network's \"Tuberculosis\" variable:</p> <pre><code>{\n  \"name\": \"tub\",\n  \"llm_name\": \"HasTB\",\n  \"display_name\": \"Tuberculosis Status\",\n  \"type\": \"binary\"\n}\n</code></pre> <ul> <li><code>name</code>: \"tub\" - the original ASIA benchmark identifier for evaluation</li> <li><code>llm_name</code>: \"HasTB\" - sent to the LLM (meaningful but not the benchmark name)</li> <li><code>display_name</code>: \"Tuberculosis Status\" - for human-readable reports</li> </ul> <p>This approach:</p> <ol> <li>Reduces memorisation - LLM sees \"HasTB\" not \"tub\" from the benchmark</li> <li>Preserves semantics - The llm_name still conveys clinical meaning</li> <li>Enables evaluation - Results mapped back via <code>name</code> field</li> </ol>"},{"location":"userguide/model_specification/#additional-variable-fields","title":"Additional Variable Fields","text":"Field Type Required Description <code>states</code> array No Possible values for discrete variables <code>role</code> string No One of: exogenous, endogenous, latent <code>category</code> string No Domain-specific category <code>short_description</code> string No Brief description <code>extended_description</code> string No Detailed description with context <code>base_rate</code> object No Prior probabilities <code>conditional_rates</code> object No Conditional probabilities <code>sensitivity_hints</code> string No Hints about causal relationships <code>related_domain_knowledge</code> array No Domain knowledge statements <code>references</code> array No Literature references"},{"location":"userguide/model_specification/#variable-types","title":"Variable Types","text":"<ul> <li>binary - Two states (yes/no, present/absent)</li> <li>categorical - Multiple unordered states (colors, categories)</li> <li>ordinal - Multiple ordered states (low/medium/high, stages)</li> <li>continuous - Numeric values (age, temperature, concentrations)</li> </ul>"},{"location":"userguide/model_specification/#variable-roles","title":"Variable Roles","text":"<ul> <li>exogenous - No parents in the causal graph (root causes)</li> <li>endogenous - Has parents (caused by other variables)</li> <li>latent - Unobserved/hidden variable</li> </ul>"},{"location":"userguide/model_specification/#prompt-details","title":"Prompt Details","text":"<p>Prompt details control how much variable information is provided to LLMs:</p>"},{"location":"userguide/model_specification/#minimal-view","title":"Minimal View","text":"<p>Only variable names - tests pure structural reasoning:</p> <pre><code>{\"include_fields\": [\"name\"]}\n</code></pre> <p>Output: <code>[{\"name\": \"smoking\"}, {\"name\": \"cancer\"}]</code></p>"},{"location":"userguide/model_specification/#standard-view","title":"Standard View","text":"<p>Names with basic metadata - balanced context:</p> <pre><code>{\"include_fields\": [\"name\", \"type\", \"short_description\", \"states\"]}\n</code></pre>"},{"location":"userguide/model_specification/#rich-view","title":"Rich View","text":"<p>Full context for complex reasoning:</p> <pre><code>{\n    \"include_fields\": [\n        \"name\", \"type\", \"role\", \"short_description\",\n        \"extended_description\", \"states\", \"sensitivity_hints\"\n    ]\n}\n</code></pre>"},{"location":"userguide/model_specification/#constraints","title":"Constraints","text":"<p>Structural constraints guide graph generation:</p>"},{"location":"userguide/model_specification/#forbidden-edges","title":"Forbidden Edges","text":"<p>Edges that must not exist:</p> <pre><code>{\n    \"forbidden_edges\": [\n        [\"effect\", \"cause\"],\n        [\"outcome\", \"exposure\"]\n    ]\n}\n</code></pre>"},{"location":"userguide/model_specification/#required-edges","title":"Required Edges","text":"<p>Edges that must exist:</p> <pre><code>{\n    \"required_edges\": [\n        [\"treatment\", \"outcome\"]\n    ]\n}\n</code></pre>"},{"location":"userguide/model_specification/#partial-order","title":"Partial Order","text":"<p>Temporal ordering constraints (A must precede B):</p> <pre><code>{\n    \"partial_order\": [\n        [\"birth_year\", \"diagnosis_age\"],\n        [\"exposure\", \"disease\"]\n    ]\n}\n</code></pre>"},{"location":"userguide/model_specification/#ground-truth","title":"Ground Truth","text":"<p>For evaluation, ground truth edges can be specified:</p> <pre><code>{\n    \"ground_truth\": {\n        \"edges_expert\": [[\"A\", \"B\"], [\"B\", \"C\"]],\n        \"edges_experiment\": [[\"A\", \"B\"]],\n        \"edges_observational\": [[\"A\", \"B\"], [\"B\", \"C\"], [\"A\", \"C\"]]\n    }\n}\n</code></pre> <ul> <li>edges_expert - Edges from domain expert consensus</li> <li>edges_experiment - Edges confirmed by experiments</li> <li>edges_observational - Edges from observational studies</li> </ul>"},{"location":"userguide/model_specification/#loading-network-context","title":"Loading Network Context","text":"<pre><code>from causaliq_knowledge.graph import NetworkContext\n\n# Load from file\ncontext = NetworkContext.load(\"models/cancer.json\")\n\n# Load with full validation\ncontext, warnings = NetworkContext.load_and_validate(\"models/cancer.json\")\n\n# Access data\nprint(f\"Network: {context.network}\")\nprint(f\"Domain: {context.domain}\")\nprint(f\"Variables: {context.get_variable_names()}\")\nprint(f\"LLM Names: {context.get_llm_names()}\")\n</code></pre>"},{"location":"userguide/model_specification/#example-network-contexts","title":"Example Network Contexts","text":"<p>Example network context files are in the <code>research/models/</code> directory:</p> <ul> <li><code>asia/</code> - ASIA network (pulmonary disease)</li> <li><code>cancer/</code> - Lung cancer model</li> <li><code>sachs/</code> - SACHS protein signalling network</li> <li><code>diabetes/</code> - Diabetes risk factors</li> <li><code>sepsis/</code> - Sepsis clinical model</li> </ul>"},{"location":"userguide/model_specification/#best-practices","title":"Best Practices","text":"<ol> <li>Use meaningful llm_names - Aids LLM reasoning whilst avoiding memorisation</li> <li>Provide short descriptions - Essential context for LLMs</li> <li>Define custom prompt details - Control information disclosure</li> <li>Set provenance - Document data sources</li> <li>Include ground truth - Enable evaluation</li> <li>Add constraints - Encode domain knowledge</li> </ol>"},{"location":"userguide/workflow_integration/","title":"Workflow Integration","text":"<p>This guide explains how to use CausalIQ Knowledge as part of automated CausalIQ Workflows for reproducible causal discovery experiments.</p>"},{"location":"userguide/workflow_integration/#overview","title":"Overview","text":"<p>CausalIQ Knowledge integrates with causaliq-workflow through Python entry points. When both packages are installed, the <code>causaliq-knowledge</code> action becomes automatically available in workflow files.</p>"},{"location":"userguide/workflow_integration/#installation","title":"Installation","text":"<pre><code>pip install causaliq-knowledge causaliq-workflow\n</code></pre> <p>Or install from Test PyPI for development versions:</p> <pre><code>pip install --extra-index-url https://test.pypi.org/simple/ \\\n    causaliq-knowledge causaliq-workflow\n</code></pre>"},{"location":"userguide/workflow_integration/#quick-start","title":"Quick Start","text":""},{"location":"userguide/workflow_integration/#1-create-a-network-context","title":"1. Create a Network Context","text":"<p>Create <code>models/smoking.json</code>:</p> <pre><code>{\n    \"schema_version\": \"2.0\",\n    \"network\": \"smoking\",\n    \"domain\": \"epidemiology\",\n    \"variables\": [\n        {\n            \"name\": \"smoking\",\n            \"llm_name\": \"tobacco_use\",\n            \"type\": \"binary\",\n            \"short_description\": \"Patient smoking status\"\n        },\n        {\n            \"name\": \"lung_cancer\",\n            \"llm_name\": \"malignancy\",\n            \"type\": \"binary\",\n            \"short_description\": \"Lung cancer diagnosis\"\n        },\n        {\n            \"name\": \"genetics\",\n            \"llm_name\": \"genetic_risk\",\n            \"type\": \"categorical\",\n            \"states\": [\"low\", \"medium\", \"high\"],\n            \"short_description\": \"Genetic risk factors\"\n        }\n    ]\n}\n</code></pre>"},{"location":"userguide/workflow_integration/#2-create-a-workflow-file","title":"2. Create a Workflow File","text":"<p>Create <code>workflow.yaml</code>:</p> <pre><code>description: \"Generate causal graph for smoking study\"\nid: \"smoking-graph\"\n\nsteps:\n  - name: \"Generate Graph\"\n    uses: \"causaliq-knowledge\"\n    with:\n      action: \"generate_graph\"\n      network_context: \"models/smoking.json\"\n      output: \"results/smoking_graph.json\"\n      llm_cache: \"cache/smoking.db\"\n      llm_model: \"groq/llama-3.1-8b-instant\"\n</code></pre>"},{"location":"userguide/workflow_integration/#3-run-the-workflow","title":"3. Run the Workflow","text":"<pre><code># Validate without executing\ncausaliq-workflow workflow.yaml --mode dry-run\n\n# Execute the workflow\ncausaliq-workflow workflow.yaml --mode run\n</code></pre>"},{"location":"userguide/workflow_integration/#action-parameters","title":"Action Parameters","text":"<p>The <code>causaliq-knowledge</code> action supports the <code>generate_graph</code> operation:</p> Parameter Required Default Description <code>action</code> Yes - Must be <code>generate_graph</code> <code>network_context</code> Yes - Path to network context JSON file <code>output</code> Yes - Output: <code>.json</code> file path or <code>none</code> for stdout <code>llm_cache</code> Yes - Cache: <code>.db</code> file path or <code>none</code> to disable <code>llm_model</code> No <code>groq/llama-3.1-8b-instant</code> LLM model identifier <code>prompt_detail</code> No <code>standard</code> Detail level: <code>minimal</code>, <code>standard</code>, <code>rich</code> <code>use_benchmark_names</code> No <code>false</code> Use original benchmark variable names <code>llm_temperature</code> No <code>0.1</code> LLM temperature (0.0-2.0)"},{"location":"userguide/workflow_integration/#llm-model-identifiers","title":"LLM Model Identifiers","text":"<p>Models must include a provider prefix:</p> <ul> <li>Groq: <code>groq/llama-3.1-8b-instant</code>, <code>groq/llama-3.1-70b-versatile</code></li> <li>Gemini: <code>gemini/gemini-2.5-flash</code>, <code>gemini/gemini-2.0-flash</code></li> <li>OpenAI: <code>openai/gpt-4o-mini</code>, <code>openai/gpt-4o</code></li> <li>Anthropic: <code>anthropic/claude-sonnet-4-20250514</code></li> <li>DeepSeek: <code>deepseek/deepseek-chat</code>, <code>deepseek/deepseek-reasoner</code></li> <li>Mistral: <code>mistral/mistral-small-latest</code></li> <li>Ollama: <code>ollama/llama3</code>, <code>ollama/mistral</code></li> </ul>"},{"location":"userguide/workflow_integration/#prompt-detail-levels","title":"Prompt Detail Levels","text":"<ul> <li>minimal: Variable names only (tests general LLM knowledge)</li> <li>standard: Names, types, states, and short descriptions</li> <li>rich: Full context including extended descriptions</li> </ul>"},{"location":"userguide/workflow_integration/#workflow-examples","title":"Workflow Examples","text":""},{"location":"userguide/workflow_integration/#comparing-multiple-llm-models","title":"Comparing Multiple LLM Models","text":"<pre><code>description: \"Compare graph generation across LLM providers\"\nid: \"model-comparison\"\nworkflow_cache: \"results/{{id}}_cache.db\"\n\nmatrix:\n  model:\n    - \"groq/llama-3.1-8b-instant\"\n    - \"gemini/gemini-2.5-flash\"\n    - \"deepseek/deepseek-chat\"\n\nsteps:\n  - name: \"Generate Graph\"\n    uses: \"causaliq-knowledge\"\n    with:\n      action: \"generate_graph\"\n      network_context: \"models/cancer.json\"\n      llm_cache: \"cache/llm_cache.db\"\n      llm_model: \"{{model}}\"\n      # Results written to workflow_cache with key: {model}\n</code></pre> <p>This generates 3 graphs, one for each model, stored in the Workflow Cache.</p>"},{"location":"userguide/workflow_integration/#comparing-prompt-detail-levels","title":"Comparing Prompt Detail Levels","text":"<pre><code>description: \"Compare prompt detail levels\"\nid: \"detail-comparison\"\nworkflow_cache: \"results/{{id}}_cache.db\"\n\nmatrix:\n  detail:\n    - \"minimal\"\n    - \"standard\"\n    - \"rich\"\n\nsteps:\n  - name: \"Generate Graph\"\n    uses: \"causaliq-knowledge\"\n    with:\n      action: \"generate_graph\"\n      network_context: \"models/asia.json\"\n      llm_cache: \"cache/asia_llm.db\"\n      llm_model: \"groq/llama-3.1-8b-instant\"\n      prompt_detail: \"{{detail}}\"\n</code></pre>"},{"location":"userguide/workflow_integration/#multi-network-analysis","title":"Multi-Network Analysis","text":"<pre><code>description: \"Generate graphs for benchmark networks\"\nid: \"benchmark-analysis\"\nworkflow_cache: \"results/{{id}}_cache.db\"\n\nmatrix:\n  network:\n    - \"asia\"\n    - \"cancer\"\n    - \"earthquake\"\n    - \"survey\"\n\nsteps:\n  - name: \"Generate Graph\"\n    uses: \"causaliq-knowledge\"\n    with:\n      action: \"generate_graph\"\n      network_context: \"models/{{network}}/{{network}}.json\"\n      llm_cache: \"cache/{{network}}_llm.db\"\n</code></pre>"},{"location":"userguide/workflow_integration/#full-comparison-matrix","title":"Full Comparison Matrix","text":"<pre><code>description: \"Full model \u00d7 detail \u00d7 network comparison\"\nid: \"full-comparison\"\nworkflow_cache: \"results/{{id}}_cache.db\"\n\nmatrix:\n  network:\n    - \"asia\"\n    - \"cancer\"\n  model:\n    - \"groq/llama-3.1-8b-instant\"\n    - \"gemini/gemini-2.5-flash\"\n  detail:\n    - \"minimal\"\n    - \"standard\"\n\nsteps:\n  - name: \"Generate Graph\"\n    uses: \"causaliq-knowledge\"\n    with:\n      action: \"generate_graph\"\n      network_context: \"models/{{network}}.json\"\n      llm_cache: \"cache/llm_cache.db\"\n      llm_model: \"{{model}}\"\n      prompt_detail: \"{{detail}}\"\n</code></pre> <p>This generates 8 graphs (2 networks \u00d7 2 models \u00d7 2 detail levels), all stored in a single Workflow Cache with matrix values as keys.</p>"},{"location":"userguide/workflow_integration/#action-output","title":"Action Output","text":"<p>When a workflow step completes successfully, it returns:</p> <pre><code>{\n    \"status\": \"success\",\n    \"edges_count\": 5,\n    \"variables_count\": 8,\n    \"output_file\": \"results/cancer_graph.json\",\n    \"cache_stats\": {\n        \"cache_hits\": 2,\n        \"cache_misses\": 6\n    }\n}\n</code></pre> <p>In dry-run mode, it returns validation results without executing:</p> <pre><code>{\n    \"status\": \"skipped\",\n    \"reason\": \"dry-run mode\",\n    \"validated_params\": {\n        \"network_context\": \"models/cancer.json\",\n        \"output\": \"results/graph.json\",\n        \"llm_cache\": \"cache/cancer.db\"\n    }\n}\n</code></pre>"},{"location":"userguide/workflow_integration/#output-file-format","title":"Output File Format","text":"<p>The generated graph JSON file contains:</p> <pre><code>{\n    \"edges\": [\n        {\"source\": \"smoking\", \"target\": \"lung_cancer\", \"confidence\": 0.95},\n        {\"source\": \"genetics\", \"target\": \"lung_cancer\", \"confidence\": 0.8}\n    ],\n    \"variables\": [\"smoking\", \"lung_cancer\", \"genetics\"],\n    \"reasoning\": \"Based on epidemiological evidence...\",\n    \"metadata\": {\n        \"model\": \"groq/llama-3.1-8b-instant\",\n        \"prompt_detail\": \"standard\",\n        \"timestamp\": \"2026-02-04T10:30:00Z\"\n    }\n}\n</code></pre>"},{"location":"userguide/workflow_integration/#environment-setup","title":"Environment Setup","text":""},{"location":"userguide/workflow_integration/#api-keys","title":"API Keys","text":"<p>Set environment variables for your chosen LLM providers:</p> <pre><code># Groq (free tier)\nexport GROQ_API_KEY=\"your-groq-key\"\n\n# Google Gemini (free tier)\nexport GEMINI_API_KEY=\"your-gemini-key\"\n\n# OpenAI\nexport OPENAI_API_KEY=\"your-openai-key\"\n\n# Anthropic\nexport ANTHROPIC_API_KEY=\"your-anthropic-key\"\n</code></pre> <p>See LLM Provider Setup for details.</p>"},{"location":"userguide/workflow_integration/#directory-structure","title":"Directory Structure","text":"<p>A typical project structure:</p> <pre><code>my-project/\n\u251c\u2500\u2500 models/\n\u2502   \u251c\u2500\u2500 asia.json\n\u2502   \u251c\u2500\u2500 cancer.json\n\u2502   \u2514\u2500\u2500 smoking.json\n\u251c\u2500\u2500 workflows/\n\u2502   \u251c\u2500\u2500 generate_graphs.yaml\n\u2502   \u2514\u2500\u2500 compare_models.yaml\n\u251c\u2500\u2500 results/           # Generated output files\n\u251c\u2500\u2500 cache/             # LLM response cache\n\u2514\u2500\u2500 .env               # API keys (add to .gitignore)\n</code></pre>"},{"location":"userguide/workflow_integration/#troubleshooting","title":"Troubleshooting","text":""},{"location":"userguide/workflow_integration/#action-not-found","title":"Action Not Found","text":"<pre><code>ActionRegistryError: Action 'causaliq-knowledge' not found\n</code></pre> <p>Ensure both packages are installed in the same environment:</p> <pre><code>pip install causaliq-knowledge causaliq-workflow\n</code></pre>"},{"location":"userguide/workflow_integration/#schema-validation-error","title":"Schema Validation Error","text":"<pre><code>WorkflowExecutionError: Schema file not found\n</code></pre> <p>Upgrade to the latest <code>causaliq-workflow</code>:</p> <pre><code>pip install --upgrade causaliq-workflow\n</code></pre>"},{"location":"userguide/workflow_integration/#invalid-llm-model","title":"Invalid LLM Model","text":"<pre><code>ValueError: LLM model must start with provider prefix\n</code></pre> <p>Include the provider prefix in <code>llm_model</code>:</p> <pre><code># Wrong\nllm_model: \"llama-3.1-8b-instant\"\n\n# Correct\nllm_model: \"groq/llama-3.1-8b-instant\"\n</code></pre>"},{"location":"userguide/workflow_integration/#cache-path-error","title":"Cache Path Error","text":"<pre><code>ValueError: llm_cache must be 'none' or a path ending with .db\n</code></pre> <p>Use <code>.db</code> extension or <code>none</code>:</p> <pre><code># Wrong\nllm_cache: \"cache/data\"\n\n# Correct\nllm_cache: \"cache/data.db\"\n# Or\nllm_cache: \"none\"\n</code></pre>"},{"location":"userguide/workflow_integration/#next-steps","title":"Next Steps","text":"<ul> <li>Network Context Format - Define variables</li> <li>CLI Reference - Command-line usage</li> <li>API Reference - Programmatic access</li> </ul>"}]}